{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, yaml\n",
    "from openvqa.models.model_loader import CfgLoader\n",
    "from utils1.exec1 import Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy\n",
    "import sys\n",
    "from openvqa.datasets.dataset_loader import DatasetLoader\n",
    "\n",
    "class Execution:\n",
    "    def __init__(self, __C):\n",
    "        self.__C = __C\n",
    "\n",
    "        print('Loading dataset........')\n",
    "        self.dataset = DatasetLoader(__C).DataSet()\n",
    "\n",
    "        # If trigger the evaluation after every epoch\n",
    "        # Will create a new cfgs with RUN_MODE = 'val'\n",
    "        self.dataset_eval = None\n",
    "        if __C.EVAL_EVERY_EPOCH:\n",
    "            __C_eval = copy.deepcopy(__C)\n",
    "            setattr(__C_eval, 'RUN_MODE', 'val')\n",
    "\n",
    "            print('Loading validation set for per-epoch evaluation........')\n",
    "            self.dataset_eval = DatasetLoader(__C_eval).DataSet()\n",
    "\n",
    "\n",
    "    def run(self, run_mode):\n",
    "        \n",
    "        if run_mode == 'train':\n",
    "            if self.__C.RESUME is False:\n",
    "                self.empty_log(self.__C.VERSION)\n",
    "            train_engine(self.__C, self.dataset, self.dataset_eval)\n",
    "\n",
    "        elif run_mode == 'val':\n",
    "            test_engine(self.__C, self.dataset, validation=True)\n",
    "\n",
    "        elif run_mode == 'test':\n",
    "            test_engine(self.__C, self.dataset)\n",
    "\n",
    "        else:\n",
    "            exit(-1)\n",
    "\n",
    "\n",
    "    def empty_log(self, version):\n",
    "        print('Initializing log file........')\n",
    "        if (os.path.exists(self.__C.LOG_PATH + '/log_run_' + version + '.txt')):\n",
    "            os.remove(self.__C.LOG_PATH + '/log_run_' + version + '.txt')\n",
    "        print('Finished!')\n",
    "        print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    '''\n",
    "    Parse input arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description='OpenVQA Args')\n",
    "\n",
    "    parser.add_argument('--RUN', dest='RUN_MODE',\n",
    "                      choices=['train', 'val', 'test'],\n",
    "                      help='{train, val, test}',\n",
    "                      default='train',\n",
    "                      type=str, required=False)\n",
    "\n",
    "    parser.add_argument('--MODEL', dest='MODEL',\n",
    "                      choices=[\n",
    "                           'mcan_small',\n",
    "                           'mcan_small_wa',\n",
    "                           'mcan_large',\n",
    "                           'ban_4',\n",
    "                           #Edits\n",
    "                           'ban_8_wa',\n",
    "                           'baseline_wa',\n",
    "                           #End of Edits\n",
    "                           'ban_8',\n",
    "                           'mfb',\n",
    "                           'mfb_wa',\n",
    "                           'mfh',\n",
    "                           'mfh_wa',\n",
    "                           'mem',\n",
    "                           'butd',\n",
    "                           'butd_wa',\n",
    "                           'baseline',\n",
    "                           'baseline_wa_no_fusion',\n",
    "                           'positional',\n",
    "                           'mcan_large_wa',\n",
    "                           'mcan_small_augmented',\n",
    "                           'mcan_small_without_a'\n",
    "                           ]\n",
    "                        ,\n",
    "                      help='{'\n",
    "                           'mcan_small,'\n",
    "                           'mcan_small_wa,'\n",
    "                           'mcan_large,'\n",
    "                            #Edits\n",
    "                           'ban_wa,'\n",
    "                           'baseline_wa,'\n",
    "                           #End of Edits\n",
    "                           'ban_4,'\n",
    "                           'ban_8,'\n",
    "                           'mfb,'\n",
    "                           'mfb_wa,'\n",
    "                           'mfh,'\n",
    "                           'mfh_wa,'\n",
    "                           'butd,'\n",
    "                           'butd_wa,'\n",
    "                           'baseline,'\n",
    "                           'baseline_wa_no_fusion,'\n",
    "                           'positional,'\n",
    "                           '}'\n",
    "                        ,\n",
    "                      type=str, required=True)\n",
    "\n",
    "    parser.add_argument('--DATASET', dest='DATASET',\n",
    "                      choices=['vqa', 'gqa', 'clevr'],\n",
    "                      help='{'\n",
    "                           'vqa,'\n",
    "                           'gqa,'\n",
    "                           'clevr,'\n",
    "                           '}'\n",
    "                        ,\n",
    "                      default='vqa',  \n",
    "                      type=str, required=False)\n",
    "\n",
    "    parser.add_argument('--SPLIT', dest='TRAIN_SPLIT',\n",
    "                      choices=['train', 'train+val', 'train+val+vg'],\n",
    "                      help=\"set training split, \"\n",
    "                           \"vqa: {'train', 'train+val', 'train+val+vg'}\"\n",
    "                           \"gqa: {'train', 'train+val'}\"\n",
    "                           \"clevr: {'train', 'train+val'}\"\n",
    "                        ,\n",
    "                        default='train', required=False,\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--EVAL_EE', dest='EVAL_EVERY_EPOCH',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='True: evaluate the val split when an epoch finished,'\n",
    "                           'False: do not evaluate on local',\n",
    "                           default='True',\n",
    "                           required=False,\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--SAVE_PRED', dest='TEST_SAVE_PRED',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='True: save the prediction vectors,'\n",
    "                           'False: do not save the prediction vectors',\n",
    "                      default='True',\n",
    "                      required=False,\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--BS', dest='BATCH_SIZE',\n",
    "                      help='batch size in training',\n",
    "                      type=int)\n",
    "\n",
    "    parser.add_argument('--GPU', dest='GPU',\n",
    "                      help=\"gpu choose, eg.'0, 1, 2, ...'\",\n",
    "                      default='0, 1',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--SEED', dest='SEED',\n",
    "                      help='fix random seed',\n",
    "                      type=int)\n",
    "\n",
    "    parser.add_argument('--VERSION', dest='VERSION',\n",
    "                      help='Enter descriptive name here (eg baseline_wa_gru), will be used for WANDB and for version',\n",
    "                      required=True,\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--RESUME', dest='RESUME',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='True: use checkpoint to resume training,'\n",
    "                           'False: start training with random init',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--CKPT_V', dest='CKPT_VERSION',\n",
    "                      help='checkpoint version',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--CKPT_E', dest='CKPT_EPOCH',\n",
    "                      help='checkpoint epoch',\n",
    "                      type=int)\n",
    "\n",
    "    parser.add_argument('--CKPT_PATH', dest='CKPT_PATH',\n",
    "                      help='load checkpoint path, we '\n",
    "                           'recommend that you use '\n",
    "                           'CKPT_VERSION and CKPT_EPOCH '\n",
    "                           'instead, it will override'\n",
    "                           'CKPT_VERSION and CKPT_EPOCH',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--ACCU', dest='GRAD_ACCU_STEPS',\n",
    "                      help='split batch to reduce gpu memory usage',\n",
    "                      type=int)\n",
    "\n",
    "    parser.add_argument('--NW', dest='NUM_WORKERS',\n",
    "                      help='multithreaded loading to accelerate IO',\n",
    "                      type=int)\n",
    "\n",
    "    parser.add_argument('--PINM', dest='PIN_MEM',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='True: use pin memory, False: not use pin memory',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--VERB', dest='VERBOSE',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='True: verbose print, False: simple print',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--USE_NEW_QUESTION', dest='USE_NEW_QUESTION',\n",
    "                      choices=['True', 'False'],\n",
    "                      help='whether to use new question while testing',\n",
    "                      default='False',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--NEW_QUESTION', dest='NEW_QUESTION',\n",
    "                      help='the new question to be asked while testing',\n",
    "                      type=str)\n",
    "\n",
    "    parser.add_argument('--IMAGE_ID', dest='IMAGE_ID',\n",
    "                      help='image id on which the questions to be asked',\n",
    "                      type=str)\n",
    "    \n",
    "    ######################################################\n",
    "    #########  CHANGE MODEL AND VERSION HERE #############\n",
    "    ######################################################\n",
    "    args = parser.parse_args(args=['--MODEL', 'baseline_wa', '--VERSION', 'hakku'])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(BATCH_SIZE=None, CKPT_EPOCH=None, CKPT_PATH=None, CKPT_VERSION=None, DATASET='vqa', EVAL_EVERY_EPOCH='True', GPU='0, 1', GRAD_ACCU_STEPS=None, IMAGE_ID=None, MODEL='baseline_wa', NEW_QUESTION=None, NUM_WORKERS=None, PIN_MEM=None, RESUME=None, RUN_MODE='train', SEED=None, TEST_SAVE_PRED='True', TRAIN_SPLIT='train', USE_NEW_QUESTION='False', VERBOSE=None, VERSION='hakku')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset ........\n",
      "Finished!\n",
      "\n",
      "Hyper Parameters:\n",
      "{ ALPHA             }->1\n",
      "{ ANS_STDDEV        }->0.01\n",
      "{ AUGMENTED_ANSWER  }->False\n",
      "{ BATCH_SIZE        }->64\n",
      "{ BBOXFEAT_EMB_SIZE }->2048\n",
      "{ BETA              }->30\n",
      "{ CACHE_PATH        }->./results/cache\n",
      "{ CAP_DIST          }->0.3\n",
      "{ CKPTS_PATH        }->./ckpts\n",
      "{ CKPT_EPOCH        }->0\n",
      "{ CKPT_PATH         }->None\n",
      "{ CKPT_VERSION      }->343612\n",
      "{ DATASET           }->vqa\n",
      "{ DATA_PATH         }->{'clevr': './data/clevr', 'vqa': './data/vqa', 'gqa': './data/gqa'}\n",
      "{ DATA_ROOT         }->./data\n",
      "{ DEVICES           }->[0, 1]\n",
      "{ DROPOUT_R         }->0.1\n",
      "{ EVAL_BATCH_SIZE   }->32\n",
      "{ EVAL_EVERY_EPOCH  }->True\n",
      "{ FEATS_PATH        }->{'clevr': {'test': './data/clevr/feats/test', 'val': './data/clevr/feats/val', 'train': './data/clevr/feats/train'}, 'vqa': {'test': './data/vqa/feats/test2015', 'val': './data/vqa/feats/val2014', 'train': './data/vqa/feats/train2014'}, 'gqa': {'default-frcn': './data/gqa/feats/gqa-frcn', 'default-grid': './data/gqa/feats/gqa-grid'}}\n",
      "{ FEAT_SIZE         }->{'clevr': {'GRID_FEAT_SIZE': (196, 1024)}, 'vqa': {'FRCN_FEAT_SIZE': (100, 2048), 'BBOX_FEAT_SIZE': (100, 5)}, 'gqa': {'FRCN_FEAT_SIZE': (100, 2048), 'BBOX_FEAT_SIZE': (100, 5), 'GRID_FEAT_SIZE': (49, 2048)}}\n",
      "{ FF_SIZE           }->2048\n",
      "{ FLAT_GLIMPSES     }->1\n",
      "{ FLAT_MLP_SIZE     }->512\n",
      "{ FLAT_OUT_SIZE     }->1024\n",
      "{ GPU               }->0, 1\n",
      "{ GRAD_ACCU_STEPS   }->1\n",
      "{ GRAD_NORM_CLIP    }->-1\n",
      "{ HIDDEN_SIZE       }->512\n",
      "{ LOG_PATH          }->./results/log\n",
      "{ LOSS_FUNC         }->bce\n",
      "{ LOSS_FUNC_NAME_DICT }->{'mse': 'MSELoss', 'kld': 'KLDivLoss', 'ce': 'CrossEntropyLoss', 'bce': 'BCEWithLogitsLoss'}\n",
      "{ LOSS_FUNC_NONLINEAR }->{'mse': [None, None], 'kld': ['log_softmax', None], 'ce': [None, 'flat'], 'bce': [None, None]}\n",
      "{ LOSS_REDUCTION    }->sum\n",
      "{ LR_BASE           }->0.0001\n",
      "{ LR_DECAY_LIST     }->[10, 12]\n",
      "{ LR_DECAY_R        }->0.2\n",
      "{ MAX_EPOCH         }->13\n",
      "{ MODEL             }->baseline_wa\n",
      "{ MODEL_USE         }->baseline_wa\n",
      "{ NUM_WORKERS       }->8\n",
      "{ N_GPU             }->2\n",
      "{ OPT               }->Adam\n",
      "{ OPT_PARAMS        }->{'amsgrad': False, 'weight_decay': 0, 'betas': (0.9, 0.98), 'eps': 1e-09}\n",
      "{ PIN_MEM           }->True\n",
      "{ PRED_PATH         }->./results/pred\n",
      "{ PROJ_STDDEV       }->0.1\n",
      "{ RAW_PATH          }->{'clevr': {'test': './data/clevr/raw/questions/CLEVR_test_questions.json', 'val': './data/clevr/raw/questions/CLEVR_val_questions.json', 'train': './data/clevr/raw/questions/CLEVR_train_questions.json'}, 'vqa': {'vg': './data/vqa/raw/VG_questions.json', 'val': './data/vqa/raw/v2_OpenEnded_mscoco_val2014_questions.json', 'train': './data/vqa/raw/v2_OpenEnded_mscoco_train2014_questions.json', 'vg-anno': './data/vqa/raw/VG_annotations.json', 'test': './data/vqa/raw/v2_OpenEnded_mscoco_test2015_questions.json', 'val-anno': './data/vqa/raw/v2_mscoco_val2014_annotations.json', 'train-anno': './data/vqa/raw/v2_mscoco_train2014_annotations.json'}, 'gqa': {'val': './data/gqa/raw/questions1.2/val_balanced_questions.json', 'train': './data/gqa/raw/questions1.2/train_balanced_questions.json', 'testdev_all': './data/gqa/raw/questions1.2/testdev_all_questions.json', 'val_choices': './data/gqa/raw/eval/val_choices.json', 'test': './data/gqa/raw/questions1.2/submission_all_questions.json', 'val_all': './data/gqa/raw/questions1.2/val_all_questions.json', 'testdev': './data/gqa/raw/questions1.2/testdev_balanced_questions.json', 'train_choices': './data/gqa/raw/eval/train_choices'}}\n",
      "{ RESULT_PATH       }->./results/result_test\n",
      "{ RESUME            }->False\n",
      "{ RUN_MODE          }->train\n",
      "{ SAVED_PATH        }->./saved\n",
      "{ SEED              }->343612\n",
      "{ SPLIT             }->{'test': 'test', 'val': 'val', 'train': 'train'}\n",
      "{ SPLITS            }->{'clevr': {'test': 'test', 'val': 'val', 'train': ''}, 'vqa': {'test': 'test', 'val': 'val', 'train': 'train'}, 'gqa': {'test': 'test', 'val': 'testdev', 'train': ''}}\n",
      "{ SUB_BATCH_SIZE    }->64\n",
      "{ TASK_LOSS_CHECK   }->{'clevr': ['ce'], 'vqa': ['bce', 'kld'], 'gqa': ['ce']}\n",
      "{ TEST_SAVE_PRED    }->False\n",
      "{ TRAIN_SPLIT       }->train\n",
      "{ USE_AUX_FEAT      }->False\n",
      "{ USE_BBOX_FEAT     }->False\n",
      "{ USE_GLOVE         }->True\n",
      "{ USE_NEW_QUESTION  }->False\n",
      "{ VERBOSE           }->True\n",
      "{ VERSION           }->hakku\n",
      "{ WARMUP_EPOCH      }->3\n",
      "{ WITH_ANSWER       }->True\n",
      "{ WITH_FUSION_LOSS  }->True\n",
      "{ WORD_EMBED_SIZE   }->300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slam/dl/lib/python3.5/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "cfg_file = \"configs/{}/{}.yml\".format(args.DATASET, args.MODEL)\n",
    "with open(cfg_file, 'r') as f:\n",
    "\n",
    "    # Loads the yaml file\n",
    "    yaml_dict = yaml.load(f)\n",
    "\n",
    "# Loads the model_cfgs + base_cfgs\n",
    "__C = CfgLoader(yaml_dict['MODEL_USE']).load()\n",
    "\n",
    "# Loads the command line cfgs\n",
    "args = __C.str_to_bool(args)\n",
    "args_dict = __C.parse_to_dict(args)\n",
    "\n",
    "# {**dict1, **dict2} creates a new dictionary by merging dict1 and dict2, using dict2 for key clashes\n",
    "args_dict = {**yaml_dict, **args_dict}\n",
    "__C.add_args(args_dict)\n",
    "__C.proc()\n",
    "\n",
    "# FINAL PREFERENCE OF CFGS:\n",
    "# COMMAND LINE > YAML FILE > MODEL CFGS > BASE CFGS\n",
    "\n",
    "print('Hyper Parameters:')\n",
    "print(__C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset........\n",
      "Loading all questions (for statistics)\n",
      "Loading all image features\n",
      "Loading split questions and answers\n",
      " ========== Dataset size: 443757\n",
      "\n",
      "Tokenising questions\n",
      " ========== Question token vocab size: 20573\n",
      "Tokenising answers\n",
      " ========== Answer token vocab size:  13488\n",
      " ========== Answer token vocab size (occur more than 8 times): 3129\n",
      "Finished!\n",
      "\n",
      "Loading validation set for per-epoch evaluation........\n",
      "Loading all questions (for statistics)\n",
      "Loading all image features\n",
      "Loading split questions and answers\n",
      " ========== Dataset size: 214354\n",
      "\n",
      "Tokenising questions\n",
      " ========== Question token vocab size: 20573\n",
      " ========== Answer token vocab size (occur more than 8 times): 3129\n",
      "Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execution = Execution(__C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, datetime, shutil, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import wandb\n",
    "from openvqa.models.model_loader import ModelLoader\n",
    "from openvqa.utils.optim import get_optim, adjust_lr\n",
    "from utils1.test_engine import test_engine, ckpt_proc\n",
    "from vis import plotter, vis_func\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "import sys\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "def train_engine(__C, dataset, dataset_eval=None):\n",
    "\n",
    "    data_size = dataset.data_size\n",
    "    token_size = dataset.token_size\n",
    "    ans_size = dataset.ans_size\n",
    "    pretrained_emb = dataset.pretrained_emb\n",
    "\n",
    "    #Edits\n",
    "    pretrained_emb_ans = dataset.pretrained_emb_ans\n",
    "    token_size_ans = dataset.token_size_ans #End of Edits\n",
    "\n",
    "    print(\"Model being used is {}\".format(__C.MODEL_USE))\n",
    "\n",
    "    net = ModelLoader(__C).Net(\n",
    "        __C,\n",
    "        pretrained_emb,\n",
    "        token_size,\n",
    "        ans_size,\n",
    "        pretrained_emb_ans,\n",
    "        token_size_ans\n",
    "    )\n",
    "\n",
    "    net.cuda()\n",
    "    net.train()\n",
    "\n",
    "    if __C.N_GPU > 1:\n",
    "        net = nn.DataParallel(net, device_ids=__C.DEVICES)\n",
    "\n",
    "    # Define Loss Function\n",
    "    loss_fn = eval('torch.nn.' + __C.LOSS_FUNC_NAME_DICT[__C.LOSS_FUNC] + \"(reduction='\" + __C.LOSS_REDUCTION + \"').cuda()\")\n",
    "\n",
    "\n",
    "    # creating a folder for saving the numpy visualization arrays\n",
    "    if (__C.WITH_ANSWER and ((__C.VERSION) not in os.listdir(__C.SAVED_PATH))):\n",
    "        os.mkdir(__C.SAVED_PATH + '/' + __C.VERSION)\n",
    "\n",
    "\n",
    "    # Load checkpoint if resume training\n",
    "    if __C.RESUME:\n",
    "        print(' ========== Resume training')\n",
    "\n",
    "        if __C.CKPT_PATH is not None:\n",
    "            print('Warning: Now using CKPT_PATH args, '\n",
    "                  'CKPT_VERSION and CKPT_EPOCH will not work')\n",
    "            path = __C.CKPT_PATH\n",
    "        else:\n",
    "            path = __C.CKPTS_PATH + \\\n",
    "                   '/ckpt_' + __C.CKPT_VERSION + \\\n",
    "                   '/epoch' + str(__C.CKPT_EPOCH) + '.pkl'\n",
    "\n",
    "        # Load the network parameters\n",
    "        print('Loading ckpt from {}'.format(path))\n",
    "        ckpt = torch.load(path)\n",
    "        print('Finish!')\n",
    "\n",
    "        if __C.N_GPU > 1:\n",
    "            net.load_state_dict(ckpt_proc(ckpt['state_dict']))\n",
    "        else:\n",
    "            net.load_state_dict(ckpt['state_dict'])\n",
    "        start_epoch = ckpt['epoch']\n",
    "\n",
    "        # Load the optimizer paramters\n",
    "        optim = get_optim(__C, net, data_size, ckpt['lr_base'])\n",
    "        optim._step = int(data_size / __C.BATCH_SIZE * start_epoch)\n",
    "        optim.optimizer.load_state_dict(ckpt['optimizer'])\n",
    "        \n",
    "        if ('ckpt_' + __C.VERSION) not in os.listdir(__C.CKPTS_PATH):\n",
    "            os.mkdir(__C.CKPTS_PATH + '/ckpt_' + __C.VERSION)\n",
    "\n",
    "    else:\n",
    "        if ('ckpt_' + __C.VERSION) not in os.listdir(__C.CKPTS_PATH):\n",
    "            #shutil.rmtree(__C.CKPTS_PATH + '/ckpt_' + __C.VERSION)\n",
    "            os.mkdir(__C.CKPTS_PATH + '/ckpt_' + __C.VERSION)\n",
    "\n",
    "        optim = get_optim(__C, net, data_size)\n",
    "        start_epoch = 0\n",
    "\n",
    "    loss_sum = 0\n",
    "    named_params = list(net.named_parameters())\n",
    "    grad_norm = np.zeros(len(named_params))\n",
    "\n",
    "    # Define multi-thread dataloader\n",
    "    # if __C.SHUFFLE_MODE in ['external']:\n",
    "    #     dataloader = Data.DataLoader(\n",
    "    #         dataset,\n",
    "    #         batch_size=__C.BATCH_SIZE,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=__C.NUM_WORKERS,\n",
    "    #         pin_memory=__C.PIN_MEM,\n",
    "    #         drop_last=True\n",
    "    #     )\n",
    "    # else:\n",
    "    dataloader = Data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=__C.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=__C.NUM_WORKERS,\n",
    "        pin_memory=__C.PIN_MEM,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    logfile = open(\n",
    "        __C.LOG_PATH +\n",
    "        '/log_run_' + __C.VERSION + '.txt',\n",
    "        'a+'\n",
    "    )\n",
    "    logfile.write(str(__C))\n",
    "    logfile.close()\n",
    "\n",
    "    # For dry runs\n",
    "    # os.environ['WANDB_MODE'] = 'dryrun' \n",
    "\n",
    "    # initializing the wandb project\n",
    "    # TODO to change the name of project later, once the proper coding starts\n",
    "    #wandb.init(project=\"openvqa\", name=__C.VERSION, config=__C)\n",
    "\n",
    "    # obtain histogram of each gradients in network as it trains\n",
    "    #wandb.watch(net, log=\"all\")\n",
    "\n",
    "    #wandb.save(\"./openvqa/models/\" + str(__C.MODEL_USE) + \"/net.py\")\n",
    "    #wandb.save(\"./utils1/train_engine.py\")\n",
    "\n",
    "    # Training script\n",
    "    for epoch in range(start_epoch, __C.MAX_EPOCH):\n",
    "\n",
    "        # Save log to file\n",
    "        logfile = open(\n",
    "            __C.LOG_PATH +\n",
    "            '/log_run_' + __C.VERSION + '.txt',\n",
    "            'a+'\n",
    "        )\n",
    "        logfile.write(\n",
    "            '=====================================\\nnowTime: ' +\n",
    "            datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') +\n",
    "            '\\n'\n",
    "        )\n",
    "        logfile.close()\n",
    "\n",
    "        # Learning Rate Decay\n",
    "        if epoch in __C.LR_DECAY_LIST:\n",
    "            adjust_lr(optim, __C.LR_DECAY_R)\n",
    "\n",
    "        # Externally shuffle data list\n",
    "        # if __C.SHUFFLE_MODE == 'external':\n",
    "        #     dataset.shuffle_list(dataset.ans_list)\n",
    "\n",
    "        time_start = time.time()\n",
    "        # Iteration\n",
    "        for step, (\n",
    "                frcn_feat_iter,\n",
    "                grid_feat_iter,\n",
    "                bbox_feat_iter,\n",
    "                ques_ix_iter,\n",
    "\n",
    "                #Edits\n",
    "                ans_ix_iter,\n",
    "                #End of Edits\n",
    "\n",
    "                ans_iter,\n",
    "                ques_type\n",
    "\n",
    "        ) in enumerate(dataloader):\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            frcn_feat_iter = frcn_feat_iter.cuda()\n",
    "            grid_feat_iter = grid_feat_iter.cuda()\n",
    "            bbox_feat_iter = bbox_feat_iter.cuda()\n",
    "            ques_ix_iter = ques_ix_iter.cuda()\n",
    "            #Edits\n",
    "            ans_ix_iter = ans_ix_iter.cuda()\n",
    "            #End of Edits\n",
    "            ans_iter = ans_iter.cuda()\n",
    "\n",
    "            loss_tmp = 0\n",
    "\n",
    "            loss_img_ques_tmp = 0\n",
    "            loss_ans_tmp = 0\n",
    "            loss_interp_tmp = 0\n",
    "            loss_fusion_tmp = 0\n",
    "\n",
    "            for accu_step in range(__C.GRAD_ACCU_STEPS):\n",
    "                loss_tmp = 0\n",
    "                loss_img_ques_tmp = 0\n",
    "                loss_ans_tmp = 0\n",
    "                loss_interp_tmp = 0\n",
    "                loss_fusion_tmp = 0\n",
    "\n",
    "                sub_frcn_feat_iter = \\\n",
    "                    frcn_feat_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                                  (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "                sub_grid_feat_iter = \\\n",
    "                    grid_feat_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                                  (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "                sub_bbox_feat_iter = \\\n",
    "                    bbox_feat_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                                  (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "                sub_ques_ix_iter = \\\n",
    "                    ques_ix_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                                 (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "                #Edits\n",
    "                sub_ans_ix_iter = \\\n",
    "                    ans_ix_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                                 (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "                #End of Edits\n",
    "\n",
    "                sub_ans_iter = \\\n",
    "                    ans_iter[accu_step * __C.SUB_BATCH_SIZE:\n",
    "                             (accu_step + 1) * __C.SUB_BATCH_SIZE]\n",
    "\n",
    "                \n",
    "                # when making predictions also pass the ans_iter which is a dictionary from which you\n",
    "                # can extract answers and pass them through decoders\n",
    "\n",
    "                if (__C.WITH_ANSWER):\n",
    "                    pred_img_ques, pred_ans, pred_fused, z_img_ques, z_ans, z_fused = net(\n",
    "                        sub_frcn_feat_iter,\n",
    "                        sub_grid_feat_iter,\n",
    "                        sub_bbox_feat_iter,\n",
    "                        sub_ques_ix_iter,\n",
    "                        sub_ans_ix_iter,\n",
    "                        step,\n",
    "                        epoch\n",
    "                    )\n",
    "                else:\n",
    "                     pred_img_ques = net(\n",
    "                        sub_frcn_feat_iter,\n",
    "                        sub_grid_feat_iter,\n",
    "                        sub_bbox_feat_iter,\n",
    "                        sub_ques_ix_iter,\n",
    "                        sub_ans_ix_iter,\n",
    "                        step,\n",
    "                        epoch\n",
    "                    )\n",
    "\n",
    "\n",
    "                # we need to change the loss terms accordingly\n",
    "                # now we need to modify the loss terms for the same\n",
    "                \n",
    "                #Edits: creating the loss items for each of the prediction vector\n",
    "\n",
    "                loss_item_img_ques = [pred_img_ques, sub_ans_iter]\n",
    "\n",
    "                # only calculate the ans and interp loss in case of WITH_ANSWER\n",
    "                if (__C.WITH_ANSWER):\n",
    "                    loss_item_ans = [pred_ans, sub_ans_iter]\n",
    "                    loss_item_interp = [pred_fused, sub_ans_iter]\n",
    "\n",
    "                \n",
    "                loss_nonlinear_list = __C.LOSS_FUNC_NONLINEAR[__C.LOSS_FUNC]\n",
    "                \n",
    "                # applying the same transformation on the all three\n",
    "                # althought for 'bce' loss the following does nothing\n",
    "                for item_ix, loss_nonlinear in enumerate(loss_nonlinear_list):\n",
    "                    if loss_nonlinear in ['flat']:\n",
    "                        loss_item_img_ques[item_ix] = loss_item_img_ques[item_ix].view(-1)\n",
    "                    elif loss_nonlinear:\n",
    "                        loss_item_img_ques[item_ix] = eval('F.' + loss_nonlinear + '(loss_item_img_ques[item_ix], dim=1)')\n",
    "\n",
    "                for item_ix, loss_nonlinear in enumerate(loss_nonlinear_list):\n",
    "                    if loss_nonlinear in ['flat'] and __C.WITH_ANSWER:\n",
    "                        loss_item_ans[item_ix] = loss_item_ans[item_ix].view(-1)\n",
    "                    elif loss_nonlinear and __C.WITH_ANSWER:\n",
    "                        loss_item_ans[item_ix] = eval('F.' + loss_nonlinear + '(loss_item_ans[item_ix], dim=1)')\n",
    "\n",
    "                for item_ix, loss_nonlinear in enumerate(loss_nonlinear_list):\n",
    "                    if loss_nonlinear in ['flat'] and __C.WITH_ANSWER:\n",
    "                        loss_item_interp[item_ix] = loss_item_interp[item_ix].view(-1)\n",
    "                    elif loss_nonlinear and __C.WITH_ANSWER:\n",
    "                        loss_item_interp[item_ix] = eval('F.' + loss_nonlinear + '(loss_item_interp[item_ix], dim=1)')\n",
    "\n",
    "\n",
    "                # Now we create all the four losses and then add them\n",
    "                #print(\"shape of loss_item_img_ques[0] is {} and of loss_item_img_ques[1] is {}\".format(loss_item_img_ques[0],loss_item_img_ques[1]))\n",
    "                loss_img_ques = loss_fn(loss_item_img_ques[0], loss_item_img_ques[1])\n",
    "\n",
    "\n",
    "                loss = 0\n",
    "                loss += loss_img_ques\n",
    "                \n",
    "                if (__C.WITH_ANSWER):\n",
    "\n",
    "                    # loss for the prediction from the answer\n",
    "                    #print(\"shape of loss_item_ans[0] is {} and of loss_item_ans[1] is {}\".format(loss_item_ans[0],loss_item_ans[1]))\n",
    "                    loss_ans = loss_fn(loss_item_ans[0], loss_item_ans[1])\n",
    "                \n",
    "                    # Loss for the prediction from the fused vector\n",
    "                    # I am keeping the loss same as bce but we can change it later for more predictions\n",
    "                    # loss_fused = interpolation loss\n",
    "                    #print(\"shape of loss_item_interp[0] is {} and of loss_item_interp[1] is {}\".format(loss_item_interp[0],loss_item_interp[1]))\n",
    "                    loss_interp = loss_fn(loss_item_interp[0], loss_item_interp[1])\n",
    "                    \n",
    "                    # we also need to multiply this fused loss by a hyperparameter alpha\n",
    "                    # put the alpha in the config and uncomment the following line\n",
    "                    loss_interp *= __C.ALPHA\n",
    "                    loss += loss_ans + loss_interp\n",
    "\n",
    "                    print(\"\\n----------  FIRST LOSS  --------\")\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    params1 = list(net.parameters())\n",
    "                    '''\n",
    "                    lays = 0\n",
    "                    for name, i in net.named_parameters():\n",
    "                        print(name, end = ' ')\n",
    "                        if (i.requires_grad == False):\n",
    "                            print (\"WWWWWWWWWWWWHHHHHHHHHHHAAAAAAAAAAAAAATTTTTTTTTTTTTTTTTTTT\")\n",
    "                        if ((i.grad != 0).sum() == 0):\n",
    "                            print(' NOT UPDATING')\n",
    "                            lays += 1\n",
    "                        else:\n",
    "                            print(' ')\n",
    "                    print (\"Gradient not updating in: \", lays, ' of total: ', len(params1))\n",
    "                    '''\n",
    "\n",
    "\n",
    "\n",
    "                    if (__C.WITH_FUSION_LOSS):\n",
    "\n",
    "                        # Now calculate the fusion loss\n",
    "                        #1. Higher loss for higher distance between vectors predicted\n",
    "                        # by different models for same example\n",
    "\n",
    "                        dist_calc = (z_img_ques - z_ans).pow(2).sum(1).sqrt()\n",
    "                        #print(\"Count of distances being clipped (true is clipped): \", np.unique((dist_calc > __C.CAP_DIST).cpu().numpy(), return_counts=True))\n",
    "\n",
    "                        '''\n",
    "                        loss_fusion = torch.min(\n",
    "                                torch.tensor(__C.CAP_DIST).cuda(),\n",
    "                                dist_calc\n",
    "                                ).mean()\n",
    "\n",
    "                        #2. Lower loss for more distance between two pred vectors of same model\n",
    "                        loss_fusion -= torch.min(\n",
    "                                torch.tensor(__C.CAP_DIST).cuda(), \n",
    "                                torch.pdist(z_img_ques, 2)\n",
    "                                ).mean() \n",
    "\n",
    "                        loss_fusion -= torch.min(\n",
    "                                torch.tensor(__C.CAP_DIST).cuda(), \n",
    "                                torch.pdist(z_ans, 2)\n",
    "                                ).mean() \n",
    "                        '''\n",
    "\n",
    "                        loss_fusion = dist_calc.mean()\n",
    "\n",
    "                        #2. Lower loss for more distance between two pred vectors of same model\n",
    "                        '''\n",
    "                        calculating pairwise intra distance on same type questions\n",
    "                        '''\n",
    "                        '''\n",
    "                        types = ['other', 'yes/no', 'number']\n",
    "                        for i in range(3):\n",
    "                            j = (i+1)%3\n",
    "                            indices_i = [k for k, val in enumerate(ques_type) if val == types[i]]\n",
    "                            indices_j = [k for k, val in enumerate(ques_type) if val == types[j]]\n",
    "                            if ((indices_i != []) and (indices_j != [])):\n",
    "                                loss_fusion -= torch.cdist(z_img_ques[indices_i], z_img_ques[indices_j]).mean()\n",
    "                                loss_fusion -= torch.cdist(z_ans[indices_i], z_ans[indices_j]).mean()\n",
    "                            if (indices_i != []):\n",
    "                                loss_fusion += torch.pdist(z_img_ques[indices_i], 2).mean()\n",
    "                                loss_fusion += torch.pdist(z_ans[indices_i], 2).mean()\n",
    "                        '''\n",
    "                        loss_fusion -= torch.pdist(z_img_ques, 2).mean() \n",
    "\n",
    "                        loss_fusion -= torch.pdist(z_ans, 2).mean() \n",
    "\n",
    "\n",
    "                        # Multiply the loss fusion with hyperparameter beta\n",
    "                        loss_fusion *= __C.BETA\n",
    "\n",
    "                        #print('fusion loss is : {}'.format(loss_fusion))\n",
    "\n",
    "                        print(\"\\n----------  FUSION LOSS  --------\")\n",
    "                        optim.zero_grad()\n",
    "                        loss_fusion.backward(retain_graph=True)\n",
    "                        params2 = list(net.parameters())\n",
    "                        lays = 0\n",
    "                        dot_sum = 0\n",
    "                        for idx, (name, i) in enumerate(net.named_parameters()):\n",
    "                            dot_sum += (params1[idx] * params2[idx]).sum()\n",
    "                            print(name, ' dot: ', (params1[idx].grad * i.grad).sum().item(), end = '   ')\n",
    "                            if ((i.grad != 0).sum() == 0):\n",
    "                                print(' NOT UPDATING')\n",
    "                                lays += 1\n",
    "                            else:\n",
    "                                print(' ')\n",
    "                        print (\"Gradient not updating in: \", lays, ' of total: ', len(params2))\n",
    "                        print(\"@@@@@@@@@@@@@@@@@@@ Overall dot product: \", dot_sum.item())\n",
    "\n",
    "                        loss += loss_fusion\n",
    "\n",
    "                loss /= __C.GRAD_ACCU_STEPS\n",
    "\n",
    "                print(\"\\n----------  MAIN LOSS  --------\")\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                params = list(net.parameters())\n",
    "                '''\n",
    "                lays = 0\n",
    "                for name, i in net.named_parameters():\n",
    "                    print(name, end = ' ')\n",
    "                    if (i.requires_grad == False):\n",
    "                        print (\"WWWWWWWWWWWWHHHHHHHHHHHAAAAAAAAAAAAAATTTTTTTTTTTTTTTTTTTT\")\n",
    "                    if ((i.grad != 0).sum() == 0):\n",
    "                        print(' NOT UPDATING')\n",
    "                        lays += 1\n",
    "                    else:\n",
    "                        print(' ')\n",
    "                print (\"Gradient not updating in: \", lays, ' of total: ', len(params))\n",
    "                '''\n",
    "\n",
    "                loss_tmp += loss.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "                loss_sum += loss.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "\n",
    "                # calculating temp loss of each type\n",
    "                if __C.WITH_ANSWER:\n",
    "                    loss_img_ques_tmp += loss_img_ques.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "                    loss_ans_tmp += loss_ans.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "                    loss_interp_tmp += loss_interp.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "                    if (__C.WITH_FUSION_LOSS):\n",
    "                        loss_fusion_tmp += loss_fusion.cpu().data.numpy() * __C.GRAD_ACCU_STEPS\n",
    "\n",
    "\n",
    "            if __C.VERBOSE:\n",
    "                if dataset_eval is not None:\n",
    "                    mode_str = __C.SPLIT['train'] + '->' + __C.SPLIT['val']\n",
    "                else:\n",
    "                    mode_str = __C.SPLIT['train'] + '->' + __C.SPLIT['test']\n",
    "\n",
    "                print(\"\\r[Version %s][Epoch %2d][Step %4d/%4d] Loss: %.4f [iq: %.4f,ans: %.4f,interp: %.4f,fusion: %.4f]\" % (\n",
    "                    __C.VERSION,\n",
    "                    epoch + 1,\n",
    "                    step,\n",
    "                    int(data_size / __C.BATCH_SIZE),\n",
    "                    loss_tmp / __C.SUB_BATCH_SIZE,\n",
    "                    loss_img_ques_tmp / __C.SUB_BATCH_SIZE,\n",
    "                    loss_ans_tmp / __C.SUB_BATCH_SIZE,\n",
    "                    loss_interp_tmp / __C.SUB_BATCH_SIZE,\n",
    "                    loss_fusion_tmp / __C.SUB_BATCH_SIZE\n",
    "                ), end = '          ')\n",
    "\n",
    "            # Gradient norm clipping\n",
    "            if __C.GRAD_NORM_CLIP > 0:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    net.parameters(),\n",
    "                    __C.GRAD_NORM_CLIP\n",
    "                )\n",
    "\n",
    "            # Save the gradient information\n",
    "            for name in range(len(named_params)):\n",
    "                norm_v = torch.norm(named_params[name][1].grad).cpu().data.numpy() \\\n",
    "                    if named_params[name][1].grad is not None else 0\n",
    "                grad_norm[name] += norm_v * __C.GRAD_ACCU_STEPS\n",
    "                # print('Param %-3s Name %-80s Grad_Norm %-20s'%\n",
    "                #       (str(grad_wt),\n",
    "                #        params[grad_wt][0],\n",
    "                #        str(norm_v)))\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "        time_end = time.time()\n",
    "        elapse_time = time_end-time_start\n",
    "        print('Finished in {}s'.format(int(elapse_time)))\n",
    "        epoch_finish = epoch + 1\n",
    "\n",
    "        # Save checkpoint\n",
    "        if __C.N_GPU > 1:\n",
    "            state = {\n",
    "                'state_dict': net.module.state_dict(),\n",
    "                'optimizer': optim.optimizer.state_dict(),\n",
    "                'lr_base': optim.lr_base,\n",
    "                'epoch': epoch_finish\n",
    "            }\n",
    "        else:\n",
    "            state = {\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optim.optimizer.state_dict(),\n",
    "                'lr_base': optim.lr_base,\n",
    "                'epoch': epoch_finish\n",
    "            }\n",
    "        torch.save(\n",
    "            state,\n",
    "            __C.CKPTS_PATH +\n",
    "            '/ckpt_' + __C.VERSION +\n",
    "            '/epoch' + str(epoch_finish) +\n",
    "            '.pkl'\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        wandb.save(\n",
    "            __C.CKPTS_PATH +\n",
    "            '/ckpt_' + __C.VERSION +\n",
    "            '/epoch' + str(epoch_finish) +\n",
    "            '.h5'\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        # Logging\n",
    "        logfile = open(\n",
    "            __C.LOG_PATH +\n",
    "            '/log_run_' + __C.VERSION + '.txt',\n",
    "            'a+'\n",
    "        )\n",
    "        logfile.write(\n",
    "            'Epoch: ' + str(epoch_finish) +\n",
    "            ', Loss: ' + str(loss_sum / data_size) +\n",
    "            ', Lr: ' + str(optim._rate) + '\\n' +\n",
    "            'Elapsed time: ' + str(int(elapse_time)) + \n",
    "            ', Speed(s/batch): ' + str(elapse_time / step) +\n",
    "            '\\n\\n'\n",
    "        )\n",
    "        logfile.close()\n",
    "\n",
    "        '''\n",
    "        wandb.log({\n",
    "            'Loss': float(loss_sum / data_size),\n",
    "            'Learning Rate': optim._rate,\n",
    "            'Elapsed time': int(elapse_time) \n",
    "            })\n",
    "        '''\n",
    "\n",
    "        # ---------------------------------------------- #\n",
    "        # ---- Create visualizations in new processes----#\n",
    "        # ---------------------------------------------- #\n",
    "        dic = {}\n",
    "        dic['version'] = __C.VERSION\n",
    "        dic['epoch'] = epoch \n",
    "        dic['num_samples'] = 1000\n",
    "\n",
    "        p = Pool(processes= 1)\n",
    "        p.map_async(vis_func, (dic, ))\n",
    "        p.close()\n",
    "\n",
    "        # Eval after every epoch\n",
    "        epoch_dict = {\n",
    "                'current_epoch': epoch\n",
    "                }\n",
    "        __C.add_args(epoch_dict)\n",
    "        if dataset_eval is not None:\n",
    "            test_engine(\n",
    "                __C,\n",
    "                dataset_eval,\n",
    "                state_dict=net.state_dict(),\n",
    "                validation=True,\n",
    "                epoch = 0\n",
    "            )\n",
    "        p.join()\n",
    "\n",
    "        # if self.__C.VERBOSE:\n",
    "        #     logfile = open(\n",
    "        #         self.__C.LOG_PATH +\n",
    "        #         '/log_run_' + self.__C.VERSION + '.txt',\n",
    "        #         'a+'\n",
    "        #     )\n",
    "        #     for name in range(len(named_params)):\n",
    "        #         logfile.write(\n",
    "        #             'Param %-3s Name %-80s Grad_Norm %-25s\\n' % (\n",
    "        #                 str(name),\n",
    "        #                 named_params[name][0],\n",
    "        #                 str(grad_norm[name] / data_size * self.__C.BATCH_SIZE)\n",
    "        #             )\n",
    "        #         )\n",
    "        #     logfile.write('\\n')\n",
    "        #     logfile.close()\n",
    "\n",
    "        loss_sum = 0\n",
    "        grad_norm = np.zeros(len(named_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing log file........\n",
      "Finished!\n",
      "\n",
      "Model being used is baseline_wa\n",
      "\n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2467.458740234375    \n",
      "module.ans_embedding.weight  dot:  1037403.5    \n",
      "module.lstm.weight_ih_l0  dot:  3137748.5    \n",
      "module.lstm.weight_hh_l0  dot:  127333.890625    \n",
      "module.lstm.bias_ih_l0  dot:  264328.875    \n",
      "module.lstm.bias_hh_l0  dot:  264328.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  169729856.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16656.427734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  13701990.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  13701990.0    \n",
      "module.adapter.frcn_linear.weight  dot:  4664240128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  3535324.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6377942.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5015.58203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5163240.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  566561792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  4987629.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.5960307121276855    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7108586430549622    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.7759363651275635    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4133068.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  4987629.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  253.10409545898438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  107.0810775756836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  210.26290893554688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  135811664.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  499801536.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452228.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    0/6933] Loss: 6314.4160 [iq: 2168.6167,ans: 2168.8389,interp: 2168.7563,fusion: -191.7957]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3050.122314453125    \n",
      "module.ans_embedding.weight  dot:  1191993.375    \n",
      "module.lstm.weight_ih_l0  dot:  3204425.0    \n",
      "module.lstm.weight_hh_l0  dot:  127495.671875    \n",
      "module.lstm.bias_ih_l0  dot:  269033.625    \n",
      "module.lstm.bias_hh_l0  dot:  269033.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  169677536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22655.298828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12968842.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12968842.0    \n",
      "module.adapter.frcn_linear.weight  dot:  4610907136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  3422976.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5236420.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4156.52392578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4460927.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  548817856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  5108172.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.858088731765747    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.62126225233078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.9660677909851074    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4196760.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  5108172.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  320.1690673828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  115.88874053955078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  287.7069396972656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  133560504.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  478663168.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452229.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    1/6933] Loss: 6284.2856 [iq: 2166.3787,ans: 2168.2852,interp: 2167.2864,fusion: -217.6648]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2571.930908203125    \n",
      "module.ans_embedding.weight  dot:  1040678.6875    \n",
      "module.lstm.weight_ih_l0  dot:  3370157.0    \n",
      "module.lstm.weight_hh_l0  dot:  132259.8125    \n",
      "module.lstm.bias_ih_l0  dot:  283698.6875    \n",
      "module.lstm.bias_hh_l0  dot:  283698.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  159871728.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20049.67578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12905580.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12905580.0    \n",
      "module.adapter.frcn_linear.weight  dot:  4591891456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  3398133.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4106588.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3177.63134765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3105690.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  545655296.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  5444596.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.463566780090332    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.351863980293274    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.4666223526000977    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4424247.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  5444596.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  116.21351623535156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.265342712402344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  122.53897094726562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  129531808.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  481778752.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452229.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    2/6933] Loss: 6258.1079 [iq: 2164.3542,ans: 2167.7258,interp: 2166.8203,fusion: -240.7923]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2578.683837890625    \n",
      "module.ans_embedding.weight  dot:  980014.125    \n",
      "module.lstm.weight_ih_l0  dot:  3149125.5    \n",
      "module.lstm.weight_hh_l0  dot:  126602.09375    \n",
      "module.lstm.bias_ih_l0  dot:  268419.1875    \n",
      "module.lstm.bias_hh_l0  dot:  268419.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  155897408.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26415.05859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12247695.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12247695.0    \n",
      "module.adapter.frcn_linear.weight  dot:  4048962304.0    \n",
      "module.adapter.frcn_linear.bias  dot:  2936593.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3802498.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2708.427001953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2880996.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  487091584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  5117106.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.565868377685547    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.4469223022460938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.652963638305664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4161486.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  5117106.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  261.4560852050781    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  97.22456359863281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  246.71456909179688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  128860296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  454698240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452229.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step    3/6933] Loss: 6225.2241 [iq: 2162.2109,ans: 2167.1626,interp: 2163.0522,fusion: -267.2014]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2848.955810546875    \n",
      "module.ans_embedding.weight  dot:  1162367.75    \n",
      "module.lstm.weight_ih_l0  dot:  3254530.0    \n",
      "module.lstm.weight_hh_l0  dot:  121089.1875    \n",
      "module.lstm.bias_ih_l0  dot:  268234.0    \n",
      "module.lstm.bias_hh_l0  dot:  268234.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  175978912.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18822.408203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11960300.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11960300.0    \n",
      "module.adapter.frcn_linear.weight  dot:  3692708352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  2750600.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3420646.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3041.26220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2765592.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  455767936.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  5241241.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.263326644897461    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2750053405761719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.196409225463867    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4251928.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  5241241.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  166.5120849609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.98420715332031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  167.30831909179688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  135901824.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  455157472.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452229.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    4/6933] Loss: 6208.3447 [iq: 2160.3394,ans: 2166.5947,interp: 2162.3242,fusion: -280.9134]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2987.665283203125    \n",
      "module.ans_embedding.weight  dot:  784826.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3262037.0    \n",
      "module.lstm.weight_hh_l0  dot:  131367.1875    \n",
      "module.lstm.bias_ih_l0  dot:  267130.40625    \n",
      "module.lstm.bias_hh_l0  dot:  267130.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  142371312.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39588.390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11587375.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11587375.0    \n",
      "module.adapter.frcn_linear.weight  dot:  3168845824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  2433926.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3935173.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3826.073974609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3516449.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  394235264.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  5036899.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.974569797515869    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.1311659812927246    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.1526906490325928    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4242683.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  5036899.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  557.7421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  209.253173828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  407.76007080078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  116948808.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  434069312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452230.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    5/6933] Loss: 6167.1250 [iq: 2158.4062,ans: 2166.0034,interp: 2164.9546,fusion: -322.2394]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2619.34912109375    \n",
      "module.ans_embedding.weight  dot:  603707.0    \n",
      "module.lstm.weight_ih_l0  dot:  2836158.75    \n",
      "module.lstm.weight_hh_l0  dot:  113928.203125    \n",
      "module.lstm.bias_ih_l0  dot:  233235.078125    \n",
      "module.lstm.bias_hh_l0  dot:  233235.078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  136716416.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25731.9765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10913838.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10913838.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2695351552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1968361.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2674028.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2505.682373046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2240984.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  337889792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  4485445.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.638997554779053    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.094611644744873    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.101356029510498    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.967582315084655e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3748148.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  4485445.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  162.24111938476562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  78.39854431152344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  150.23770141601562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  114357872.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  410597888.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452230.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    6/6933] Loss: 6134.9194 [iq: 2155.9473,ans: 2165.3828,interp: 2162.8000,fusion: -349.2104]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2088.893310546875    \n",
      "module.ans_embedding.weight  dot:  769661.875    \n",
      "module.lstm.weight_ih_l0  dot:  2658365.0    \n",
      "module.lstm.weight_hh_l0  dot:  101630.6171875    \n",
      "module.lstm.bias_ih_l0  dot:  224712.53125    \n",
      "module.lstm.bias_hh_l0  dot:  224712.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  138026000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32417.33984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10492968.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10492968.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2400885248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1766197.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1522568.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1659.906494140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1480067.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  308043328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  4403289.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.84346866607666    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.1531949043273926    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.95466685295105    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3529711.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  4403289.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  268.48834228515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  122.42257690429688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  219.2092742919922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4784973245696165e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  116264528.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  405516960.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452230.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step    7/6933] Loss: 6121.9053 [iq: 2153.7622,ans: 2164.7441,interp: 2159.1809,fusion: -355.7815]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2608.907958984375    \n",
      "module.ans_embedding.weight  dot:  923786.875    \n",
      "module.lstm.weight_ih_l0  dot:  2856563.5    \n",
      "module.lstm.weight_hh_l0  dot:  113841.0625    \n",
      "module.lstm.bias_ih_l0  dot:  235744.703125    \n",
      "module.lstm.bias_hh_l0  dot:  235744.703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  146915008.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13212.015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10429733.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10429733.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2086927232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1586392.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1635136.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1688.2044677734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1314242.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  281460928.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  4340502.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.297996520996094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.0955734252929688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  9.024426460266113    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3618374.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  4340502.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  426.79193115234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  204.02767944335938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  409.6763610839844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  115848512.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  406707840.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452231.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    8/6933] Loss: 6113.0479 [iq: 2151.5779,ans: 2164.0862,interp: 2158.3604,fusion: -360.9764]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1756.5877685546875    \n",
      "module.ans_embedding.weight  dot:  986017.4375    \n",
      "module.lstm.weight_ih_l0  dot:  2612243.0    \n",
      "module.lstm.weight_hh_l0  dot:  100577.8046875    \n",
      "module.lstm.bias_ih_l0  dot:  217103.625    \n",
      "module.lstm.bias_hh_l0  dot:  217103.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  149514320.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1622.653564453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9598185.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9598185.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1862293120.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1385398.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  758068.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  757.5751953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  641454.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  258785760.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  4132477.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.890425682067871    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.6261868476867676    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.14737319946289    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.589928271845565e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3371770.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  4132477.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  155.4464111328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  65.04322814941406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  169.06446838378906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  116274208.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  395975392.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452231.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step    9/6933] Loss: 6094.8213 [iq: 2148.8477,ans: 2163.3818,interp: 2158.0884,fusion: -375.4965]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2156.912353515625    \n",
      "module.ans_embedding.weight  dot:  793124.6875    \n",
      "module.lstm.weight_ih_l0  dot:  2468613.0    \n",
      "module.lstm.weight_hh_l0  dot:  100191.875    \n",
      "module.lstm.bias_ih_l0  dot:  205005.1875    \n",
      "module.lstm.bias_hh_l0  dot:  205005.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  131538480.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8391.755859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9599708.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9599708.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1494377216.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1161967.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  581980.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  574.5152587890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  529963.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  220264992.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  3839818.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.798351287841797    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.571681261062622    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.422172546386719    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3220859.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  3839818.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  62.631202697753906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  25.328453063964844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  57.02874755859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  111041696.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  383373184.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452232.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   10/6933] Loss: 6068.8794 [iq: 2146.4744,ans: 2162.6533,interp: 2151.4199,fusion: -391.6683]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1964.7938232421875    \n",
      "module.ans_embedding.weight  dot:  1340457.0    \n",
      "module.lstm.weight_ih_l0  dot:  2283598.0    \n",
      "module.lstm.weight_hh_l0  dot:  82165.171875    \n",
      "module.lstm.bias_ih_l0  dot:  187124.828125    \n",
      "module.lstm.bias_hh_l0  dot:  187124.828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  150529504.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18789.380859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9459734.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9459734.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1335380736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1014291.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  675128.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  653.911376953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  668552.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  201563872.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  3642085.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.269576072692871    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.150482654571533    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.666391372680664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2946279.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  3642085.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  77.60845947265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.02891540527344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  58.30206298828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9122126104775816e-10    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.attflat_ans.linear_merge.weight  dot:  119186360.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  364977792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452232.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   11/6933] Loss: 6088.2695 [iq: 2142.7622,ans: 2161.8909,interp: 2159.2998,fusion: -375.6831]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1646.4697265625    \n",
      "module.ans_embedding.weight  dot:  619553.8125    \n",
      "module.lstm.weight_ih_l0  dot:  2327989.0    \n",
      "module.lstm.weight_hh_l0  dot:  100888.296875    \n",
      "module.lstm.bias_ih_l0  dot:  196109.484375    \n",
      "module.lstm.bias_hh_l0  dot:  196109.484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  124105168.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11577.7119140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8967620.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8967620.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1334906624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1012798.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  684124.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  619.58056640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  610329.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  189774400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  3480880.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.830597877502441    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.6761322021484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.866145133972168    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.469580509678053e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2959183.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  3480880.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  247.90328979492188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  82.7793197631836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  234.68206787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  103113912.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  367196544.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452233.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   12/6933] Loss: 6030.2446 [iq: 2140.1819,ans: 2160.9761,interp: 2143.0391,fusion: -413.9527]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2056.16845703125    \n",
      "module.ans_embedding.weight  dot:  695132.25    \n",
      "module.lstm.weight_ih_l0  dot:  2082221.875    \n",
      "module.lstm.weight_hh_l0  dot:  78980.2890625    \n",
      "module.lstm.bias_ih_l0  dot:  168406.25    \n",
      "module.lstm.bias_hh_l0  dot:  168406.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113607912.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26205.16015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8701975.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8701975.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1183500032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  869963.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  631644.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  593.4473876953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  545023.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  167271888.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  3079582.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.700811386108398    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.460864782333374    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.55523681640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2577659.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  3079582.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  293.44158935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  186.76206970214844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  318.97161865234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98182976.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  372443136.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452233.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   13/6933] Loss: 6031.6772 [iq: 2135.2466,ans: 2160.0952,interp: 2158.6892,fusion: -422.3535]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1603.1182861328125    \n",
      "module.ans_embedding.weight  dot:  1108293.625    \n",
      "module.lstm.weight_ih_l0  dot:  1890041.25    \n",
      "module.lstm.weight_hh_l0  dot:  67413.4921875    \n",
      "module.lstm.bias_ih_l0  dot:  155687.296875    \n",
      "module.lstm.bias_hh_l0  dot:  155687.296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  139642304.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34550.96875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9208660.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9208660.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1045492800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  761083.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  502926.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  444.38824462890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  471023.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  156033456.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2923760.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.580434799194336    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7713465690612793    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.067096710205078    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.304911899704166e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2346338.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2923760.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  446.3094787597656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  189.87191772460938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  415.23480224609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  116351352.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  376337408.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452234.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   14/6933] Loss: 6019.1787 [iq: 2131.0337,ans: 2159.1816,interp: 2137.4788,fusion: -408.5152]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1460.454345703125    \n",
      "module.ans_embedding.weight  dot:  993880.625    \n",
      "module.lstm.weight_ih_l0  dot:  1873810.625    \n",
      "module.lstm.weight_hh_l0  dot:  76071.140625    \n",
      "module.lstm.bias_ih_l0  dot:  157183.21875    \n",
      "module.lstm.bias_hh_l0  dot:  157183.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  119377872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  58208.578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8829054.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8829054.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1018712320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  755016.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  967355.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1092.28466796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1020936.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  140165616.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2692530.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16.327938079833984    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.226017475128174    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.834589004516602    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.105604745063829e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2266504.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2692530.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  754.264892578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  339.89288330078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  811.471435546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.035971308738226e-09    \n",
      "module.attflat_ans.linear_merge.weight  dot:  102093272.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  351265600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452234.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   15/6933] Loss: 6002.6875 [iq: 2126.3984,ans: 2158.1035,interp: 2137.9878,fusion: -419.8025]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1689.2744140625    \n",
      "module.ans_embedding.weight  dot:  975380.375    \n",
      "module.lstm.weight_ih_l0  dot:  1689110.5    \n",
      "module.lstm.weight_hh_l0  dot:  66550.359375    \n",
      "module.lstm.bias_ih_l0  dot:  142686.53125    \n",
      "module.lstm.bias_hh_l0  dot:  142686.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  133229296.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22718.6953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9003479.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9003479.0    \n",
      "module.adapter.frcn_linear.weight  dot:  888745536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  674828.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  346146.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  356.85870361328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  296399.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  128956080.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2636766.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.534077644348145    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.995046615600586    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.146899223327637    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.9960036108132044e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2169987.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2636766.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  234.07586669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  108.22759246826172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  217.55886840820312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  107146384.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  368522688.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452235.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   16/6933] Loss: 5999.0786 [iq: 2122.0225,ans: 2156.9958,interp: 2146.8530,fusion: -426.7924]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1860.9346923828125    \n",
      "module.ans_embedding.weight  dot:  903021.4375    \n",
      "module.lstm.weight_ih_l0  dot:  1708811.75    \n",
      "module.lstm.weight_hh_l0  dot:  61793.0625    \n",
      "module.lstm.bias_ih_l0  dot:  140789.671875    \n",
      "module.lstm.bias_hh_l0  dot:  140789.671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  121129184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  73415.6875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9268629.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9268629.0    \n",
      "module.adapter.frcn_linear.weight  dot:  916783296.0    \n",
      "module.adapter.frcn_linear.bias  dot:  710342.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  312512.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  322.80462646484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  254240.703125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  122366192.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2576451.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.62615966796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.987046957015991    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11.59041976928711    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0880185641326534e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2114483.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2576451.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  819.6831665039062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  381.2010498046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  737.1634521484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  107397104.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  361679424.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452235.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   17/6933] Loss: 5996.4307 [iq: 2117.1958,ans: 2155.7993,interp: 2151.8750,fusion: -428.4393]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1950.073486328125    \n",
      "module.ans_embedding.weight  dot:  682745.25    \n",
      "module.lstm.weight_ih_l0  dot:  1670952.0    \n",
      "module.lstm.weight_hh_l0  dot:  64539.95703125    \n",
      "module.lstm.bias_ih_l0  dot:  133336.09375    \n",
      "module.lstm.bias_hh_l0  dot:  133336.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  116802384.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5981.0986328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8380544.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8380544.0    \n",
      "module.adapter.frcn_linear.weight  dot:  906255296.0    \n",
      "module.adapter.frcn_linear.bias  dot:  667447.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  540407.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  521.3772583007812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  541494.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  116794560.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2326329.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.841154098510742    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.2895913124084473    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11.66512680053711    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2021593.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2326329.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  109.16813659667969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.288021087646484    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  112.47160339355469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98527048.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  348489984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452236.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   18/6933] Loss: 5942.7241 [iq: 2110.7720,ans: 2154.4526,interp: 2125.2322,fusion: -447.7325]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1347.3170166015625    \n",
      "module.ans_embedding.weight  dot:  917145.875    \n",
      "module.lstm.weight_ih_l0  dot:  1416447.375    \n",
      "module.lstm.weight_hh_l0  dot:  58455.8828125    \n",
      "module.lstm.bias_ih_l0  dot:  119702.7109375    \n",
      "module.lstm.bias_hh_l0  dot:  119702.7109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  118005328.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10329.3779296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8505372.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8505372.0    \n",
      "module.adapter.frcn_linear.weight  dot:  686958400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  519394.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  575894.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  534.468017578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  561697.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  96784320.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2073767.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.065539360046387    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.214418649673462    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.148640632629395    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1766777.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2073767.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  239.33489990234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  97.9614028930664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  223.43682861328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  96963200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  355538624.0    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452236.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   19/6933] Loss: 5944.6069 [iq: 2102.2744,ans: 2153.1033,interp: 2134.0044,fusion: -444.7751]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1341.365478515625    \n",
      "module.ans_embedding.weight  dot:  782617.0625    \n",
      "module.lstm.weight_ih_l0  dot:  1460323.5    \n",
      "module.lstm.weight_hh_l0  dot:  55031.03125    \n",
      "module.lstm.bias_ih_l0  dot:  119237.96875    \n",
      "module.lstm.bias_hh_l0  dot:  119237.96875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  108103464.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19084.98828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8063036.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8063036.0    \n",
      "module.adapter.frcn_linear.weight  dot:  831409728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  617077.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  371874.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  305.6727600097656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  334133.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  101089424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2101163.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.51014518737793    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.502999782562256    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.696927070617676    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.304911899704166e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1758235.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2101163.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  460.6484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  205.3579559326172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  464.76751708984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  93817200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  339482688.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452237.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   20/6933] Loss: 5919.8428 [iq: 2095.7041,ans: 2151.4595,interp: 2126.9868,fusion: -454.3076]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1479.7318115234375    \n",
      "module.ans_embedding.weight  dot:  798102.6875    \n",
      "module.lstm.weight_ih_l0  dot:  1426162.75    \n",
      "module.lstm.weight_hh_l0  dot:  54672.12890625    \n",
      "module.lstm.bias_ih_l0  dot:  115761.5859375    \n",
      "module.lstm.bias_hh_l0  dot:  115761.5859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  111115456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22017.3828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7710409.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7710409.0    \n",
      "module.adapter.frcn_linear.weight  dot:  847385408.0    \n",
      "module.adapter.frcn_linear.bias  dot:  633533.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  414103.03125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  434.1341552734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  359791.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  98448128.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  2034636.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.168153762817383    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7076516151428223    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.86292839050293    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1738804.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  2034636.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  511.613037109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  223.0505828857422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  574.2246704101562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  97289056.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  338288160.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452238.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   21/6933] Loss: 5920.3311 [iq: 2086.1255,ans: 2149.7761,interp: 2143.4602,fusion: -459.0306]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1118.702392578125    \n",
      "module.ans_embedding.weight  dot:  923409.4375    \n",
      "module.lstm.weight_ih_l0  dot:  1321505.25    \n",
      "module.lstm.weight_hh_l0  dot:  54370.078125    \n",
      "module.lstm.bias_ih_l0  dot:  105913.8828125    \n",
      "module.lstm.bias_hh_l0  dot:  105913.8828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  129987984.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17610.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8354738.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8354738.0    \n",
      "module.adapter.frcn_linear.weight  dot:  803937728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  583304.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  740636.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  747.7102661132812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  619097.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  94045168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1831513.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.537043571472168    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.6800892353057861    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.663181304931641    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1642931.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1831513.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  271.7110595703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  115.31849670410156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  227.20993041992188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  105358944.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  349536736.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452238.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   22/6933] Loss: 5847.0244 [iq: 2073.3767,ans: 2148.0063,interp: 2080.7747,fusion: -455.1340]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1094.8026123046875    \n",
      "module.ans_embedding.weight  dot:  537528.375    \n",
      "module.lstm.weight_ih_l0  dot:  1400365.25    \n",
      "module.lstm.weight_hh_l0  dot:  57415.2421875    \n",
      "module.lstm.bias_ih_l0  dot:  115543.765625    \n",
      "module.lstm.bias_hh_l0  dot:  115543.765625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  90330480.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6665.65576171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7469461.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7469461.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1053623232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  748775.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  724025.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  640.7681884765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  654401.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  100010840.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1869640.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16.707895278930664    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.573899269104004    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.377185821533203    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1644198.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1869640.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  146.22769165039062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.88326644897461    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  135.25791931152344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  80843104.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  331146240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452239.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   23/6933] Loss: 5817.0576 [iq: 2061.3066,ans: 2146.0342,interp: 2078.8979,fusion: -469.1809]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1254.58447265625    \n",
      "module.ans_embedding.weight  dot:  757556.3125    \n",
      "module.lstm.weight_ih_l0  dot:  1317795.375    \n",
      "module.lstm.weight_hh_l0  dot:  52314.203125    \n",
      "module.lstm.bias_ih_l0  dot:  105009.75    \n",
      "module.lstm.bias_hh_l0  dot:  105009.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113127920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10428.49609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7328599.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7328599.0    \n",
      "module.adapter.frcn_linear.weight  dot:  968422016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  716369.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  650874.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  581.8045043945312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  538286.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  93506896.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1782783.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.028871536254883    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.3185276985168457    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.140020370483398    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1595906.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1782783.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  214.00933837890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  98.83792877197266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  204.0961151123047    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98734144.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  332724160.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452240.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   24/6933] Loss: 5807.0044 [iq: 2046.2705,ans: 2143.8147,interp: 2085.4932,fusion: -468.5736]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1215.4359130859375    \n",
      "module.ans_embedding.weight  dot:  670385.8125    \n",
      "module.lstm.weight_ih_l0  dot:  1429060.0    \n",
      "module.lstm.weight_hh_l0  dot:  64813.74609375    \n",
      "module.lstm.bias_ih_l0  dot:  119658.5703125    \n",
      "module.lstm.bias_hh_l0  dot:  119658.5703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  97175280.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5451.16845703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7235366.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7235366.5    \n",
      "module.adapter.frcn_linear.weight  dot:  1230692864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  894453.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  980242.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  898.894287109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  928369.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  105692848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1844023.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.750221252441406    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.4512929916381836    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.314809799194336    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1667045.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1844023.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  153.00547790527344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.54681396484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  132.74847412109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.204139258945361e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  88029120.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  332173088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452240.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   25/6933] Loss: 5756.8716 [iq: 2026.2091,ans: 2141.5667,interp: 2053.2275,fusion: -464.1312]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1142.3626708984375    \n",
      "module.ans_embedding.weight  dot:  566360.25    \n",
      "module.lstm.weight_ih_l0  dot:  1338707.0    \n",
      "module.lstm.weight_hh_l0  dot:  54651.2265625    \n",
      "module.lstm.bias_ih_l0  dot:  108388.3125    \n",
      "module.lstm.bias_hh_l0  dot:  108388.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  93956512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9342.9375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7125654.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7125654.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1175774720.0    \n",
      "module.adapter.frcn_linear.bias  dot:  847671.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  422797.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  338.01104736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  349906.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  101641888.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1736537.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.035865783691406    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.4986000061035156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.198112487792969    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.298783551348606e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1548417.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1736537.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  95.38321685791016    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.72821044921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  85.02104187011719    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  85578560.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  325627520.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452241.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   26/6933] Loss: 5734.3608 [iq: 2008.6361,ans: 2139.0537,interp: 2058.3772,fusion: -471.7060]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1189.237060546875    \n",
      "module.ans_embedding.weight  dot:  727901.4375    \n",
      "module.lstm.weight_ih_l0  dot:  1424678.375    \n",
      "module.lstm.weight_hh_l0  dot:  66682.890625    \n",
      "module.lstm.bias_ih_l0  dot:  119899.453125    \n",
      "module.lstm.bias_hh_l0  dot:  119899.453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  95985872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18412.833984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7001770.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7001770.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1510531584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1112113.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  631939.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  626.34228515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  621988.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  111493392.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1838085.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.41732120513916    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.936021327972412    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.408029556274414    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1686048.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1838085.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  256.8960266113281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  105.03650665283203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  210.8936004638672    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7909229654833325e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  91186704.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  320290112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452242.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   27/6933] Loss: 5730.8154 [iq: 1985.9347,ans: 2136.3118,interp: 2075.9507,fusion: -467.3818]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  976.1968994140625    \n",
      "module.ans_embedding.weight  dot:  840209.75    \n",
      "module.lstm.weight_ih_l0  dot:  1321334.75    \n",
      "module.lstm.weight_hh_l0  dot:  59965.171875    \n",
      "module.lstm.bias_ih_l0  dot:  109348.09375    \n",
      "module.lstm.bias_hh_l0  dot:  109348.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  106162088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20826.466796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7437391.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7437391.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1446982272.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1050143.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  742578.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  667.48974609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  635529.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  114146496.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1742115.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.122645378112793    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8906329870224    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.266216278076172    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0892176877396196e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1610429.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1742115.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  288.92340087890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  124.76812744140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  269.1058044433594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.029185791092459e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  97280112.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  334310112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452243.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   28/6933] Loss: 5703.2417 [iq: 1955.1002,ans: 2133.2944,interp: 2081.1006,fusion: -466.2534]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1337.1788330078125    \n",
      "module.ans_embedding.weight  dot:  595777.625    \n",
      "module.lstm.weight_ih_l0  dot:  1396836.875    \n",
      "module.lstm.weight_hh_l0  dot:  61440.58984375    \n",
      "module.lstm.bias_ih_l0  dot:  114678.46875    \n",
      "module.lstm.bias_hh_l0  dot:  114678.46875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  103625080.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15308.3212890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7229631.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7229631.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1794793984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1319706.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  900471.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  807.2538452148438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  752278.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  133354464.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1902125.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.349618911743164    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.5148487091064453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11.384967803955078    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1741143.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1902125.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  320.8426513671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  125.85591888427734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  283.2808837890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  96789000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  328454720.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452244.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   29/6933] Loss: 5623.4185 [iq: 1919.6770,ans: 2129.9346,interp: 2037.9976,fusion: -464.1907]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1128.550048828125    \n",
      "module.ans_embedding.weight  dot:  544561.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1326484.0    \n",
      "module.lstm.weight_hh_l0  dot:  60945.5546875    \n",
      "module.lstm.bias_ih_l0  dot:  109916.421875    \n",
      "module.lstm.bias_hh_l0  dot:  109916.421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  91737664.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8108.49560546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6897041.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6897041.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1972542464.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1409528.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  911961.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  759.2469482421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  737498.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  148575680.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1907224.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.776595115661621    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.4167263507843018    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.00723934173584    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1781124.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1907224.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  64.62329864501953    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.116085052490234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  63.518035888671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  91333120.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  336231104.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452245.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   30/6933] Loss: 5478.9014 [iq: 1881.7073,ans: 2126.7263,interp: 1930.5044,fusion: -460.0368]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1072.6932373046875    \n",
      "module.ans_embedding.weight  dot:  672629.875    \n",
      "module.lstm.weight_ih_l0  dot:  1304603.25    \n",
      "module.lstm.weight_hh_l0  dot:  57038.87890625    \n",
      "module.lstm.bias_ih_l0  dot:  107837.21875    \n",
      "module.lstm.bias_hh_l0  dot:  107837.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  100560224.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19623.142578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7474214.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7474214.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1992139904.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1399190.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1165578.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1046.9298095703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  997010.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  156709920.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1783740.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.179489135742188    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.9458810091018677    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.295867919921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.551115123125783e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1654210.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1783740.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  249.08689880371094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  139.36619567871094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  243.60342407226562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.1151436158106662e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98429048.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  339628032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452246.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   31/6933] Loss: 5394.5122 [iq: 1828.9491,ans: 2122.9958,interp: 1894.2913,fusion: -451.7242]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1040.481201171875    \n",
      "module.ans_embedding.weight  dot:  458182.21875    \n",
      "module.lstm.weight_ih_l0  dot:  1411630.0    \n",
      "module.lstm.weight_hh_l0  dot:  62375.890625    \n",
      "module.lstm.bias_ih_l0  dot:  115167.703125    \n",
      "module.lstm.bias_hh_l0  dot:  115167.703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  88965952.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20030.005859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7033453.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7033453.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2310346752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1635959.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2205683.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1712.046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1995605.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.7853275241795927e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  185890832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1968025.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.92210578918457    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.4471735954284668    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.063314437866211    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  1857120.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1968025.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  167.69772338867188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  63.63956832885742    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  144.39952087402344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.978719410544727e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  91130640.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  333862656.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452247.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   32/6933] Loss: 5540.6016 [iq: 1779.0928,ans: 2118.7417,interp: 2092.4221,fusion: -449.6551]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  923.3207397460938    \n",
      "module.ans_embedding.weight  dot:  606071.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1395035.625    \n",
      "module.lstm.weight_hh_l0  dot:  61983.25    \n",
      "module.lstm.bias_ih_l0  dot:  114872.59375    \n",
      "module.lstm.bias_hh_l0  dot:  114872.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  89642904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1455.6248779296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6765464.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6765464.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2221538304.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1601315.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1614170.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1243.49072265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1495808.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  190124736.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1887095.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.328452110290527    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.266818642616272    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.690662860870361    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.004086117172847e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1765792.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1887095.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  61.81834411621094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.8548641204834    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  61.62302780151367    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  90356368.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  328002176.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452248.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   33/6933] Loss: 5403.7734 [iq: 1728.1362,ans: 2114.3135,interp: 2006.5881,fusion: -445.2639]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1089.95166015625    \n",
      "module.ans_embedding.weight  dot:  429427.09375    \n",
      "module.lstm.weight_ih_l0  dot:  1377284.125    \n",
      "module.lstm.weight_hh_l0  dot:  65576.4453125    \n",
      "module.lstm.bias_ih_l0  dot:  111705.0703125    \n",
      "module.lstm.bias_hh_l0  dot:  111705.0703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  86694368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22180.052734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6990009.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6990009.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2364687360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1646885.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2088040.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1606.007568359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1958773.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  215321456.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1836950.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.978353023529053    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.175050973892212    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.563405513763428    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8673951274195133e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1823108.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1836950.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  421.712158203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  153.43856811523438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  438.20489501953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  93268384.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  340697984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452248.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   34/6933] Loss: 5250.3301 [iq: 1641.1619,ans: 2109.6470,interp: 1931.7969,fusion: -432.2753]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1422.8682861328125    \n",
      "module.ans_embedding.weight  dot:  691284.0625    \n",
      "module.lstm.weight_ih_l0  dot:  1375407.625    \n",
      "module.lstm.weight_hh_l0  dot:  62337.28125    \n",
      "module.lstm.bias_ih_l0  dot:  110602.09375    \n",
      "module.lstm.bias_hh_l0  dot:  110602.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113118336.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24329.65625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8304300.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8304300.5    \n",
      "module.adapter.frcn_linear.weight  dot:  2311968000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1650440.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2434689.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1703.3310546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2018622.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  233315424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1844445.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.3839786052703857    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8900930881500244    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.705942153930664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1819571.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1844445.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  229.80389404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  90.1346664428711    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  214.0038604736328    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  105225296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  355254912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452249.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   35/6933] Loss: 4947.0776 [iq: 1573.5723,ans: 2104.3428,interp: 1683.0997,fusion: -413.9371]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1007.2898559570312    \n",
      "module.ans_embedding.weight  dot:  685920.6875    \n",
      "module.lstm.weight_ih_l0  dot:  1241261.75    \n",
      "module.lstm.weight_hh_l0  dot:  60409.8203125    \n",
      "module.lstm.bias_ih_l0  dot:  106495.9609375    \n",
      "module.lstm.bias_hh_l0  dot:  106495.9609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  103895056.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24847.19921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8340705.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8340705.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2414509568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1688594.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2515456.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1840.0692138671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2322807.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  249076448.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1777409.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.508373260498047    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6223701238632202    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.5201568603515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1696202.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1777409.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  446.4348449707031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  188.97463989257812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  451.34808349609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2920687570149312e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  106641056.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  366759712.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452250.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   36/6933] Loss: 4716.7031 [iq: 1482.4446,ans: 2099.3018,interp: 1541.9613,fusion: -407.0047]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1038.39599609375    \n",
      "module.ans_embedding.weight  dot:  405691.59375    \n",
      "module.lstm.weight_ih_l0  dot:  1189380.25    \n",
      "module.lstm.weight_hh_l0  dot:  51913.8671875    \n",
      "module.lstm.bias_ih_l0  dot:  95953.109375    \n",
      "module.lstm.bias_hh_l0  dot:  95953.109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  84842864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21382.28125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7444068.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7444068.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2458828288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1593152.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4835849.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3502.9384765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4306042.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  273761472.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1593936.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.0185210704803467    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5628427863121033    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.161222457885742    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.672262990534364e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1554825.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1593936.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  245.30743408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  143.28350830078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  241.5872344970703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0090737962163985e-09    \n",
      "module.attflat_ans.linear_merge.weight  dot:  93121344.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  355335008.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452251.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   37/6933] Loss: 4516.6582 [iq: 1353.8641,ans: 2092.9966,interp: 1467.8823,fusion: -398.0847]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  974.8856811523438    \n",
      "module.ans_embedding.weight  dot:  545215.4375    \n",
      "module.lstm.weight_ih_l0  dot:  1111975.0    \n",
      "module.lstm.weight_hh_l0  dot:  53988.01953125    \n",
      "module.lstm.bias_ih_l0  dot:  91618.0625    \n",
      "module.lstm.bias_hh_l0  dot:  91618.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  96732880.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11362.2080078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7861628.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7861628.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2251926528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1464000.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3135972.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2163.43115234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2794841.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  279272320.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1479651.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.626076579093933    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.41899919509887695    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.5904736518859863    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1486605.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1479651.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.09613037109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  170.58261108398438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  342.270263671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  103397296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  368780992.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452252.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   38/6933] Loss: 4393.3970 [iq: 1254.3284,ans: 2086.2007,interp: 1435.0889,fusion: -382.2212]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  805.3101806640625    \n",
      "module.ans_embedding.weight  dot:  507428.625    \n",
      "module.lstm.weight_ih_l0  dot:  933117.5    \n",
      "module.lstm.weight_hh_l0  dot:  45042.69921875    \n",
      "module.lstm.bias_ih_l0  dot:  79108.09375    \n",
      "module.lstm.bias_hh_l0  dot:  79108.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  91135840.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9355.19921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7493705.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7493705.0    \n",
      "module.adapter.frcn_linear.weight  dot:  2150243328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1376264.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4142051.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2994.453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3564187.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  283953504.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1341535.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.8612533807754517    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5405120849609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.7945473194122314    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3877787807814457e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1328516.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1341535.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  262.49993896484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  120.16680908203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  242.31234741210938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  103703344.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  372473280.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452254.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   39/6933] Loss: 4496.8452 [iq: 1131.6833,ans: 2079.5066,interp: 1654.5547,fusion: -368.8996]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  735.8106079101562    \n",
      "module.ans_embedding.weight  dot:  400460.78125    \n",
      "module.lstm.weight_ih_l0  dot:  942227.625    \n",
      "module.lstm.weight_hh_l0  dot:  48615.38671875    \n",
      "module.lstm.bias_ih_l0  dot:  78618.7890625    \n",
      "module.lstm.bias_hh_l0  dot:  78618.7890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  83604200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4354.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7304452.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7304452.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1962364672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1348078.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3686647.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2592.94873046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3767365.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6575540939811617e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  279571968.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1302468.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.4962445497512817    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3581971228122711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.2627545595169067    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1351318.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1302468.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  108.46769714355469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  50.65522766113281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  111.53358459472656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  93570000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  368189312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452255.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   40/6933] Loss: 4563.8740 [iq: 1044.2026,ans: 2072.0156,interp: 1815.5927,fusion: -367.9370]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  651.276611328125    \n",
      "module.ans_embedding.weight  dot:  578279.3125    \n",
      "module.lstm.weight_ih_l0  dot:  908341.625    \n",
      "module.lstm.weight_hh_l0  dot:  45461.359375    \n",
      "module.lstm.bias_ih_l0  dot:  75718.453125    \n",
      "module.lstm.bias_hh_l0  dot:  75718.453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  98290592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3034.98095703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8065850.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8065850.5    \n",
      "module.adapter.frcn_linear.weight  dot:  1828093312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1256912.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4239511.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3063.985107421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3755295.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  285872800.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  1194724.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.2546809911727905    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3861745595932007    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0625226497650146    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1239122.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  1194724.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  39.79301452636719    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.455644607543945    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  34.8076286315918    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  108515064.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  388665216.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452256.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   41/6933] Loss: 4184.6543 [iq: 922.4713,ans: 2063.7415,interp: 1541.4542,fusion: -343.0127]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  630.3861694335938    \n",
      "module.ans_embedding.weight  dot:  466130.375    \n",
      "module.lstm.weight_ih_l0  dot:  684875.9375    \n",
      "module.lstm.weight_hh_l0  dot:  32139.693359375    \n",
      "module.lstm.bias_ih_l0  dot:  57066.6953125    \n",
      "module.lstm.bias_hh_l0  dot:  57066.6953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  93443840.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6104.73486328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7757794.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7757794.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1626536192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1037654.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4243944.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2701.4609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4565614.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  282906496.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  975920.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.315995216369629    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2559070587158203    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0965839624404907    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  985062.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  975920.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  110.287109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.27922821044922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  129.21202087402344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  106261592.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  382929248.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452257.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   42/6933] Loss: 3985.1199 [iq: 773.0341,ans: 2054.1919,interp: 1495.0764,fusion: -337.1823]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  550.5297241210938    \n",
      "module.ans_embedding.weight  dot:  478539.53125    \n",
      "module.lstm.weight_ih_l0  dot:  716599.125    \n",
      "module.lstm.weight_hh_l0  dot:  36297.94140625    \n",
      "module.lstm.bias_ih_l0  dot:  58236.125    \n",
      "module.lstm.bias_hh_l0  dot:  58236.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  93572424.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5643.291015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7988634.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7988634.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1506730880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  1003789.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3672662.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2423.517578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3756581.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  281418944.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  944452.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.5730854272842407    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.31526100635528564    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.1784822940826416    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1014560.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  944452.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  144.8953857421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.645389556884766    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  133.52984619140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  111433064.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  403032128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452258.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   43/6933] Loss: 3564.4695 [iq: 686.5832,ans: 2045.3898,interp: 1156.6779,fusion: -324.1815]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  594.7300415039062    \n",
      "module.ans_embedding.weight  dot:  295210.125    \n",
      "module.lstm.weight_ih_l0  dot:  565664.0    \n",
      "module.lstm.weight_hh_l0  dot:  30060.40234375    \n",
      "module.lstm.bias_ih_l0  dot:  46573.3203125    \n",
      "module.lstm.bias_hh_l0  dot:  46573.3203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  96629408.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5053.6669921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7820370.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7820370.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1341336960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  856229.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5350073.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3056.835693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6153590.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  269769856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  784095.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.4363598823547363    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.27851903438568115    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.1481412649154663    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  861157.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  784095.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  20.639497756958008    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.801280975341797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  20.296554565429688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.555378710501827e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  109847904.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  372632960.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452259.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   44/6933] Loss: 3395.6670 [iq: 561.3834,ans: 2033.1658,interp: 1120.8604,fusion: -319.7424]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  453.7129211425781    \n",
      "module.ans_embedding.weight  dot:  709015.5    \n",
      "module.lstm.weight_ih_l0  dot:  497002.5625    \n",
      "module.lstm.weight_hh_l0  dot:  22620.791015625    \n",
      "module.lstm.bias_ih_l0  dot:  40627.73046875    \n",
      "module.lstm.bias_hh_l0  dot:  40627.73046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  118349648.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15810.599609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9550000.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9550000.0    \n",
      "module.adapter.frcn_linear.weight  dot:  1201368576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  715829.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5016165.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2496.426513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5426367.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  270264352.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  669432.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.4281303882598877    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.355414479970932    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0142724514007568    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  702544.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  669432.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  330.9859619140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  115.04833221435547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  276.46600341796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  129340064.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  415677376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452261.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   45/6933] Loss: 2901.6592 [iq: 441.2739,ans: 2024.0430,interp: 731.3948,fusion: -295.0524]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  309.27020263671875    \n",
      "module.ans_embedding.weight  dot:  263593.78125    \n",
      "module.lstm.weight_ih_l0  dot:  436281.5625    \n",
      "module.lstm.weight_hh_l0  dot:  24729.71875    \n",
      "module.lstm.bias_ih_l0  dot:  35300.6328125    \n",
      "module.lstm.bias_hh_l0  dot:  35300.6328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  78070624.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13327.4580078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7279359.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7279359.5    \n",
      "module.adapter.frcn_linear.weight  dot:  1073623552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  629323.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6732603.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3086.36962890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8053758.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  248386560.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  560874.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.280663013458252    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.28646278381347656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.8496620655059814    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  654787.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  560874.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  246.11924743652344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  79.49360656738281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  253.71826171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  97650816.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  369793824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452262.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   46/6933] Loss: 2872.1672 [iq: 359.0984,ans: 2012.2535,interp: 802.6775,fusion: -301.8624]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  314.4257507324219    \n",
      "module.ans_embedding.weight  dot:  248952.03125    \n",
      "module.lstm.weight_ih_l0  dot:  381874.5625    \n",
      "module.lstm.weight_hh_l0  dot:  21462.53125    \n",
      "module.lstm.bias_ih_l0  dot:  31437.171875    \n",
      "module.lstm.bias_hh_l0  dot:  31437.171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  79899944.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4293.548828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7070406.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7070406.0    \n",
      "module.adapter.frcn_linear.weight  dot:  963402752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  580724.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5733210.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2489.574462890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6811471.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  240050640.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  515695.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.862679660320282    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.20072221755981445    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.8400572538375854    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.9960036108132044e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  595055.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  515695.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  71.80690002441406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.64691925048828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  53.73686218261719    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98517816.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  367880640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452263.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   47/6933] Loss: 2690.6287 [iq: 297.8965,ans: 1998.5774,interp: 694.0175,fusion: -299.8625]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  364.5921630859375    \n",
      "module.ans_embedding.weight  dot:  329084.8125    \n",
      "module.lstm.weight_ih_l0  dot:  327061.125    \n",
      "module.lstm.weight_hh_l0  dot:  16843.490234375    \n",
      "module.lstm.bias_ih_l0  dot:  26654.478515625    \n",
      "module.lstm.bias_hh_l0  dot:  26654.478515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  88463216.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31449.45703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7409390.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7409390.0    \n",
      "module.adapter.frcn_linear.weight  dot:  842280000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  487501.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7977711.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3062.03271484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9691511.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  225554304.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  433166.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.48250150680542    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.24706263840198517    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.4785041809082031    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.551115123125783e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  493850.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  433166.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  231.93756103515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.7947006225586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  225.37185668945312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  111613072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  367168736.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452264.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   48/6933] Loss: 2533.7229 [iq: 230.2714,ans: 1983.9813,interp: 604.3450,fusion: -284.8750]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  246.6448974609375    \n",
      "module.ans_embedding.weight  dot:  305345.90625    \n",
      "module.lstm.weight_ih_l0  dot:  290736.28125    \n",
      "module.lstm.weight_hh_l0  dot:  15499.234375    \n",
      "module.lstm.bias_ih_l0  dot:  23507.736328125    \n",
      "module.lstm.bias_hh_l0  dot:  23507.736328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  88800320.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32398.6953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8071837.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8071837.0    \n",
      "module.adapter.frcn_linear.weight  dot:  760909696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  440118.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4835677.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1939.9144287109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6074567.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  218446304.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  388279.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.8572671413421631    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.17728020250797272    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7989860773086548    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  460102.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  388279.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  453.62841796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  150.30709838867188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  425.38934326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  108839488.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  383888064.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452266.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   49/6933] Loss: 2620.4216 [iq: 182.3838,ans: 1972.0151,interp: 737.5596,fusion: -271.5369]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  280.61077880859375    \n",
      "module.ans_embedding.weight  dot:  361089.8125    \n",
      "module.lstm.weight_ih_l0  dot:  279987.5    \n",
      "module.lstm.weight_hh_l0  dot:  14920.62109375    \n",
      "module.lstm.bias_ih_l0  dot:  22405.4609375    \n",
      "module.lstm.bias_hh_l0  dot:  22405.4609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  101978864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  52022.84375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8619067.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8619067.0    \n",
      "module.adapter.frcn_linear.weight  dot:  699736576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  405309.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5242702.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1814.592529296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7822919.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  213386912.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  354535.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.2554240226745605    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2171640694141388    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.2758641242980957    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7985612998927536e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  426306.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  354535.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  562.064208984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  225.95932006835938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  639.6714477539062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  119155456.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  382484352.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452267.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   50/6933] Loss: 2149.6411 [iq: 147.7231,ans: 1956.4690,interp: 315.1154,fusion: -269.6665]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  225.4712371826172    \n",
      "module.ans_embedding.weight  dot:  319526.75    \n",
      "module.lstm.weight_ih_l0  dot:  214888.9375    \n",
      "module.lstm.weight_hh_l0  dot:  11696.392578125    \n",
      "module.lstm.bias_ih_l0  dot:  17391.498046875    \n",
      "module.lstm.bias_hh_l0  dot:  17391.498046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  81675144.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8581.265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7132089.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7132089.0    \n",
      "module.adapter.frcn_linear.weight  dot:  606459136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  322708.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7889454.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2321.598388671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11240823.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  197114592.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  281217.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.247768759727478    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2051040083169937    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.266754150390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  344787.21875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  281217.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  100.12429809570312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.92488098144531    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  93.50230407714844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  104516816.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  363238880.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452268.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   51/6933] Loss: 2003.1191 [iq: 112.0452,ans: 1941.6409,interp: 207.8885,fusion: -258.4553]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  191.11602783203125    \n",
      "module.ans_embedding.weight  dot:  285151.90625    \n",
      "module.lstm.weight_ih_l0  dot:  190519.40625    \n",
      "module.lstm.weight_hh_l0  dot:  10811.9287109375    \n",
      "module.lstm.bias_ih_l0  dot:  15468.2919921875    \n",
      "module.lstm.bias_hh_l0  dot:  15468.2919921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  79404800.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8969.080078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7209821.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7209821.0    \n",
      "module.adapter.frcn_linear.weight  dot:  541775168.0    \n",
      "module.adapter.frcn_linear.bias  dot:  289608.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8456502.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2463.123291015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12045762.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  182024640.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  252925.703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.0721544027328491    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.1931774616241455    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.065904140472412    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  311676.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  252925.703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  70.22252655029297    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.053726196289062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  57.85077667236328    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  96402592.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  345219776.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452269.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   52/6933] Loss: 2298.3093 [iq: 92.4323,ans: 1926.3857,interp: 530.8224,fusion: -251.3310]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  196.9712371826172    \n",
      "module.ans_embedding.weight  dot:  181016.609375    \n",
      "module.lstm.weight_ih_l0  dot:  175214.78125    \n",
      "module.lstm.weight_hh_l0  dot:  10365.90234375    \n",
      "module.lstm.bias_ih_l0  dot:  13952.49609375    \n",
      "module.lstm.bias_hh_l0  dot:  13952.49609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  73993792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4142.68359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6431316.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6431316.0    \n",
      "module.adapter.frcn_linear.weight  dot:  499104416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  259316.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7413387.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2393.5419921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10617026.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  171364016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  221213.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.844695806503296    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.30059701204299927    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.956916332244873    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  287309.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  221213.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  71.42622375488281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.370281219482422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  73.59082794189453    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  91029416.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  314223872.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452271.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   53/6933] Loss: 2374.2368 [iq: 76.2278,ans: 1906.0271,interp: 645.0225,fusion: -253.0405]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  190.7529754638672    \n",
      "module.ans_embedding.weight  dot:  271587.71875    \n",
      "module.lstm.weight_ih_l0  dot:  168270.828125    \n",
      "module.lstm.weight_hh_l0  dot:  9550.5146484375    \n",
      "module.lstm.bias_ih_l0  dot:  13716.9951171875    \n",
      "module.lstm.bias_hh_l0  dot:  13716.9951171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  74753456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5481.8447265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6458893.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6458893.0    \n",
      "module.adapter.frcn_linear.weight  dot:  478058048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  249512.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6083489.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1906.5819091796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8418834.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  170985696.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  212760.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.147142171859741    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.33817052841186523    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.7178559303283691    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  268151.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  212760.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  140.33740234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.9544677734375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  140.9406280517578    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  98143032.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  325829408.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452272.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   54/6933] Loss: 2151.9915 [iq: 62.9501,ans: 1890.1833,interp: 450.5147,fusion: -251.6567]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  149.52638244628906    \n",
      "module.ans_embedding.weight  dot:  218840.8125    \n",
      "module.lstm.weight_ih_l0  dot:  147939.671875    \n",
      "module.lstm.weight_hh_l0  dot:  8928.92578125    \n",
      "module.lstm.bias_ih_l0  dot:  12009.099609375    \n",
      "module.lstm.bias_hh_l0  dot:  12009.099609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  63114104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4423.59814453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5385892.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5385892.5    \n",
      "module.adapter.frcn_linear.weight  dot:  437272288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  222147.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5957915.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1686.8719482421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8906234.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  160223328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  182978.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.0982866287231445    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.29847922921180725    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.673966407775879    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.932343174360085e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  238188.703125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  182978.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  94.02901458740234    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.102684020996094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  83.15445709228516    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  85329600.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  292644192.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452273.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   55/6933] Loss: 2157.9978 [iq: 52.7859,ans: 1870.7441,interp: 491.6443,fusion: -257.1765]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125.763671875    \n",
      "module.ans_embedding.weight  dot:  162050.046875    \n",
      "module.lstm.weight_ih_l0  dot:  133189.328125    \n",
      "module.lstm.weight_hh_l0  dot:  8617.5888671875    \n",
      "module.lstm.bias_ih_l0  dot:  10779.580078125    \n",
      "module.lstm.bias_hh_l0  dot:  10779.580078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60111488.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20462.48828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5952125.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5952125.5    \n",
      "module.adapter.frcn_linear.weight  dot:  401692416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  197854.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7393324.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1720.342041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10745883.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  150502976.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  164629.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.601879596710205    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.28349265456199646    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.8335051536560059    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.377298440909726e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  222414.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  164629.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  174.21640014648438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.21324920654297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  167.08731079101562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  80144464.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  287713920.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452274.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   56/6933] Loss: 1990.5034 [iq: 44.7545,ans: 1853.3835,interp: 343.0418,fusion: -250.6762]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  163.40553283691406    \n",
      "module.ans_embedding.weight  dot:  243751.078125    \n",
      "module.lstm.weight_ih_l0  dot:  125860.5    \n",
      "module.lstm.weight_hh_l0  dot:  7664.1044921875    \n",
      "module.lstm.bias_ih_l0  dot:  10043.25390625    \n",
      "module.lstm.bias_hh_l0  dot:  10043.25390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  71502312.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2473.88623046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6804041.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6804041.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387941088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  192199.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6621660.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1670.456298828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11851281.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  153314080.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160581.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.950140357017517    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2902117967605591    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.592316150665283    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7200464103316335e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  220478.71875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160581.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  126.06781005859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.02932357788086    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  108.38384246826172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.696243311424041e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  76840896.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  298724320.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452276.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   57/6933] Loss: 1665.9540 [iq: 39.0931,ans: 1835.1094,interp: 40.7076,fusion: -248.9561]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  130.38307189941406    \n",
      "module.ans_embedding.weight  dot:  215280.71875    \n",
      "module.lstm.weight_ih_l0  dot:  120191.4765625    \n",
      "module.lstm.weight_hh_l0  dot:  7615.10595703125    \n",
      "module.lstm.bias_ih_l0  dot:  9850.291015625    \n",
      "module.lstm.bias_hh_l0  dot:  9850.291015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  62432504.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8470.0517578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5627606.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5627606.0    \n",
      "module.adapter.frcn_linear.weight  dot:  345068832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  172012.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7018629.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1875.6864013671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10921305.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  138989408.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141462.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.39347505569458    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3119030296802521    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.8383750915527344    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1338486533295509e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  193324.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141462.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  112.70381927490234    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  40.09138107299805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  102.53260803222656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  80455992.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  270452032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452277.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   58/6933] Loss: 1645.2644 [iq: 33.8178,ans: 1816.3811,interp: 37.4429,fusion: -242.3774]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.98365783691406    \n",
      "module.ans_embedding.weight  dot:  316025.96875    \n",
      "module.lstm.weight_ih_l0  dot:  112367.34375    \n",
      "module.lstm.weight_hh_l0  dot:  7146.296875    \n",
      "module.lstm.bias_ih_l0  dot:  9089.79296875    \n",
      "module.lstm.bias_hh_l0  dot:  9089.79296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  75528296.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  656.364501953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6383190.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6383190.5    \n",
      "module.adapter.frcn_linear.weight  dot:  322719840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  160945.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8900108.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1707.2506103515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15024330.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  141857152.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  138094.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.2075252532958984    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.362310528755188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.4708259105682373    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3877787807814457e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  193385.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  138094.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.7621660232543945    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.1312851905822754    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7.61806583404541    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  86691024.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  281358208.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452278.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   59/6933] Loss: 2432.7942 [iq: 30.8393,ans: 1792.7263,interp: 846.5095,fusion: -237.2811]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  127.51820373535156    \n",
      "module.ans_embedding.weight  dot:  236276.234375    \n",
      "module.lstm.weight_ih_l0  dot:  108369.7578125    \n",
      "module.lstm.weight_hh_l0  dot:  6524.41259765625    \n",
      "module.lstm.bias_ih_l0  dot:  8674.0517578125    \n",
      "module.lstm.bias_hh_l0  dot:  8674.0517578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  59743152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1989.029296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5419883.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5419883.5    \n",
      "module.adapter.frcn_linear.weight  dot:  313395392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  156503.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7386747.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1767.4930419921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11406210.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  132777456.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  128605.1171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.6072323322296143    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.23278990387916565    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.220933437347412    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.889178034124143e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  177833.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  128605.1171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  37.35957336425781    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.062328338623047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  38.42643356323242    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  72974368.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  253449792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452279.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   60/6933] Loss: 1585.6571 [iq: 27.3914,ans: 1775.6600,interp: 27.3223,fusion: -244.7168]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  106.91967010498047    \n",
      "module.ans_embedding.weight  dot:  302692.0625    \n",
      "module.lstm.weight_ih_l0  dot:  97692.7890625    \n",
      "module.lstm.weight_hh_l0  dot:  6139.63134765625    \n",
      "module.lstm.bias_ih_l0  dot:  7873.2548828125    \n",
      "module.lstm.bias_hh_l0  dot:  7873.2548828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  67730000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3570.443359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5884499.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5884499.0    \n",
      "module.adapter.frcn_linear.weight  dot:  294273280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133428.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7886383.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1515.9461669921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13724548.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  131070816.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  113884.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.3394432067871094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.22120912373065948    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.853035807609558    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  158967.21875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  113884.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  188.3408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  81.59868621826172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  175.9336700439453    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9454660105111543e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  84082880.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  266275200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452280.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   61/6933] Loss: 2221.9307 [iq: 24.9667,ans: 1754.2720,interp: 675.5807,fusion: -232.8885]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  111.87567901611328    \n",
      "module.ans_embedding.weight  dot:  155652.640625    \n",
      "module.lstm.weight_ih_l0  dot:  89318.453125    \n",
      "module.lstm.weight_hh_l0  dot:  5646.541015625    \n",
      "module.lstm.bias_ih_l0  dot:  7114.5029296875    \n",
      "module.lstm.bias_hh_l0  dot:  7114.5029296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48065520.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4128.4423828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4309089.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4309089.0    \n",
      "module.adapter.frcn_linear.weight  dot:  274469920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  126399.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7768499.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1516.548583984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14657069.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  118724744.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  102409.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.1639469861984253    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.16467514634132385    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.6302534341812134    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  145863.859375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  102409.5078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  133.92613220214844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.21499252319336    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  130.39781188964844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  66747280.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  220447744.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452281.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   62/6933] Loss: 1537.9084 [iq: 22.8660,ans: 1730.9224,interp: 25.9304,fusion: -241.8104]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  116.12076568603516    \n",
      "module.ans_embedding.weight  dot:  142932.171875    \n",
      "module.lstm.weight_ih_l0  dot:  85157.0859375    \n",
      "module.lstm.weight_hh_l0  dot:  6023.52490234375    \n",
      "module.lstm.bias_ih_l0  dot:  6947.34521484375    \n",
      "module.lstm.bias_hh_l0  dot:  6947.34521484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48996376.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3549.81982421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4510529.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4510529.5    \n",
      "module.adapter.frcn_linear.weight  dot:  267862288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  123946.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7599410.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1670.9029541015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12656866.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  116425624.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99854.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.1308979988098145    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.32063937187194824    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.169468402862549    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  148437.59375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99854.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  21.224763870239258    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.865907192230225    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  18.120319366455078    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.780531526193954e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  64541444.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  218594432.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452282.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   63/6933] Loss: 1689.2771 [iq: 24.5969,ans: 1710.4646,interp: 195.5487,fusion: -241.3331]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  97.23097229003906    \n",
      "module.ans_embedding.weight  dot:  164827.875    \n",
      "module.lstm.weight_ih_l0  dot:  82782.6875    \n",
      "module.lstm.weight_hh_l0  dot:  5781.93701171875    \n",
      "module.lstm.bias_ih_l0  dot:  6700.15771484375    \n",
      "module.lstm.bias_hh_l0  dot:  6700.15771484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50760872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18215.26171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5012867.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5012867.5    \n",
      "module.adapter.frcn_linear.weight  dot:  272360448.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119182.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6370102.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1357.067626953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10735544.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  117297712.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96801.1796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.313554286956787    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3837471902370453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.643239974975586    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143440.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96801.1796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  205.09591674804688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  86.94168090820312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  216.42124938964844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  65142672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  221277984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452283.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   64/6933] Loss: 1493.8351 [iq: 21.9953,ans: 1692.7295,interp: 19.5947,fusion: -240.4845]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  103.79243469238281    \n",
      "module.ans_embedding.weight  dot:  157723.40625    \n",
      "module.lstm.weight_ih_l0  dot:  76288.59375    \n",
      "module.lstm.weight_hh_l0  dot:  4701.3974609375    \n",
      "module.lstm.bias_ih_l0  dot:  6060.0224609375    \n",
      "module.lstm.bias_hh_l0  dot:  6060.0224609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44017944.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2932.386474609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4115246.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4115246.5    \n",
      "module.adapter.frcn_linear.weight  dot:  258532768.0    \n",
      "module.adapter.frcn_linear.bias  dot:  112308.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8066996.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1621.364013671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14039216.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  114283400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  88191.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.372835159301758    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.35776805877685547    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.7298126220703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8673951274195133e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  128514.015625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  88191.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  38.08159255981445    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.74315643310547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  34.97917938232422    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59360816.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  200472800.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452284.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   65/6933] Loss: 1461.9221 [iq: 21.1010,ans: 1667.2039,interp: 20.3327,fusion: -246.7155]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  90.87256622314453    \n",
      "module.ans_embedding.weight  dot:  161486.421875    \n",
      "module.lstm.weight_ih_l0  dot:  74991.9921875    \n",
      "module.lstm.weight_hh_l0  dot:  4755.62890625    \n",
      "module.lstm.bias_ih_l0  dot:  5914.419921875    \n",
      "module.lstm.bias_hh_l0  dot:  5914.419921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40959696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  987.7872924804688    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3859343.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3859343.75    \n",
      "module.adapter.frcn_linear.weight  dot:  238682352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  104969.3515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8126767.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1560.13671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14675178.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  108190872.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  83930.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.7799094915390015    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.32169365882873535    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.892956018447876    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  124058.59375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  83930.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  45.80959701538086    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.359573364257812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  42.68229675292969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55024512.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  187311136.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452285.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   66/6933] Loss: 1440.6378 [iq: 20.0436,ans: 1645.4587,interp: 16.9294,fusion: -241.7940]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  121.89979553222656    \n",
      "module.ans_embedding.weight  dot:  203988.34375    \n",
      "module.lstm.weight_ih_l0  dot:  92632.3515625    \n",
      "module.lstm.weight_hh_l0  dot:  6152.88037109375    \n",
      "module.lstm.bias_ih_l0  dot:  7527.68115234375    \n",
      "module.lstm.bias_hh_l0  dot:  7527.68115234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  52131552.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2925.0244140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4494107.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4494107.0    \n",
      "module.adapter.frcn_linear.weight  dot:  255867520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  121325.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6785180.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1534.5472412109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11942563.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  113086000.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96208.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.34000301361084    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.602440357208252    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.610787391662598    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143489.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96208.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  31.487306594848633    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.69049072265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37.696327209472656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  63074488.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  182720128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452286.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   67/6933] Loss: 1402.4594 [iq: 17.1587,ans: 1614.9624,interp: 16.9268,fusion: -246.5885]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  105.40122985839844    \n",
      "module.ans_embedding.weight  dot:  133397.46875    \n",
      "module.lstm.weight_ih_l0  dot:  71123.3046875    \n",
      "module.lstm.weight_hh_l0  dot:  4625.419921875    \n",
      "module.lstm.bias_ih_l0  dot:  5676.10302734375    \n",
      "module.lstm.bias_hh_l0  dot:  5676.10302734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40144416.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1729.463134765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3580483.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3580483.75    \n",
      "module.adapter.frcn_linear.weight  dot:  234458128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  103813.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  10734642.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2051.494140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  19858414.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  105281872.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  81410.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.118427276611328    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.4814026653766632    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.271235466003418    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.496403249731884e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121878.078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  81410.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  48.98516082763672    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.32552719116211    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  56.37176513671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  52544832.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  167590400.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452287.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   68/6933] Loss: 1394.3196 [iq: 25.5551,ans: 1596.3677,interp: 20.5354,fusion: -248.1386]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  121.08314514160156    \n",
      "module.ans_embedding.weight  dot:  203681.859375    \n",
      "module.lstm.weight_ih_l0  dot:  71444.4375    \n",
      "module.lstm.weight_hh_l0  dot:  4420.7919921875    \n",
      "module.lstm.bias_ih_l0  dot:  5887.5400390625    \n",
      "module.lstm.bias_hh_l0  dot:  5887.5400390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49350288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6395.6611328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4015106.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4015106.75    \n",
      "module.adapter.frcn_linear.weight  dot:  221812864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96394.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6366496.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1242.2158203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11725615.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  105791344.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78110.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.5906028747558594    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.42125827074050903    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.340433120727539    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  112987.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78110.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  287.07574462890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  112.97380065917969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  276.3891296386719    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  62067976.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  169868128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452288.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   69/6933] Loss: 1391.8136 [iq: 19.9308,ans: 1561.2748,interp: 49.0489,fusion: -238.4408]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  98.2889633178711    \n",
      "module.ans_embedding.weight  dot:  146531.453125    \n",
      "module.lstm.weight_ih_l0  dot:  81184.3125    \n",
      "module.lstm.weight_hh_l0  dot:  5656.82958984375    \n",
      "module.lstm.bias_ih_l0  dot:  6499.767578125    \n",
      "module.lstm.bias_hh_l0  dot:  6499.767578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38026464.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4323.208984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3507103.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3507103.5    \n",
      "module.adapter.frcn_linear.weight  dot:  224756896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101802.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6200879.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1415.42626953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11350621.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  103900416.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80727.390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.352705955505371    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5262364149093628    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.683864593505859    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  123868.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80727.390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  128.56497192382812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.795082092285156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  132.229736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  51079632.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  156718304.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452289.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   70/6933] Loss: 1337.6266 [iq: 24.0274,ans: 1547.9241,interp: 16.8351,fusion: -251.1600]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  104.89613342285156    \n",
      "module.ans_embedding.weight  dot:  109550.0859375    \n",
      "module.lstm.weight_ih_l0  dot:  70392.8046875    \n",
      "module.lstm.weight_hh_l0  dot:  4826.96484375    \n",
      "module.lstm.bias_ih_l0  dot:  5579.6728515625    \n",
      "module.lstm.bias_hh_l0  dot:  5579.6728515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32601354.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2151.359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2993559.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2993559.5    \n",
      "module.adapter.frcn_linear.weight  dot:  229290400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99895.9296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6853396.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1202.1163330078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13876633.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  102487840.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76353.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.7166404724121094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.4615148901939392    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.917430877685547    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0746958878371515e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  119562.671875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76353.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  102.8801040649414    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.137752532958984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  115.90188598632812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  47836128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  148411520.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452290.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   71/6933] Loss: 1305.2704 [iq: 24.2168,ans: 1519.4475,interp: 15.7852,fusion: -254.1791]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  91.33305358886719    \n",
      "module.ans_embedding.weight  dot:  99451.4375    \n",
      "module.lstm.weight_ih_l0  dot:  67472.7734375    \n",
      "module.lstm.weight_hh_l0  dot:  4514.6474609375    \n",
      "module.lstm.bias_ih_l0  dot:  5290.083984375    \n",
      "module.lstm.bias_hh_l0  dot:  5290.083984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31022530.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1948.753662109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2844745.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2844745.0    \n",
      "module.adapter.frcn_linear.weight  dot:  211966352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88850.0234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7402038.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1399.842529296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13521822.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  96604752.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69314.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.02864933013916    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.46910762786865234    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.566309452056885    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  108374.1640625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69314.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  78.37847900390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.044492721557617    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  81.75643920898438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42862364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  137987424.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452291.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   72/6933] Loss: 1297.1057 [iq: 23.3680,ans: 1504.8386,interp: 18.7804,fusion: -249.8814]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  96.43562316894531    \n",
      "module.ans_embedding.weight  dot:  127133.3515625    \n",
      "module.lstm.weight_ih_l0  dot:  67700.2578125    \n",
      "module.lstm.weight_hh_l0  dot:  4927.72900390625    \n",
      "module.lstm.bias_ih_l0  dot:  5446.525390625    \n",
      "module.lstm.bias_hh_l0  dot:  5446.525390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32529012.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3340.70947265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3101558.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3101558.25    \n",
      "module.adapter.frcn_linear.weight  dot:  218552704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  90535.0078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7537019.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1323.6171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15345297.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  99220000.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70171.9921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.3350892066955566    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5434608459472656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.914486885070801    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.716849298982197e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  111297.515625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70171.9921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  60.14015197753906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.968103408813477    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  55.564544677734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42836536.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  132720120.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452291.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   73/6933] Loss: 1261.9084 [iq: 26.2763,ans: 1472.2834,interp: 18.2992,fusion: -254.9505]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  120.10874938964844    \n",
      "module.ans_embedding.weight  dot:  191810.75    \n",
      "module.lstm.weight_ih_l0  dot:  82757.0625    \n",
      "module.lstm.weight_hh_l0  dot:  5495.6982421875    \n",
      "module.lstm.bias_ih_l0  dot:  6417.5810546875    \n",
      "module.lstm.bias_hh_l0  dot:  6417.5810546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41718104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2570.623291015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4166952.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4166952.25    \n",
      "module.adapter.frcn_linear.weight  dot:  224612544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99159.4296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7997986.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1763.688232421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14584730.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  102813200.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78453.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.8927829265594482    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6657613515853882    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.179450988769531    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  123033.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78453.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  15.228646278381348    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.3706231117248535    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.324956893920898    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  43896608.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  140413472.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452292.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   74/6933] Loss: 1851.5452 [iq: 23.5828,ans: 1455.6638,interp: 621.7372,fusion: -249.4385]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.69638061523438    \n",
      "module.ans_embedding.weight  dot:  219200.28125    \n",
      "module.lstm.weight_ih_l0  dot:  75855.578125    \n",
      "module.lstm.weight_hh_l0  dot:  4779.01025390625    \n",
      "module.lstm.bias_ih_l0  dot:  6220.8359375    \n",
      "module.lstm.bias_hh_l0  dot:  6220.8359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45973936.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12514.76953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4164615.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4164615.5    \n",
      "module.adapter.frcn_linear.weight  dot:  223581488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92530.2890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7765031.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1330.685302734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14443819.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  103379856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  74505.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.066357612609863    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.671870231628418    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.3756208419799805    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.792167077193881e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  109361.6484375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  74505.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  177.0716094970703    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  70.39884185791016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  161.50692749023438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  50130208.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  130524336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452293.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   75/6933] Loss: 1195.8984 [iq: 26.7472,ans: 1400.6696,interp: 17.8656,fusion: -249.3839]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  106.35391235351562    \n",
      "module.ans_embedding.weight  dot:  149880.59375    \n",
      "module.lstm.weight_ih_l0  dot:  71618.078125    \n",
      "module.lstm.weight_hh_l0  dot:  4674.8857421875    \n",
      "module.lstm.bias_ih_l0  dot:  5730.79296875    \n",
      "module.lstm.bias_hh_l0  dot:  5730.79296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33128512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3813.361328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3131507.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3131507.5    \n",
      "module.adapter.frcn_linear.weight  dot:  216325184.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92626.2578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6862454.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1388.28857421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14064532.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  100358152.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72854.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.209875106811523    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6823087334632874    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.089781761169434    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.566746732351021e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  111168.3046875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72854.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11.55435848236084    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.433530330657959    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11.398599624633789    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41826440.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  121322896.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452293.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   76/6933] Loss: 1176.7856 [iq: 25.8460,ans: 1385.6980,interp: 21.6022,fusion: -256.3606]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  95.68547821044922    \n",
      "module.ans_embedding.weight  dot:  89426.5703125    \n",
      "module.lstm.weight_ih_l0  dot:  60505.828125    \n",
      "module.lstm.weight_hh_l0  dot:  4077.676025390625    \n",
      "module.lstm.bias_ih_l0  dot:  4841.330078125    \n",
      "module.lstm.bias_hh_l0  dot:  4841.330078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29388016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4818.6279296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2923746.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2923746.25    \n",
      "module.adapter.frcn_linear.weight  dot:  210641952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  84256.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7071551.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1203.607421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12996900.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  97147296.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65369.42578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.829904079437256    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6520293951034546    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.911191940307617    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.417089082333405e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  100537.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65369.42578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  42.54555130004883    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.896900177001953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  45.72785186767578    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37529312.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  118180800.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452294.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   77/6933] Loss: 1151.8792 [iq: 29.3259,ans: 1368.1213,interp: 13.7102,fusion: -259.2784]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  167.69094848632812    \n",
      "module.ans_embedding.weight  dot:  282667.0625    \n",
      "module.lstm.weight_ih_l0  dot:  84116.3984375    \n",
      "module.lstm.weight_hh_l0  dot:  5295.84765625    \n",
      "module.lstm.bias_ih_l0  dot:  6729.0107421875    \n",
      "module.lstm.bias_hh_l0  dot:  6729.0107421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  52749904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1519.989501953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4455240.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4455240.0    \n",
      "module.adapter.frcn_linear.weight  dot:  224566256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  90465.5078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6440045.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1351.8133544921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12083976.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  107487496.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78194.1015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.323497772216797    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.004563808441162    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.28843879699707    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9581892491137296e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  116596.171875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78194.1015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.986321449279785    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.489053249359131    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6.161018371582031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  46836864.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  115148192.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452295.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   78/6933] Loss: 1083.1750 [iq: 21.1502,ans: 1301.9347,interp: 12.3582,fusion: -252.2682]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  118.20719909667969    \n",
      "module.ans_embedding.weight  dot:  103600.71875    \n",
      "module.lstm.weight_ih_l0  dot:  75659.0    \n",
      "module.lstm.weight_hh_l0  dot:  5073.51171875    \n",
      "module.lstm.bias_ih_l0  dot:  5998.7880859375    \n",
      "module.lstm.bias_hh_l0  dot:  5998.7880859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25626712.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6343.69140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2730456.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2730456.5    \n",
      "module.adapter.frcn_linear.weight  dot:  222106528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  95133.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8647258.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1766.41162109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  18570816.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  97442368.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72939.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.541739463806152    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8768622875213623    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11.687912940979004    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2159162565694714e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  115143.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72939.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  73.59147644042969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.46786880493164    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  84.38098907470703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33403894.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  104060504.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452296.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   79/6933] Loss: 1098.9058 [iq: 31.2296,ans: 1308.7505,interp: 25.7064,fusion: -266.7808]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  120.39666748046875    \n",
      "module.ans_embedding.weight  dot:  87668.1640625    \n",
      "module.lstm.weight_ih_l0  dot:  70147.921875    \n",
      "module.lstm.weight_hh_l0  dot:  4954.609375    \n",
      "module.lstm.bias_ih_l0  dot:  5576.2744140625    \n",
      "module.lstm.bias_hh_l0  dot:  5576.2744140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28617186.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6387.24951171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2721976.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2721976.5    \n",
      "module.adapter.frcn_linear.weight  dot:  208378048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83469.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6504239.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1339.426513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12676244.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  94223728.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65693.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.856097221374512    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8949391841888428    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.924034118652344    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3877787807814457e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  106120.28125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65693.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  51.67931365966797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.686872482299805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  55.752220153808594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34799808.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  96869712.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452297.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   80/6933] Loss: 1058.2834 [iq: 34.9184,ans: 1255.3849,interp: 28.7756,fusion: -260.7955]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  144.01966857910156    \n",
      "module.ans_embedding.weight  dot:  236896.859375    \n",
      "module.lstm.weight_ih_l0  dot:  78354.578125    \n",
      "module.lstm.weight_hh_l0  dot:  5690.6953125    \n",
      "module.lstm.bias_ih_l0  dot:  6541.251953125    \n",
      "module.lstm.bias_hh_l0  dot:  6541.251953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40961924.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8163.82666015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3890360.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3890360.25    \n",
      "module.adapter.frcn_linear.weight  dot:  213429760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  85157.046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7004663.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1397.787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12544672.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  101636400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70586.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.76262903213501    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9399858713150024    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.310708999633789    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  109980.4453125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70586.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  34.306663513183594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.346672058105469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37.501365661621094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42401480.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  104103360.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452298.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   81/6933] Loss: 1108.2667 [iq: 22.6055,ans: 1211.3711,interp: 130.0432,fusion: -255.7531]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  111.85271453857422    \n",
      "module.ans_embedding.weight  dot:  67421.46875    \n",
      "module.lstm.weight_ih_l0  dot:  73710.390625    \n",
      "module.lstm.weight_hh_l0  dot:  5243.66943359375    \n",
      "module.lstm.bias_ih_l0  dot:  5937.943359375    \n",
      "module.lstm.bias_hh_l0  dot:  5937.943359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22196016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2667.508544921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1977055.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1977055.375    \n",
      "module.adapter.frcn_linear.weight  dot:  209225248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  84721.421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7861228.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1584.4683837890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  17132666.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  91902032.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64870.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.211435317993164    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.1477601528167725    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.401998519897461    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1338486533295509e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  104272.34375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64870.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  46.90541076660156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.1187162399292    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  43.07573699951172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31293746.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80895072.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452299.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   82/6933] Loss: 1260.2974 [iq: 35.9323,ans: 1183.2963,interp: 309.9850,fusion: -268.9161]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.09902954101562    \n",
      "module.ans_embedding.weight  dot:  93739.5546875    \n",
      "module.lstm.weight_ih_l0  dot:  79624.484375    \n",
      "module.lstm.weight_hh_l0  dot:  5874.412109375    \n",
      "module.lstm.bias_ih_l0  dot:  6281.345703125    \n",
      "module.lstm.bias_hh_l0  dot:  6281.345703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29532016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3401.3828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2711265.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2711265.0    \n",
      "module.adapter.frcn_linear.weight  dot:  224075856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92270.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6407101.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1575.50390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11831505.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  96667528.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71580.8046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.079612731933594    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2601944208145142    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.772897720336914    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.672262990534364e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  117594.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71580.8046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16.2537841796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.953154563903809    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  16.998245239257812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33637632.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  90357272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452299.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   83/6933] Loss: 1001.3475 [iq: 39.0248,ans: 1158.0415,interp: 75.3240,fusion: -271.0427]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  133.4498748779297    \n",
      "module.ans_embedding.weight  dot:  92013.34375    \n",
      "module.lstm.weight_ih_l0  dot:  80255.7578125    \n",
      "module.lstm.weight_hh_l0  dot:  5550.64306640625    \n",
      "module.lstm.bias_ih_l0  dot:  6389.2412109375    \n",
      "module.lstm.bias_hh_l0  dot:  6389.2412109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26741926.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7304.47607421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2560514.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2560514.0    \n",
      "module.adapter.frcn_linear.weight  dot:  212292176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88421.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6530005.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1266.43798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14366484.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  93055488.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68437.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.004190444946289    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.3238487243652344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.757699966430664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1338486533295509e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  109220.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68437.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  19.079444885253906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.409748554229736    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  19.796173095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31148470.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  78646928.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452300.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   84/6933] Loss: 892.2556 [iq: 35.5249,ans: 1106.3903,interp: 20.0372,fusion: -269.6967]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  133.64212036132812    \n",
      "module.ans_embedding.weight  dot:  82540.40625    \n",
      "module.lstm.weight_ih_l0  dot:  83913.46875    \n",
      "module.lstm.weight_hh_l0  dot:  6013.59130859375    \n",
      "module.lstm.bias_ih_l0  dot:  6759.67626953125    \n",
      "module.lstm.bias_hh_l0  dot:  6759.67626953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22379444.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9547.1875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2296995.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2296995.0    \n",
      "module.adapter.frcn_linear.weight  dot:  222777328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  91937.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7212916.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1557.289794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13820633.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  94158840.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71116.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.180862426757812    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.234938144683838    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.719511032104492    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  115600.734375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71116.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  70.65943908691406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.484121322631836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  66.39851379394531    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.524202986047385e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29603258.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  77781264.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452301.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   85/6933] Loss: 862.0892 [iq: 32.2698,ans: 1083.1617,interp: 20.5343,fusion: -273.8766]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  139.37939453125    \n",
      "module.ans_embedding.weight  dot:  160116.96875    \n",
      "module.lstm.weight_ih_l0  dot:  84238.328125    \n",
      "module.lstm.weight_hh_l0  dot:  5816.849609375    \n",
      "module.lstm.bias_ih_l0  dot:  6590.6728515625    \n",
      "module.lstm.bias_hh_l0  dot:  6590.6728515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41665968.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4132.83349609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3362214.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3362214.5    \n",
      "module.adapter.frcn_linear.weight  dot:  205649200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  82485.5703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5697869.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1272.4361572265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11056370.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  95521392.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68682.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.774806022644043    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2679522037506104    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.458830833435059    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7200464103316335e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  110662.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68682.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3.401024103164673    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.3930790424346924    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3.3923778533935547    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  35913672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  79534560.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452302.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   86/6933] Loss: 853.9058 [iq: 36.1645,ans: 1010.3627,interp: 69.8422,fusion: -262.4636]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  128.7012176513672    \n",
      "module.ans_embedding.weight  dot:  129814.40625    \n",
      "module.lstm.weight_ih_l0  dot:  77635.84375    \n",
      "module.lstm.weight_hh_l0  dot:  4688.3251953125    \n",
      "module.lstm.bias_ih_l0  dot:  6337.41748046875    \n",
      "module.lstm.bias_hh_l0  dot:  6337.41748046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27383236.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2422.953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2720136.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2720136.5    \n",
      "module.adapter.frcn_linear.weight  dot:  212059936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88635.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6713497.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1392.836669921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14820354.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  95364584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70432.8828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.622734069824219    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8992739915847778    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.255096435546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  103596.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70432.8828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9.831329345703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.1237597465515137    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10.990896224975586    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.072742060794553e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29823644.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  74167112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452303.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   87/6933] Loss: 780.6280 [iq: 34.2630,ans: 995.7231,interp: 23.1638,fusion: -272.5219]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  119.64615631103516    \n",
      "module.ans_embedding.weight  dot:  98560.9375    \n",
      "module.lstm.weight_ih_l0  dot:  66439.703125    \n",
      "module.lstm.weight_hh_l0  dot:  4747.5146484375    \n",
      "module.lstm.bias_ih_l0  dot:  5368.8642578125    \n",
      "module.lstm.bias_hh_l0  dot:  5368.8642578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24737492.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1922.0230712890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2425722.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2425722.0    \n",
      "module.adapter.frcn_linear.weight  dot:  208120800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  77798.1953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6946563.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1432.70703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12504123.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  92976456.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62910.8046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.881494522094727    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.030578851699829    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.189861297607422    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3509193763638905e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  100419.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62910.8046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11.169027328491211    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.8747525215148926    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11.479776382446289    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28867408.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73106864.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452304.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   88/6933] Loss: 888.9356 [iq: 33.3756,ans: 955.8798,interp: 168.9265,fusion: -269.2463]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  161.01370239257812    \n",
      "module.ans_embedding.weight  dot:  128932.59375    \n",
      "module.lstm.weight_ih_l0  dot:  88380.203125    \n",
      "module.lstm.weight_hh_l0  dot:  6299.0283203125    \n",
      "module.lstm.bias_ih_l0  dot:  7044.7607421875    \n",
      "module.lstm.bias_hh_l0  dot:  7044.7607421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31461476.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1042.107421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2821714.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2821714.75    \n",
      "module.adapter.frcn_linear.weight  dot:  227961616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  87900.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7213657.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1507.328857421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12829208.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  96330976.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71298.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.542566299438477    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.7760694026947021    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.674274444580078    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  115090.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71298.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.13484853506088257    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.06851940602064133    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  0.1640814244747162    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29015352.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  67240400.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452305.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   89/6933] Loss: 692.9855 [iq: 36.7289,ans: 907.3941,interp: 29.0120,fusion: -280.1495]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  168.0789337158203    \n",
      "module.ans_embedding.weight  dot:  182401.171875    \n",
      "module.lstm.weight_ih_l0  dot:  79961.921875    \n",
      "module.lstm.weight_hh_l0  dot:  5412.9677734375    \n",
      "module.lstm.bias_ih_l0  dot:  6476.5068359375    \n",
      "module.lstm.bias_hh_l0  dot:  6476.5068359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34473704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1648.0496826171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3210153.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3210153.5    \n",
      "module.adapter.frcn_linear.weight  dot:  219267248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  84152.9765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6209833.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1323.206787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11118916.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  99352760.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70195.4296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.965812683105469    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2047057151794434    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.541028022766113    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  109374.40625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70195.4296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8.927873611450195    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.171797275543213    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8.585890769958496    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31258702.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  69362584.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452305.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   90/6933] Loss: 648.2698 [iq: 36.3876,ans: 855.9554,interp: 31.2665,fusion: -275.3398]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  127.9031982421875    \n",
      "module.ans_embedding.weight  dot:  98853.140625    \n",
      "module.lstm.weight_ih_l0  dot:  80266.4375    \n",
      "module.lstm.weight_hh_l0  dot:  5671.396484375    \n",
      "module.lstm.bias_ih_l0  dot:  6328.08740234375    \n",
      "module.lstm.bias_hh_l0  dot:  6328.08740234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26153394.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2078.279052734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2536301.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2536301.5    \n",
      "module.adapter.frcn_linear.weight  dot:  203893584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  78204.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7000191.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1458.033447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13537551.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  90651160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64336.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.018743515014648    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.3160579204559326    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.910578727722168    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  106197.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64336.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  14.351125717163086    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.2590227127075195    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  12.647591590881348    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.298783551348606e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26536242.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  63314280.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452307.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   91/6933] Loss: 663.8146 [iq: 51.1997,ans: 841.0688,interp: 42.8010,fusion: -271.2548]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  153.0369110107422    \n",
      "module.ans_embedding.weight  dot:  120003.046875    \n",
      "module.lstm.weight_ih_l0  dot:  90641.015625    \n",
      "module.lstm.weight_hh_l0  dot:  6550.9443359375    \n",
      "module.lstm.bias_ih_l0  dot:  7373.46435546875    \n",
      "module.lstm.bias_hh_l0  dot:  7373.46435546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27686514.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2845.30078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2666488.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2666488.5    \n",
      "module.adapter.frcn_linear.weight  dot:  217630016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93728.1953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5234297.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1207.69677734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11329698.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  96165976.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76457.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.455188751220703    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8906821012496948    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.89023208618164    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  120623.484375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76457.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  30.939889907836914    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.34213638305664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  29.17819595336914    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28227810.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  65186496.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452307.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   92/6933] Loss: 575.9363 [iq: 37.5375,ans: 802.1102,interp: 16.3262,fusion: -280.0376]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  136.80697631835938    \n",
      "module.ans_embedding.weight  dot:  128030.9609375    \n",
      "module.lstm.weight_ih_l0  dot:  86249.1875    \n",
      "module.lstm.weight_hh_l0  dot:  5877.39404296875    \n",
      "module.lstm.bias_ih_l0  dot:  7076.76806640625    \n",
      "module.lstm.bias_hh_l0  dot:  7076.76806640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30737190.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2905.167236328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3309278.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3309278.5    \n",
      "module.adapter.frcn_linear.weight  dot:  220171360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  89552.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5606277.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1192.481689453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11504714.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  100940144.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75968.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.161431789398193    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2740063667297363    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.037420272827148    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  116206.984375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75968.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16.87500762939453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.457197189331055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.278849601745605    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29037818.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  70619384.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452308.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   93/6933] Loss: 595.0281 [iq: 42.3325,ans: 769.2529,interp: 61.5991,fusion: -278.1565]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  165.07472229003906    \n",
      "module.ans_embedding.weight  dot:  117772.640625    \n",
      "module.lstm.weight_ih_l0  dot:  87374.15625    \n",
      "module.lstm.weight_hh_l0  dot:  6216.55322265625    \n",
      "module.lstm.bias_ih_l0  dot:  7201.0302734375    \n",
      "module.lstm.bias_hh_l0  dot:  7201.0302734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26184370.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  855.4044189453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2622320.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2622320.5    \n",
      "module.adapter.frcn_linear.weight  dot:  216216128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  87326.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7094430.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1652.349609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15352576.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  96977424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73725.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.005904197692871    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.441796898841858    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.031673431396484    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  115291.515625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73725.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12.261576652526855    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.226711273193359    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11.48350715637207    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26503108.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  64510756.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452309.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   94/6933] Loss: 529.4342 [iq: 40.8335,ans: 745.9913,interp: 23.6058,fusion: -280.9964]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  166.58706665039062    \n",
      "module.ans_embedding.weight  dot:  171118.8125    \n",
      "module.lstm.weight_ih_l0  dot:  99566.1875    \n",
      "module.lstm.weight_hh_l0  dot:  6638.50732421875    \n",
      "module.lstm.bias_ih_l0  dot:  8018.9912109375    \n",
      "module.lstm.bias_hh_l0  dot:  8018.9912109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39279808.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7280.69970703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3559425.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3559425.75    \n",
      "module.adapter.frcn_linear.weight  dot:  229579408.0    \n",
      "module.adapter.frcn_linear.bias  dot:  94390.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5565220.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1338.983642578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10967414.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  99633072.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80638.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.8843994140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.7255308628082275    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.6669864654541    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  124420.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80638.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9.832191467285156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.059870719909668    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9.346769332885742    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3656631381309126e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29651412.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  62443100.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452310.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   95/6933] Loss: 453.8120 [iq: 41.6071,ans: 667.2385,interp: 26.8648,fusion: -281.8983]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  197.3265838623047    \n",
      "module.ans_embedding.weight  dot:  189336.984375    \n",
      "module.lstm.weight_ih_l0  dot:  97051.96875    \n",
      "module.lstm.weight_hh_l0  dot:  6585.5419921875    \n",
      "module.lstm.bias_ih_l0  dot:  8023.1787109375    \n",
      "module.lstm.bias_hh_l0  dot:  8023.1787109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36940220.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11363.7412109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3741433.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3741433.5    \n",
      "module.adapter.frcn_linear.weight  dot:  220416736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  90385.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5975751.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1358.939697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12107537.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  98543856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78204.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.281091690063477    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.7032241821289062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  18.47592544555664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  119513.4453125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78204.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  31.828243255615234    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.198476791381836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  36.70171356201172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27809668.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59744512.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452311.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   96/6933] Loss: 563.3506 [iq: 41.1986,ans: 635.6926,interp: 168.5092,fusion: -282.0498]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  169.5550079345703    \n",
      "module.ans_embedding.weight  dot:  143053.03125    \n",
      "module.lstm.weight_ih_l0  dot:  95258.5703125    \n",
      "module.lstm.weight_hh_l0  dot:  6867.140625    \n",
      "module.lstm.bias_ih_l0  dot:  7918.7587890625    \n",
      "module.lstm.bias_hh_l0  dot:  7918.7587890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29483456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7049.96240234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3272042.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3272042.5    \n",
      "module.adapter.frcn_linear.weight  dot:  235178976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93171.453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8600138.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1953.8336181640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  18254914.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  101445968.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78647.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.421186447143555    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.6656084060668945    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  18.380022048950195    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  122328.171875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78647.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  50.51151657104492    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.469810485839844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  54.356529235839844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26454472.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59319688.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452312.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   97/6933] Loss: 393.5858 [iq: 44.1097,ans: 611.9955,interp: 26.3479,fusion: -288.8674]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  162.69882202148438    \n",
      "module.ans_embedding.weight  dot:  189902.296875    \n",
      "module.lstm.weight_ih_l0  dot:  85257.421875    \n",
      "module.lstm.weight_hh_l0  dot:  5697.861328125    \n",
      "module.lstm.bias_ih_l0  dot:  6887.95703125    \n",
      "module.lstm.bias_hh_l0  dot:  6887.95703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44391672.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15227.0458984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4453569.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4453569.5    \n",
      "module.adapter.frcn_linear.weight  dot:  228238688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  85169.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8081441.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1898.309814453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15051099.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  106114864.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78564.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.016643524169922    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.5144466161727905    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.506937026977539    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  120573.7578125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78564.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  28.518604278564453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.533527374267578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  23.477954864501953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  32195948.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  67122192.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452313.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step   98/6933] Loss: 347.9850 [iq: 41.9413,ans: 558.8450,interp: 21.5866,fusion: -274.3878]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  184.60733032226562    \n",
      "module.ans_embedding.weight  dot:  162782.140625    \n",
      "module.lstm.weight_ih_l0  dot:  107109.46875    \n",
      "module.lstm.weight_hh_l0  dot:  7557.232421875    \n",
      "module.lstm.bias_ih_l0  dot:  8663.720703125    \n",
      "module.lstm.bias_hh_l0  dot:  8663.720703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36719668.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4529.25537109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3684862.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3684862.5    \n",
      "module.adapter.frcn_linear.weight  dot:  252202416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  98188.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7172581.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1740.899169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14994830.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  105166184.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  84428.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.283309936523438    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.259398937225342    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  23.435054779052734    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  131688.546875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  84428.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8.003791809082031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.941661834716797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6.490819931030273    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6488199694464356e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27616212.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60920276.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452314.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step   99/6933] Loss: 327.1936 [iq: 52.7249,ans: 544.7852,interp: 24.5207,fusion: -294.8372]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  165.27081298828125    \n",
      "module.ans_embedding.weight  dot:  216374.171875    \n",
      "module.lstm.weight_ih_l0  dot:  102730.875    \n",
      "module.lstm.weight_hh_l0  dot:  6990.8193359375    \n",
      "module.lstm.bias_ih_l0  dot:  8211.5341796875    \n",
      "module.lstm.bias_hh_l0  dot:  8211.5341796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41965920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12385.2041015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4034011.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4034011.75    \n",
      "module.adapter.frcn_linear.weight  dot:  236723824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  94042.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5343896.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1476.07666015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10033192.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  102991664.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  82434.4453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.84744644165039    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.5434545278549194    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  16.05266571044922    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  127438.046875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  82434.4453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  34.01909637451172    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.952676773071289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  32.34858322143555    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28959120.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58087328.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452315.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  100/6933] Loss: 272.5012 [iq: 42.8906,ans: 486.9631,interp: 32.2469,fusion: -289.5994]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  165.5467529296875    \n",
      "module.ans_embedding.weight  dot:  176142.0    \n",
      "module.lstm.weight_ih_l0  dot:  102218.6484375    \n",
      "module.lstm.weight_hh_l0  dot:  7129.353515625    \n",
      "module.lstm.bias_ih_l0  dot:  8208.4013671875    \n",
      "module.lstm.bias_hh_l0  dot:  8208.4013671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41116184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10781.9521484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3884310.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3884310.5    \n",
      "module.adapter.frcn_linear.weight  dot:  243735264.0    \n",
      "module.adapter.frcn_linear.bias  dot:  94507.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5954497.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1425.3717041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11461726.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  107507200.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86044.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.946174621582031    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.118164539337158    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21.836563110351562    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  138212.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86044.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  67.06697845458984    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  22.087894439697266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  55.04536056518555    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29402982.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60194172.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452316.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  101/6933] Loss: 274.2230 [iq: 54.7960,ans: 472.1407,interp: 36.5757,fusion: -289.2895]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  242.37551879882812    \n",
      "module.ans_embedding.weight  dot:  353584.6875    \n",
      "module.lstm.weight_ih_l0  dot:  121850.1953125    \n",
      "module.lstm.weight_hh_l0  dot:  7661.4267578125    \n",
      "module.lstm.bias_ih_l0  dot:  10016.951171875    \n",
      "module.lstm.bias_hh_l0  dot:  10016.951171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49861400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12454.044921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5011016.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5011016.0    \n",
      "module.adapter.frcn_linear.weight  dot:  233515008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99638.9140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5990526.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1426.4674072265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13842998.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  111110592.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  98173.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.555473327636719    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.436481475830078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  25.76903533935547    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  142361.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  98173.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  39.877052307128906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.202497482299805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  40.91468048095703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31806268.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61515420.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452317.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  102/6933] Loss: 192.2789 [iq: 33.4680,ans: 423.4139,interp: 19.4005,fusion: -284.0034]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  198.60894775390625    \n",
      "module.ans_embedding.weight  dot:  306366.25    \n",
      "module.lstm.weight_ih_l0  dot:  107465.5859375    \n",
      "module.lstm.weight_hh_l0  dot:  6865.93359375    \n",
      "module.lstm.bias_ih_l0  dot:  9048.443359375    \n",
      "module.lstm.bias_hh_l0  dot:  9048.443359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42358432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4664.41015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4610382.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4610382.5    \n",
      "module.adapter.frcn_linear.weight  dot:  225333840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93702.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6690941.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1463.1845703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15003614.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  107347352.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  92166.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.318365097045898    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.4106502532958984    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  22.962074279785156    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  131151.34375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  92166.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  22.312023162841797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.407880783081055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  24.419361114501953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29184630.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60062080.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452318.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  103/6933] Loss: 180.9662 [iq: 44.4696,ans: 397.1442,interp: 27.9926,fusion: -288.6402]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  212.32772827148438    \n",
      "module.ans_embedding.weight  dot:  182706.75    \n",
      "module.lstm.weight_ih_l0  dot:  125406.65625    \n",
      "module.lstm.weight_hh_l0  dot:  8655.4443359375    \n",
      "module.lstm.bias_ih_l0  dot:  10168.400390625    \n",
      "module.lstm.bias_hh_l0  dot:  10168.400390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42180940.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2900.309326171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4309881.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4309881.5    \n",
      "module.adapter.frcn_linear.weight  dot:  263660704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  105082.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8467944.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2367.15234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15932416.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  112960264.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99181.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.601551055908203    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.0242462158203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21.157649993896484    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2159162565694714e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  152283.828125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99181.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  95.03485870361328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  36.982051849365234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  91.59683990478516    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29203066.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60649032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452318.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  104/6933] Loss: 229.8759 [iq: 56.1886,ans: 387.9658,interp: 82.0717,fusion: -296.3502]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  221.9903106689453    \n",
      "module.ans_embedding.weight  dot:  184579.078125    \n",
      "module.lstm.weight_ih_l0  dot:  126534.171875    \n",
      "module.lstm.weight_hh_l0  dot:  8910.978515625    \n",
      "module.lstm.bias_ih_l0  dot:  10278.42578125    \n",
      "module.lstm.bias_hh_l0  dot:  10278.42578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42509368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7272.10693359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4403037.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4403037.0    \n",
      "module.adapter.frcn_linear.weight  dot:  253986912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102645.3671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6650612.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1666.7730712890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13943452.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  113047032.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99324.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16.78597640991211    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.757779598236084    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  27.184131622314453    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  153164.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99324.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12.797054290771484    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.634732484817505    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.019198417663574    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.attflat_ans.mlp.linear.bias  dot:  8.97082408357619e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29621488.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  62878408.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452319.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  105/6933] Loss: 170.6628 [iq: 54.9212,ans: 373.2015,interp: 47.8659,fusion: -305.3259]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  162.47369384765625    \n",
      "module.ans_embedding.weight  dot:  316915.46875    \n",
      "module.lstm.weight_ih_l0  dot:  106884.765625    \n",
      "module.lstm.weight_hh_l0  dot:  7292.884765625    \n",
      "module.lstm.bias_ih_l0  dot:  9021.611328125    \n",
      "module.lstm.bias_hh_l0  dot:  9021.611328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48224608.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6025.78369140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4518726.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4518726.0    \n",
      "module.adapter.frcn_linear.weight  dot:  259139200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101571.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6187095.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1733.08984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12978146.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  114712432.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  97986.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.27071475982666    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.4708452224731445    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.342483520507812    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.904965322793942e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  140483.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  97986.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5.235811233520508    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.652569055557251    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5.48534631729126    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9454660105111543e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31854076.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57530272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452320.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  106/6933] Loss: 102.8458 [iq: 46.8588,ans: 319.7061,interp: 33.9510,fusion: -297.6701]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  208.8429412841797    \n",
      "module.ans_embedding.weight  dot:  185676.640625    \n",
      "module.lstm.weight_ih_l0  dot:  117113.59375    \n",
      "module.lstm.weight_hh_l0  dot:  7483.765625    \n",
      "module.lstm.bias_ih_l0  dot:  9389.048828125    \n",
      "module.lstm.bias_hh_l0  dot:  9389.048828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37716264.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14624.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4466060.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4466060.0    \n",
      "module.adapter.frcn_linear.weight  dot:  255088256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  98851.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7913152.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2180.20849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  16572279.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  111209840.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95521.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.256317138671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.569901466369629    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  25.219478607177734    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3857803793371204e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143651.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95521.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  456.26214599609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  134.81320190429688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  418.070068359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.5367263845055277e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26646888.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57390512.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452321.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  107/6933] Loss: 100.0977 [iq: 55.0046,ans: 328.8774,interp: 16.0672,fusion: -299.8516]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  198.51397705078125    \n",
      "module.ans_embedding.weight  dot:  233536.75    \n",
      "module.lstm.weight_ih_l0  dot:  133990.40625    \n",
      "module.lstm.weight_hh_l0  dot:  8920.5703125    \n",
      "module.lstm.bias_ih_l0  dot:  10643.384765625    \n",
      "module.lstm.bias_hh_l0  dot:  10643.384765625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51355704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2380.568603515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4698627.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4698627.0    \n",
      "module.adapter.frcn_linear.weight  dot:  277960384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  113459.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5887793.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1737.53271484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12107248.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  123550832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  114377.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.726025581359863    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.21311354637146    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21.201839447021484    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  173742.078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  114377.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13.173648834228516    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.126749038696289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15.850552558898926    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33460836.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  65001304.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452322.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  108/6933] Loss: 64.7364 [iq: 59.2861,ans: 291.4512,interp: 16.7806,fusion: -302.7815]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  214.5057830810547    \n",
      "module.ans_embedding.weight  dot:  269451.3125    \n",
      "module.lstm.weight_ih_l0  dot:  130999.359375    \n",
      "module.lstm.weight_hh_l0  dot:  8486.07421875    \n",
      "module.lstm.bias_ih_l0  dot:  10466.01953125    \n",
      "module.lstm.bias_hh_l0  dot:  10466.01953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45130064.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9619.51953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4757055.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4757055.0    \n",
      "module.adapter.frcn_linear.weight  dot:  269626560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102398.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5880023.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1586.090087890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10399782.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  116514528.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  104694.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.762189865112305    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.078456163406372    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.398544311523438    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  154840.171875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  104694.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  24.93231201171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.56367301940918    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  26.563684463500977    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1951328815484885e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28473864.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56284408.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452323.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  109/6933] Loss: 45.0569 [iq: 56.4036,ans: 269.7537,interp: 23.9950,fusion: -305.0954]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  239.4518585205078    \n",
      "module.ans_embedding.weight  dot:  260277.90625    \n",
      "module.lstm.weight_ih_l0  dot:  136128.5625    \n",
      "module.lstm.weight_hh_l0  dot:  8193.5322265625    \n",
      "module.lstm.bias_ih_l0  dot:  10592.193359375    \n",
      "module.lstm.bias_hh_l0  dot:  10592.193359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47823232.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6401.767578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4980504.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4980504.5    \n",
      "module.adapter.frcn_linear.weight  dot:  274268928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107684.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  7304658.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2023.5006103515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  14167495.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  116746720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  108779.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.271072387695312    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.3152623176574707    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21.534400939941406    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.377298440909726e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  161376.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  108779.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  27.194292068481445    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.653315544128418    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  28.194149017333984    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.403677505455562e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29576762.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56715328.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452324.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  110/6933] Loss: 0.0540 [iq: 51.7738,ans: 245.4694,interp: 11.8521,fusion: -309.0412]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  274.42498779296875    \n",
      "module.ans_embedding.weight  dot:  487322.09375    \n",
      "module.lstm.weight_ih_l0  dot:  150586.625    \n",
      "module.lstm.weight_hh_l0  dot:  9875.1376953125    \n",
      "module.lstm.bias_ih_l0  dot:  12427.173828125    \n",
      "module.lstm.bias_hh_l0  dot:  12427.173828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  62024924.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12253.4755859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6067399.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6067399.0    \n",
      "module.adapter.frcn_linear.weight  dot:  265357680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  112068.3984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4572473.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1414.1512451171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9838386.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  121306032.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124304.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16.765350341796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.8070719242095947    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  27.080373764038086    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.72937269744034e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  176722.640625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124304.7890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  80.72653198242188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.476533889770508    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  83.16203308105469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  35128428.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59824668.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452325.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  111/6933] Loss: -30.9355 [iq: 44.5059,ans: 208.5140,interp: 19.5000,fusion: -303.4554]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  247.68728637695312    \n",
      "module.ans_embedding.weight  dot:  241564.515625    \n",
      "module.lstm.weight_ih_l0  dot:  157978.40625    \n",
      "module.lstm.weight_hh_l0  dot:  10563.58984375    \n",
      "module.lstm.bias_ih_l0  dot:  12563.27734375    \n",
      "module.lstm.bias_hh_l0  dot:  12563.27734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49387392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21304.02734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5435815.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5435815.0    \n",
      "module.adapter.frcn_linear.weight  dot:  275083968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111543.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6130907.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1603.2857666015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13032310.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  121385224.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  121250.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19.437969207763672    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.153059720993042    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  29.55441665649414    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0039525594484076e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  183017.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  121250.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  44.42463684082031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.63066291809082    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  49.39645004272461    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7408297026122455e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29614540.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57317112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452325.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  112/6933] Loss: 7.4812 [iq: 58.9325,ans: 219.4969,interp: 41.5933,fusion: -312.5416]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  225.8983154296875    \n",
      "module.ans_embedding.weight  dot:  260514.375    \n",
      "module.lstm.weight_ih_l0  dot:  138319.1875    \n",
      "module.lstm.weight_hh_l0  dot:  7836.525390625    \n",
      "module.lstm.bias_ih_l0  dot:  11219.1767578125    \n",
      "module.lstm.bias_hh_l0  dot:  11219.1767578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49516944.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3524.117919921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4991548.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4991548.5    \n",
      "module.adapter.frcn_linear.weight  dot:  288158272.0    \n",
      "module.adapter.frcn_linear.bias  dot:  115933.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6885026.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1973.0648193359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  15042252.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  124211168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122836.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.408764839172363    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.3786873817443848    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  23.672504425048828    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  167984.828125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122836.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12.662800788879395    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.7038326263427734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  17.352439880371094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  30089294.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56189080.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452326.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  113/6933] Loss: -31.9979 [iq: 51.4190,ans: 209.3339,interp: 26.9921,fusion: -319.7430]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  276.9277648925781    \n",
      "module.ans_embedding.weight  dot:  439064.34375    \n",
      "module.lstm.weight_ih_l0  dot:  153275.96875    \n",
      "module.lstm.weight_hh_l0  dot:  9091.1318359375    \n",
      "module.lstm.bias_ih_l0  dot:  12283.17578125    \n",
      "module.lstm.bias_hh_l0  dot:  12283.17578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  56746732.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10310.8603515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5619475.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5619475.0    \n",
      "module.adapter.frcn_linear.weight  dot:  274766432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  110356.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5359561.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1538.47705078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10443996.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  120209016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122366.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.44856071472168    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.2714200019836426    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  20.12002944946289    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  173036.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122366.0234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  296.71844482421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  113.19458770751953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  372.731201171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  30657892.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52649240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452327.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  114/6933] Loss: -75.4886 [iq: 46.9599,ans: 174.3368,interp: 13.9636,fusion: -310.7489]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  247.28611755371094    \n",
      "module.ans_embedding.weight  dot:  374082.0    \n",
      "module.lstm.weight_ih_l0  dot:  172517.25    \n",
      "module.lstm.weight_hh_l0  dot:  11463.9765625    \n",
      "module.lstm.bias_ih_l0  dot:  14233.9140625    \n",
      "module.lstm.bias_hh_l0  dot:  14233.9140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  59988228.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12665.341796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5915546.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5915546.0    \n",
      "module.adapter.frcn_linear.weight  dot:  285632736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  118729.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4647356.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1564.55224609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9919735.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  124676160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  135361.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16.82758331298828    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.0255160331726074    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  26.425081253051758    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2650770148402444e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  191260.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  135361.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  54.79569625854492    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.39337730407715    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  58.69852828979492    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33158146.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57670376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452328.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  115/6933] Loss: -75.8827 [iq: 58.7534,ans: 168.2000,interp: 13.8129,fusion: -316.6491]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  287.8094177246094    \n",
      "module.ans_embedding.weight  dot:  411854.375    \n",
      "module.lstm.weight_ih_l0  dot:  159322.90625    \n",
      "module.lstm.weight_hh_l0  dot:  10290.4375    \n",
      "module.lstm.bias_ih_l0  dot:  12738.646484375    \n",
      "module.lstm.bias_hh_l0  dot:  12738.646484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  65305572.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4803.46484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6065641.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6065641.0    \n",
      "module.adapter.frcn_linear.weight  dot:  305502752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119010.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8550210.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2438.9404296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  17336436.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  128523576.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133821.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.470394134521484    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.8363795280456543    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  22.77749252319336    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  193347.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133821.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  138.37350463867188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  50.57518005371094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  157.78326416015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34529580.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57313532.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452328.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  116/6933] Loss: -103.1322 [iq: 46.7128,ans: 149.1573,interp: 12.0434,fusion: -311.0457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  246.79103088378906    \n",
      "module.ans_embedding.weight  dot:  726329.8125    \n",
      "module.lstm.weight_ih_l0  dot:  160018.890625    \n",
      "module.lstm.weight_hh_l0  dot:  10245.724609375    \n",
      "module.lstm.bias_ih_l0  dot:  13210.8359375    \n",
      "module.lstm.bias_hh_l0  dot:  13210.8359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  70499000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6687.92724609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6145820.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6145820.5    \n",
      "module.adapter.frcn_linear.weight  dot:  265212080.0    \n",
      "module.adapter.frcn_linear.bias  dot:  110168.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4891680.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1400.434326171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8524842.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  119126288.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133510.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.01842975616455    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.4581661224365234    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21.59546661376953    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  184126.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133510.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  202.59837341308594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  59.905426025390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  177.73912048339844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34374404.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52253932.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452329.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  117/6933] Loss: -126.2689 [iq: 43.4065,ans: 124.8581,interp: 11.4359,fusion: -305.9695]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  301.59832763671875    \n",
      "module.ans_embedding.weight  dot:  332953.75    \n",
      "module.lstm.weight_ih_l0  dot:  174063.234375    \n",
      "module.lstm.weight_hh_l0  dot:  11451.697265625    \n",
      "module.lstm.bias_ih_l0  dot:  14672.4267578125    \n",
      "module.lstm.bias_hh_l0  dot:  14672.4267578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50981112.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18867.5    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6070360.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6070360.0    \n",
      "module.adapter.frcn_linear.weight  dot:  293318272.0    \n",
      "module.adapter.frcn_linear.bias  dot:  122731.6640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6160432.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2037.970703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10668973.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  116925792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141507.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18.511886596679688    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.227917194366455    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  28.425655364990234    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  195329.984375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141507.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  320.1669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  105.60179138183594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  262.0920104980469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.672262990534364e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28995618.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  54328376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452330.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  118/6933] Loss: -108.1182 [iq: 42.4430,ans: 152.7348,interp: 28.7449,fusion: -332.0410]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  216.635009765625    \n",
      "module.ans_embedding.weight  dot:  489046.875    \n",
      "module.lstm.weight_ih_l0  dot:  146213.09375    \n",
      "module.lstm.weight_hh_l0  dot:  8240.4873046875    \n",
      "module.lstm.bias_ih_l0  dot:  11789.98828125    \n",
      "module.lstm.bias_hh_l0  dot:  11789.98828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  61657008.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4467.5556640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6009916.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6009916.0    \n",
      "module.adapter.frcn_linear.weight  dot:  301920448.0    \n",
      "module.adapter.frcn_linear.bias  dot:  122602.265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5528787.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1790.54296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10567999.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  120064000.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136550.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.856813430786133    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8704135417938232    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.918513298034668    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6867397195928788e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  183034.859375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136550.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  33.84581756591797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.122014045715332    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  32.094276428222656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33708488.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  55645848.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452331.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  119/6933] Loss: -121.9654 [iq: 47.8841,ans: 127.6225,interp: 28.3273,fusion: -325.7992]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  256.906494140625    \n",
      "module.ans_embedding.weight  dot:  463025.8125    \n",
      "module.lstm.weight_ih_l0  dot:  161866.28125    \n",
      "module.lstm.weight_hh_l0  dot:  10418.716796875    \n",
      "module.lstm.bias_ih_l0  dot:  13508.97265625    \n",
      "module.lstm.bias_hh_l0  dot:  13508.97265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60000332.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15765.08203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6679797.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6679797.0    \n",
      "module.adapter.frcn_linear.weight  dot:  263159840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  115550.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4111772.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1358.3822021484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8017257.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  107991728.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  139254.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.104009628295898    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.6807169914245605    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  22.013010025024414    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  190529.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  139254.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  402.8362731933594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  141.93255615234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  434.5615234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.916866830877552e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  32072278.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  54012192.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452331.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  120/6933] Loss: -146.8440 [iq: 50.0476,ans: 122.5762,interp: 15.4128,fusion: -334.8806]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  310.9332275390625    \n",
      "module.ans_embedding.weight  dot:  548020.125    \n",
      "module.lstm.weight_ih_l0  dot:  173229.890625    \n",
      "module.lstm.weight_hh_l0  dot:  9643.8125    \n",
      "module.lstm.bias_ih_l0  dot:  14129.11328125    \n",
      "module.lstm.bias_hh_l0  dot:  14129.11328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  71387776.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30417.04296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6839524.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6839524.0    \n",
      "module.adapter.frcn_linear.weight  dot:  282433856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  116946.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4139226.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1349.3787841796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6974306.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  117478416.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  146910.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.863188743591309    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.033933162689209    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  22.226234436035156    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  191676.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  146910.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  78.47591400146484    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.267471313476562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  80.06910705566406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8387513733841843e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34640112.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52764604.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452332.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  121/6933] Loss: -170.4897 [iq: 42.6586,ans: 102.1634,interp: 10.3989,fusion: -325.7106]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  359.04229736328125    \n",
      "module.ans_embedding.weight  dot:  399747.90625    \n",
      "module.lstm.weight_ih_l0  dot:  192859.125    \n",
      "module.lstm.weight_hh_l0  dot:  12498.16015625    \n",
      "module.lstm.bias_ih_l0  dot:  16001.291015625    \n",
      "module.lstm.bias_hh_l0  dot:  16001.291015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50402992.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1708.316162109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5945597.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5945597.0    \n",
      "module.adapter.frcn_linear.weight  dot:  318915648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  131786.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  8318718.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3162.76025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  13652954.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  116104096.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  158649.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22.186458587646484    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  4.05610990524292    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  32.06851577758789    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  216467.609375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  158649.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  73.34514617919922    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.023353576660156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  85.0054931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29172558.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56054480.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452333.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  122/6933] Loss: -141.8641 [iq: 50.6467,ans: 129.7480,interp: 24.4081,fusion: -346.6669]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  300.89764404296875    \n",
      "module.ans_embedding.weight  dot:  563880.5    \n",
      "module.lstm.weight_ih_l0  dot:  159624.125    \n",
      "module.lstm.weight_hh_l0  dot:  9664.2890625    \n",
      "module.lstm.bias_ih_l0  dot:  13431.443359375    \n",
      "module.lstm.bias_hh_l0  dot:  13431.443359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  79188824.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40749.5    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7361095.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7361095.0    \n",
      "module.adapter.frcn_linear.weight  dot:  258953536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  106308.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4329198.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1360.16650390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7954996.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  110072176.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141543.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.605669021606445    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7650346755981445    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  20.357757568359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  183230.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141543.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  133.3900146484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.66758728027344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  133.62672424316406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36061512.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53026032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452333.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  123/6933] Loss: -175.5939 [iq: 44.4399,ans: 89.1607,interp: 23.4674,fusion: -332.6619]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  288.55072021484375    \n",
      "module.ans_embedding.weight  dot:  535740.125    \n",
      "module.lstm.weight_ih_l0  dot:  188308.8125    \n",
      "module.lstm.weight_hh_l0  dot:  11981.859375    \n",
      "module.lstm.bias_ih_l0  dot:  15370.7431640625    \n",
      "module.lstm.bias_hh_l0  dot:  15370.7431640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  55152112.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4111.2421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6427011.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6427011.0    \n",
      "module.adapter.frcn_linear.weight  dot:  305834304.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127399.3203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5179347.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1933.8443603515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8874732.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  115148576.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  157569.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17.44533920288086    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.1486456394195557    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  24.483985900878906    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  214981.03125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  157569.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  101.40306091308594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  29.687707901000977    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  109.8768310546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31121648.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  55461492.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452334.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  124/6933] Loss: -163.8782 [iq: 43.8102,ans: 111.4726,interp: 22.7415,fusion: -341.9025]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  260.42120361328125    \n",
      "module.ans_embedding.weight  dot:  532508.25    \n",
      "module.lstm.weight_ih_l0  dot:  160034.84375    \n",
      "module.lstm.weight_hh_l0  dot:  9694.208984375    \n",
      "module.lstm.bias_ih_l0  dot:  13032.1796875    \n",
      "module.lstm.bias_hh_l0  dot:  13032.1796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  79381192.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8598.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6985458.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6985458.0    \n",
      "module.adapter.frcn_linear.weight  dot:  273226592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111556.5859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3484719.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1290.689208984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7019949.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  110362712.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  147273.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.679588317871094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.0282769203186035    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.84151554107666    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  193259.796875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  147273.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29.56436538696289    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.507268905639648    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  30.31940269470215    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37211768.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53933344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452334.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  125/6933] Loss: -191.8818 [iq: 47.7129,ans: 84.6728,interp: 20.3824,fusion: -344.6499]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  238.89979553222656    \n",
      "module.ans_embedding.weight  dot:  684680.4375    \n",
      "module.lstm.weight_ih_l0  dot:  166490.140625    \n",
      "module.lstm.weight_hh_l0  dot:  9258.716796875    \n",
      "module.lstm.bias_ih_l0  dot:  13306.095703125    \n",
      "module.lstm.bias_hh_l0  dot:  13306.095703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  70277096.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13825.5625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7220315.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7220315.0    \n",
      "module.adapter.frcn_linear.weight  dot:  265721440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111631.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3413831.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1205.2276611328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6564692.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  104843120.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  150560.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.250930786132812    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8613606691360474    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.378974914550781    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0746958878371515e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  195058.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  150560.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  327.64605712890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  97.07796478271484    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  360.07843017578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34976916.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  55736496.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452335.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  126/6933] Loss: -194.4867 [iq: 39.1341,ans: 85.5952,interp: 28.2930,fusion: -347.5090]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  282.3515930175781    \n",
      "module.ans_embedding.weight  dot:  525100.875    \n",
      "module.lstm.weight_ih_l0  dot:  180155.109375    \n",
      "module.lstm.weight_hh_l0  dot:  10469.47265625    \n",
      "module.lstm.bias_ih_l0  dot:  14785.314453125    \n",
      "module.lstm.bias_hh_l0  dot:  14785.314453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60495808.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14431.14453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7020992.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7020992.0    \n",
      "module.adapter.frcn_linear.weight  dot:  291498624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  122160.8515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3636924.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1450.58447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7964841.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  113106432.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  165596.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.685239791870117    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.579469680786133    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.790618896484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  211432.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  165596.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  43.309852600097656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.092249870300293    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  56.817237854003906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  32845724.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58384600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452336.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  127/6933] Loss: -197.1402 [iq: 46.9789,ans: 91.5716,interp: 20.0001,fusion: -355.6908]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  236.28805541992188    \n",
      "module.ans_embedding.weight  dot:  575951.8125    \n",
      "module.lstm.weight_ih_l0  dot:  156392.5625    \n",
      "module.lstm.weight_hh_l0  dot:  8608.8486328125    \n",
      "module.lstm.bias_ih_l0  dot:  12829.4423828125    \n",
      "module.lstm.bias_hh_l0  dot:  12829.4423828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  80770304.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14425.3115234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7319021.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7319021.0    \n",
      "module.adapter.frcn_linear.weight  dot:  269359616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111150.2421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4004807.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1470.3602294921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7391189.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  102983616.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  152497.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.287554740905762    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8181357383728027    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.901018142700195    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.105604745063829e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  190695.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  152497.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  75.98079681396484    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.9305419921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  65.56950378417969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37943096.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  54691384.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452336.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  128/6933] Loss: -209.0042 [iq: 46.4181,ans: 69.2694,interp: 27.0519,fusion: -351.7436]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  249.6814422607422    \n",
      "module.ans_embedding.weight  dot:  554320.4375    \n",
      "module.lstm.weight_ih_l0  dot:  179085.40625    \n",
      "module.lstm.weight_hh_l0  dot:  10701.5986328125    \n",
      "module.lstm.bias_ih_l0  dot:  14705.60546875    \n",
      "module.lstm.bias_hh_l0  dot:  14705.60546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  73846912.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7823.6728515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7081847.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7081847.0    \n",
      "module.adapter.frcn_linear.weight  dot:  269719168.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119790.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5000227.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2063.81982421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9258780.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  103199776.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  171387.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.217671394348145    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.1161532402038574    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.275679588317871    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0039525594484076e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  220421.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  171387.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  105.04734802246094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.913063049316406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  103.50700378417969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  35554816.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  55762528.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452337.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  129/6933] Loss: -211.2876 [iq: 40.6745,ans: 74.8553,interp: 28.9760,fusion: -355.7934]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  327.74920654296875    \n",
      "module.ans_embedding.weight  dot:  916176.125    \n",
      "module.lstm.weight_ih_l0  dot:  192436.171875    \n",
      "module.lstm.weight_hh_l0  dot:  11122.251953125    \n",
      "module.lstm.bias_ih_l0  dot:  15983.181640625    \n",
      "module.lstm.bias_hh_l0  dot:  15983.181640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  80776144.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4710.38671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7392318.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7392318.0    \n",
      "module.adapter.frcn_linear.weight  dot:  249395328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  103945.5234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3779367.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1656.3309326171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7382932.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  96023312.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  161629.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.752530097961426    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.742889881134033    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  18.195796966552734    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.879385536085465e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  200409.390625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  161629.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  65.26203918457031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.248294830322266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  67.948974609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  35805896.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53588708.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452337.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  130/6933] Loss: -231.8850 [iq: 36.8171,ans: 67.9101,interp: 13.9458,fusion: -350.5580]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  266.1359558105469    \n",
      "module.ans_embedding.weight  dot:  664597.125    \n",
      "module.lstm.weight_ih_l0  dot:  182850.6875    \n",
      "module.lstm.weight_hh_l0  dot:  10831.982421875    \n",
      "module.lstm.bias_ih_l0  dot:  14890.1005859375    \n",
      "module.lstm.bias_hh_l0  dot:  14890.1005859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  78888728.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20414.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7967037.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7967037.0    \n",
      "module.adapter.frcn_linear.weight  dot:  273629376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  117962.1484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2744963.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1073.1197509765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5747215.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  98435888.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  169073.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.099040985107422    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.3554580211639404    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.517704010009766    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  217880.453125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  169073.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8.483613967895508    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.373589515686035    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9.043081283569336    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38791576.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58543376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452338.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  131/6933] Loss: -237.0772 [iq: 39.0614,ans: 68.9313,interp: 17.9333,fusion: -363.0032]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  215.62689208984375    \n",
      "module.ans_embedding.weight  dot:  670558.875    \n",
      "module.lstm.weight_ih_l0  dot:  152668.96875    \n",
      "module.lstm.weight_hh_l0  dot:  9446.6435546875    \n",
      "module.lstm.bias_ih_l0  dot:  12873.091796875    \n",
      "module.lstm.bias_hh_l0  dot:  12873.091796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  80036448.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12987.291015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8072323.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8072323.0    \n",
      "module.adapter.frcn_linear.weight  dot:  228490432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  97893.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2636608.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1184.488037109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5577250.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  86118712.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  147943.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.684447288513184    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.863844871520996    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.611496925354004    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  184949.609375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  147943.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1852.6920166015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  656.1544189453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1880.878173828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36889152.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  57241796.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452338.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  132/6933] Loss: -253.6816 [iq: 40.1576,ans: 67.4153,interp: 15.7050,fusion: -376.9595]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  267.8083801269531    \n",
      "module.ans_embedding.weight  dot:  501903.9375    \n",
      "module.lstm.weight_ih_l0  dot:  170700.0    \n",
      "module.lstm.weight_hh_l0  dot:  9635.5400390625    \n",
      "module.lstm.bias_ih_l0  dot:  13897.4716796875    \n",
      "module.lstm.bias_hh_l0  dot:  13897.4716796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  83497752.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17061.462890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8236297.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8236297.0    \n",
      "module.adapter.frcn_linear.weight  dot:  256013984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  106539.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2773075.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1099.0928955078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5685002.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  98100848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  165826.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.389266967773438    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.8019754886627197    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.11578369140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.377298440909726e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  206619.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  165826.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  530.11669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  215.33779907226562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  553.83251953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  39158876.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60189496.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452339.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  133/6933] Loss: -218.7022 [iq: 45.4174,ans: 66.2299,interp: 41.6557,fusion: -372.0051]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  299.20147705078125    \n",
      "module.ans_embedding.weight  dot:  922467.875    \n",
      "module.lstm.weight_ih_l0  dot:  199694.59375    \n",
      "module.lstm.weight_hh_l0  dot:  10414.052734375    \n",
      "module.lstm.bias_ih_l0  dot:  16113.341796875    \n",
      "module.lstm.bias_hh_l0  dot:  16113.341796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  86657600.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2854.9296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7903409.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7903409.0    \n",
      "module.adapter.frcn_linear.weight  dot:  316740448.0    \n",
      "module.adapter.frcn_linear.bias  dot:  137010.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4173950.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1738.064697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8024546.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  105001624.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  198067.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.171992301940918    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.370487928390503    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.79410171508789    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.474820679613913e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  239508.796875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  198067.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8.065753936767578    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.8873364925384521    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10.54342269897461    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41793204.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58850360.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452339.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  134/6933] Loss: -252.3122 [iq: 27.1097,ans: 55.6741,interp: 22.8202,fusion: -357.9163]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  223.43338012695312    \n",
      "module.ans_embedding.weight  dot:  647969.6875    \n",
      "module.lstm.weight_ih_l0  dot:  178426.703125    \n",
      "module.lstm.weight_hh_l0  dot:  10112.30859375    \n",
      "module.lstm.bias_ih_l0  dot:  14483.8828125    \n",
      "module.lstm.bias_hh_l0  dot:  14483.8828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  73979856.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6429.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8274558.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8274558.0    \n",
      "module.adapter.frcn_linear.weight  dot:  253301664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  108465.6171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2689561.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1297.25244140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5223723.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  88672328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  169843.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.678058624267578    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.04362154006958    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.64413833618164    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  208809.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  169843.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  160.48196411132812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.458824157714844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  181.39956665039062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38426744.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  63446124.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452340.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  135/6933] Loss: -255.2361 [iq: 44.2007,ans: 68.4496,interp: 16.4059,fusion: -384.2921]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  180.74774169921875    \n",
      "module.ans_embedding.weight  dot:  718920.1875    \n",
      "module.lstm.weight_ih_l0  dot:  148081.65625    \n",
      "module.lstm.weight_hh_l0  dot:  7387.4931640625    \n",
      "module.lstm.bias_ih_l0  dot:  11861.6015625    \n",
      "module.lstm.bias_hh_l0  dot:  11861.6015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  84464176.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10129.7626953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8290572.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8290572.0    \n",
      "module.adapter.frcn_linear.weight  dot:  220263536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96812.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1414601.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  633.0999145507812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2726086.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  80604624.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  158696.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.787597179412842    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2668232917785645    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.29965591430664    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  187807.328125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  158696.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  138.6033935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.092758178710938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  162.74453735351562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8098911258590533e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  39601704.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61778432.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452340.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  136/6933] Loss: -259.5749 [iq: 45.5606,ans: 62.0587,interp: 13.0156,fusion: -380.2097]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  179.58294677734375    \n",
      "module.ans_embedding.weight  dot:  536538.8125    \n",
      "module.lstm.weight_ih_l0  dot:  148968.75    \n",
      "module.lstm.weight_hh_l0  dot:  8374.380859375    \n",
      "module.lstm.bias_ih_l0  dot:  12028.240234375    \n",
      "module.lstm.bias_hh_l0  dot:  12028.240234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  69504416.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8914.6943359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8003809.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8003809.5    \n",
      "module.adapter.frcn_linear.weight  dot:  242087616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  100532.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1823413.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  814.0894165039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3473047.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  86222192.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  166023.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.943548679351807    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.205588459968567    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.241437911987305    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  201885.546875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  166023.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  44.090248107910156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.661226272583008    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  43.13106155395508    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37063768.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  66970204.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452341.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  137/6933] Loss: -266.6348 [iq: 37.5381,ans: 75.9170,interp: 20.4692,fusion: -400.5591]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  213.88763427734375    \n",
      "module.ans_embedding.weight  dot:  949372.5    \n",
      "module.lstm.weight_ih_l0  dot:  145298.03125    \n",
      "module.lstm.weight_hh_l0  dot:  7578.44482421875    \n",
      "module.lstm.bias_ih_l0  dot:  11756.455078125    \n",
      "module.lstm.bias_hh_l0  dot:  11756.455078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  93339400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11470.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8417801.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8417801.0    \n",
      "module.adapter.frcn_linear.weight  dot:  178751872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  76525.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1373953.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  642.558837890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2456563.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  67671648.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141736.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.493538856506348    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.3532898426055908    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.952903747558594    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  166669.578125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141736.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  128.105712890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.948652267456055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  169.95037841796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  40811000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58198448.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452341.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  138/6933] Loss: -262.5687 [iq: 40.8987,ans: 49.6473,interp: 28.1148,fusion: -381.2295]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  168.49948120117188    \n",
      "module.ans_embedding.weight  dot:  878432.75    \n",
      "module.lstm.weight_ih_l0  dot:  130172.1640625    \n",
      "module.lstm.weight_hh_l0  dot:  7225.8115234375    \n",
      "module.lstm.bias_ih_l0  dot:  10650.9990234375    \n",
      "module.lstm.bias_hh_l0  dot:  10650.9990234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  95582000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10618.1591796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9004233.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9004233.0    \n",
      "module.adapter.frcn_linear.weight  dot:  174545504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  76795.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  965947.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  479.392578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1592367.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.143885234952904e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  63712212.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  140901.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.423036098480225    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8584178686141968    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.012607574462891    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  166964.40625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  140901.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  877.6826171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  342.9652099609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  988.9993286132812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  44080480.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  62928472.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452342.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  139/6933] Loss: -287.4825 [iq: 35.2877,ans: 53.2466,interp: 16.5024,fusion: -392.5193]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  220.5736083984375    \n",
      "module.ans_embedding.weight  dot:  879606.6875    \n",
      "module.lstm.weight_ih_l0  dot:  149004.34375    \n",
      "module.lstm.weight_hh_l0  dot:  7533.7841796875    \n",
      "module.lstm.bias_ih_l0  dot:  12103.8251953125    \n",
      "module.lstm.bias_hh_l0  dot:  12103.8251953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  102119344.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14544.4677734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9484344.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9484344.0    \n",
      "module.adapter.frcn_linear.weight  dot:  194316992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  86551.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  787040.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  388.3613586425781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1426062.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  64699184.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  154525.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.955729961395264    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.4589046239852905    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.945724487304688    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.792167077193881e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  175618.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  154525.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  909.2791748046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  329.2059326171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1089.054931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  44581128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61906120.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452342.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  140/6933] Loss: -296.1827 [iq: 28.6204,ans: 47.5103,interp: 13.1065,fusion: -385.4200]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  180.82017517089844    \n",
      "module.ans_embedding.weight  dot:  882417.25    \n",
      "module.lstm.weight_ih_l0  dot:  160687.0625    \n",
      "module.lstm.weight_hh_l0  dot:  9283.693359375    \n",
      "module.lstm.bias_ih_l0  dot:  12799.708984375    \n",
      "module.lstm.bias_hh_l0  dot:  12799.708984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  91743440.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14190.07421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9099019.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9099019.0    \n",
      "module.adapter.frcn_linear.weight  dot:  205928320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  89525.3515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  654183.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.40399169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1060113.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  68896048.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160146.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.891944408416748    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9399799108505249    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.346318244934082    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.474820679613913e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  197435.359375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160146.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  120.8692626953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.461700439453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  121.34541320800781    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.568967592102126e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41789584.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  65152368.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452343.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  141/6933] Loss: -284.0580 [iq: 34.0754,ans: 55.9242,interp: 17.4274,fusion: -391.4849]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  195.1374969482422    \n",
      "module.ans_embedding.weight  dot:  832839.9375    \n",
      "module.lstm.weight_ih_l0  dot:  135162.875    \n",
      "module.lstm.weight_hh_l0  dot:  7533.166015625    \n",
      "module.lstm.bias_ih_l0  dot:  10987.1142578125    \n",
      "module.lstm.bias_hh_l0  dot:  10987.1142578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  100048576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21613.74609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10033300.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10033300.0    \n",
      "module.adapter.frcn_linear.weight  dot:  153657136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  68191.5703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  980903.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  505.24810791015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1856669.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  54614272.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136532.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.876911163330078    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2342357635498047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.54722785949707    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  160532.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136532.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  133.38754272460938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.56970977783203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  121.30337524414062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  43464200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  68145976.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452343.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  142/6933] Loss: -278.9988 [iq: 33.3156,ans: 53.6814,interp: 28.1454,fusion: -394.1412]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  170.83616638183594    \n",
      "module.ans_embedding.weight  dot:  633818.4375    \n",
      "module.lstm.weight_ih_l0  dot:  143885.21875    \n",
      "module.lstm.weight_hh_l0  dot:  7934.65283203125    \n",
      "module.lstm.bias_ih_l0  dot:  11557.9013671875    \n",
      "module.lstm.bias_hh_l0  dot:  11557.9013671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  82960512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2722.7587890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9666632.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9666632.0    \n",
      "module.adapter.frcn_linear.weight  dot:  295741056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  128996.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1052377.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  507.109130859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1496828.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  69049344.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  168780.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.9609923362731934    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5346847772598267    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.969726085662842    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.812950369474493e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  198370.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  168780.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  53.85742950439453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.216915130615234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  63.46368408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42671752.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  75826696.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452344.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  143/6933] Loss: -293.7973 [iq: 36.8999,ans: 58.0720,interp: 13.2878,fusion: -402.0569]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  159.80992126464844    \n",
      "module.ans_embedding.weight  dot:  1241238.875    \n",
      "module.lstm.weight_ih_l0  dot:  125525.546875    \n",
      "module.lstm.weight_hh_l0  dot:  6592.3623046875    \n",
      "module.lstm.bias_ih_l0  dot:  10667.978515625    \n",
      "module.lstm.bias_hh_l0  dot:  10667.978515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113761360.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11653.53515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9078918.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9078918.0    \n",
      "module.adapter.frcn_linear.weight  dot:  214012016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93307.015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  638490.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  268.9034423828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  888506.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  59668352.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  152084.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.952047348022461    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5302542448043823    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.229553699493408    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  165128.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  152084.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  38.306461334228516    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.756304740905762    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  49.223846435546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  47706364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61489452.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452344.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  144/6933] Loss: -308.0001 [iq: 21.8574,ans: 36.4003,interp: 10.0209,fusion: -376.2786]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  159.9107208251953    \n",
      "module.ans_embedding.weight  dot:  1210764.0    \n",
      "module.lstm.weight_ih_l0  dot:  132168.21875    \n",
      "module.lstm.weight_hh_l0  dot:  6609.09716796875    \n",
      "module.lstm.bias_ih_l0  dot:  10813.841796875    \n",
      "module.lstm.bias_hh_l0  dot:  10813.841796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  108889792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24748.431640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9940041.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9940041.0    \n",
      "module.adapter.frcn_linear.weight  dot:  269123040.0    \n",
      "module.adapter.frcn_linear.bias  dot:  114997.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  659231.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  254.77459716796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  555064.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  64217892.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  169270.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.573744297027588    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7413190007209778    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.939657211303711    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  188188.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  169270.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  61.58755874633789    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.471302032470703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  81.00395965576172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  50778760.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  66966856.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452345.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  145/6933] Loss: -289.5424 [iq: 30.5968,ans: 41.9501,interp: 23.5716,fusion: -385.6608]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.53341674804688    \n",
      "module.ans_embedding.weight  dot:  974476.875    \n",
      "module.lstm.weight_ih_l0  dot:  115021.0546875    \n",
      "module.lstm.weight_hh_l0  dot:  5838.9189453125    \n",
      "module.lstm.bias_ih_l0  dot:  9503.4765625    \n",
      "module.lstm.bias_hh_l0  dot:  9503.4765625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  117138496.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4683.9677734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10277804.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10277804.0    \n",
      "module.adapter.frcn_linear.weight  dot:  225989856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102142.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  511853.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  180.1543731689453    \n",
      "module.attflat_img.mlp.linear.weight  dot:  497546.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  49433880.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137192.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.3644704818725586    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.41460323333740234    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.4059293270111084    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0880185641326534e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  150469.453125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137192.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  94.70796966552734    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.398584365844727    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  125.99162292480469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  50582456.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  70105880.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452345.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  146/6933] Loss: -300.4601 [iq: 32.9829,ans: 39.3512,interp: 21.8883,fusion: -394.6826]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  172.24090576171875    \n",
      "module.ans_embedding.weight  dot:  1027573.0625    \n",
      "module.lstm.weight_ih_l0  dot:  130522.53125    \n",
      "module.lstm.weight_hh_l0  dot:  6828.46728515625    \n",
      "module.lstm.bias_ih_l0  dot:  10623.6650390625    \n",
      "module.lstm.bias_hh_l0  dot:  10623.6650390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  100642576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13927.0703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10432232.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10432232.0    \n",
      "module.adapter.frcn_linear.weight  dot:  209775056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96509.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  514445.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  250.7924346923828    \n",
      "module.attflat_img.mlp.linear.weight  dot:  525868.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  50862912.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  150048.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.152127265930176    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7985115051269531    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.3602614402771    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0116907311896739e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  168929.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  150048.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  43.78293991088867    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.914522171020508    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  65.33441925048828    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  47191288.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  72816544.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452346.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  147/6933] Loss: -317.7824 [iq: 24.7142,ans: 48.4167,interp: 18.5981,fusion: -409.5114]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  192.01470947265625    \n",
      "module.ans_embedding.weight  dot:  1047839.3125    \n",
      "module.lstm.weight_ih_l0  dot:  131059.5625    \n",
      "module.lstm.weight_hh_l0  dot:  6830.49267578125    \n",
      "module.lstm.bias_ih_l0  dot:  10587.154296875    \n",
      "module.lstm.bias_hh_l0  dot:  10587.154296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  99180312.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19449.060546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10247866.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10247866.0    \n",
      "module.adapter.frcn_linear.weight  dot:  196162848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  89869.578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  739408.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  459.815185546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1600007.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  47634536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  146249.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.9775280952453613    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7853819131851196    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.7268834114074707    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1338486533295509e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  162024.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  146249.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  99.77047729492188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.79247283935547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  123.03009033203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46481464.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  70475568.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452346.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  148/6933] Loss: -305.8547 [iq: 29.4096,ans: 51.4111,interp: 20.0140,fusion: -406.6894]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125.58256530761719    \n",
      "module.ans_embedding.weight  dot:  1090953.375    \n",
      "module.lstm.weight_ih_l0  dot:  105382.21875    \n",
      "module.lstm.weight_hh_l0  dot:  5160.181640625    \n",
      "module.lstm.bias_ih_l0  dot:  8362.0234375    \n",
      "module.lstm.bias_hh_l0  dot:  8362.0234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  120480456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26163.7734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10825690.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10825690.0    \n",
      "module.adapter.frcn_linear.weight  dot:  247556384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102857.4609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  456036.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  162.52130126953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  438396.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  45419276.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  129563.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.6969038248062134    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.1381189525127411    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.6040847301483154    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4480417692984702e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143792.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  129563.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  120.59860229492188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.075340270996094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  139.4757537841797    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53819188.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73928344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452346.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  149/6933] Loss: -314.4576 [iq: 26.3234,ans: 37.6764,interp: 23.9325,fusion: -402.3898]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  159.23428344726562    \n",
      "module.ans_embedding.weight  dot:  1445102.75    \n",
      "module.lstm.weight_ih_l0  dot:  110436.8203125    \n",
      "module.lstm.weight_hh_l0  dot:  5359.35986328125    \n",
      "module.lstm.bias_ih_l0  dot:  9157.9365234375    \n",
      "module.lstm.bias_hh_l0  dot:  9157.9365234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  117537680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1594.4600830078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10087922.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10087922.0    \n",
      "module.adapter.frcn_linear.weight  dot:  219291888.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93992.421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  608642.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  212.58984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  610536.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  45493788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137262.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.7532250881195068    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3858546316623688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.473052740097046    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  142297.40625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137262.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  24.708492279052734    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.0055513381958    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  28.05347442626953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  52666412.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  67669624.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452347.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  150/6933] Loss: -328.4694 [iq: 20.5475,ans: 35.9473,interp: 15.3019,fusion: -400.2660]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  122.44246673583984    \n",
      "module.ans_embedding.weight  dot:  1425482.25    \n",
      "module.lstm.weight_ih_l0  dot:  90073.203125    \n",
      "module.lstm.weight_hh_l0  dot:  4528.349609375    \n",
      "module.lstm.bias_ih_l0  dot:  7218.65380859375    \n",
      "module.lstm.bias_hh_l0  dot:  7218.65380859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  119130000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30384.08203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11115046.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11115046.0    \n",
      "module.adapter.frcn_linear.weight  dot:  188813664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  82501.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  572670.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  341.9773864746094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  791057.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  35367136.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  115154.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.835735559463501    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.21270182728767395    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.6696764826774597    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  126357.015625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  115154.2109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  174.95425415039062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.02662658691406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  183.44955444335938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.206324095117452e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  51816148.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73278784.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452347.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  151/6933] Loss: -323.2166 [iq: 19.8458,ans: 47.5424,interp: 15.0571,fusion: -405.6619]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  128.69322204589844    \n",
      "module.ans_embedding.weight  dot:  1113576.5    \n",
      "module.lstm.weight_ih_l0  dot:  118707.921875    \n",
      "module.lstm.weight_hh_l0  dot:  6158.7421875    \n",
      "module.lstm.bias_ih_l0  dot:  10056.34375    \n",
      "module.lstm.bias_hh_l0  dot:  10056.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  111002848.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7298.2109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10890040.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10890040.0    \n",
      "module.adapter.frcn_linear.weight  dot:  443116864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  169222.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1321247.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  547.79296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1347643.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  55598584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  158177.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.30240172147750854    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.11264560371637344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.25334829092025757    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.996003610813204e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  164999.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  158177.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  201.9527130126953    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  61.09220504760742    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  288.5753479003906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  51704008.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82034544.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452348.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  152/6933] Loss: -306.4172 [iq: 29.9400,ans: 42.4948,interp: 25.4358,fusion: -404.2878]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  191.4086456298828    \n",
      "module.ans_embedding.weight  dot:  953549.625    \n",
      "module.lstm.weight_ih_l0  dot:  151783.421875    \n",
      "module.lstm.weight_hh_l0  dot:  7898.43603515625    \n",
      "module.lstm.bias_ih_l0  dot:  12153.3369140625    \n",
      "module.lstm.bias_hh_l0  dot:  12153.3369140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  102333632.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18989.27734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11661420.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11661420.0    \n",
      "module.adapter.frcn_linear.weight  dot:  362608864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  169388.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1175518.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  492.4044189453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1084612.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  48588000.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  172191.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.5346919298171997    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.31057149171829224    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.39058256149292    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  190946.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  172191.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  74.06407928466797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.418622970581055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  109.59518432617188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49474672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86279664.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452348.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  153/6933] Loss: -309.2374 [iq: 24.9509,ans: 50.5959,interp: 32.8279,fusion: -417.6120]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  132.04751586914062    \n",
      "module.ans_embedding.weight  dot:  836123.0    \n",
      "module.lstm.weight_ih_l0  dot:  100052.5703125    \n",
      "module.lstm.weight_hh_l0  dot:  5201.32177734375    \n",
      "module.lstm.bias_ih_l0  dot:  7933.0830078125    \n",
      "module.lstm.bias_hh_l0  dot:  7933.0830078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  118694824.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13433.35546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11560495.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11560495.0    \n",
      "module.adapter.frcn_linear.weight  dot:  166123232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  74838.0234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  192859.890625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  97.5987548828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  171608.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  31849168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  109560.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.6404521465301514    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3686644434928894    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.3842637538909912    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.642086632282826e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121913.0078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  109560.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  114.55764770507812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.71727752685547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  132.56219482421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49878944.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  79271152.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452349.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  154/6933] Loss: -321.7686 [iq: 29.2615,ans: 43.1587,interp: 22.8899,fusion: -417.0788]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125.02119445800781    \n",
      "module.ans_embedding.weight  dot:  1082223.625    \n",
      "module.lstm.weight_ih_l0  dot:  84038.359375    \n",
      "module.lstm.weight_hh_l0  dot:  4297.35546875    \n",
      "module.lstm.bias_ih_l0  dot:  6621.53662109375    \n",
      "module.lstm.bias_hh_l0  dot:  6621.53662109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  122295856.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10956.78125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11717322.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11717322.0    \n",
      "module.adapter.frcn_linear.weight  dot:  191510112.0    \n",
      "module.adapter.frcn_linear.bias  dot:  79885.0234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  547232.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  346.6988830566406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  784604.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.9454660105111543e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  32106778.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  111325.9609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.5916668176651001    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.177801713347435    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.40971890091896057    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.792167077193881e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121690.171875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  111325.9609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  117.66169738769531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.84165573120117    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  113.81976318359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.412026217120001e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  54256308.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80163616.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452349.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  155/6933] Loss: -332.8078 [iq: 26.8433,ans: 38.6849,interp: 18.0823,fusion: -416.4183]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  169.20603942871094    \n",
      "module.ans_embedding.weight  dot:  1263829.625    \n",
      "module.lstm.weight_ih_l0  dot:  129294.6015625    \n",
      "module.lstm.weight_hh_l0  dot:  6220.52978515625    \n",
      "module.lstm.bias_ih_l0  dot:  9824.173828125    \n",
      "module.lstm.bias_hh_l0  dot:  9824.173828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  130372656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9122.15234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11002023.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11002023.0    \n",
      "module.adapter.frcn_linear.weight  dot:  285050560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  131602.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  661980.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  268.44671630859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  636934.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  41030328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  157059.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.4880852699279785    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3303053677082062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0625076293945312    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2002898674978724e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  174866.359375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  157059.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  79.19923400878906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.67346954345703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  86.72383117675781    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57582876.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  74418832.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452350.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  156/6933] Loss: -340.9801 [iq: 22.5040,ans: 31.7668,interp: 13.3380,fusion: -408.5890]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  114.72671508789062    \n",
      "module.ans_embedding.weight  dot:  900961.0625    \n",
      "module.lstm.weight_ih_l0  dot:  95686.171875    \n",
      "module.lstm.weight_hh_l0  dot:  4604.7158203125    \n",
      "module.lstm.bias_ih_l0  dot:  7637.0869140625    \n",
      "module.lstm.bias_hh_l0  dot:  7637.0869140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  121145832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6176.4521484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11815611.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11815611.0    \n",
      "module.adapter.frcn_linear.weight  dot:  243572640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  100842.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  496063.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  225.17886352539062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  436994.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  33857120.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  117950.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.2275466918945312    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3535099923610687    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0303661823272705    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6867397195928788e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  125185.703125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  117950.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  44.48408508300781    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.679634094238281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  75.51942443847656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  52907520.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87599024.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452350.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  157/6933] Loss: -332.7594 [iq: 27.4479,ans: 43.4706,interp: 20.1921,fusion: -423.8700]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  150.2178955078125    \n",
      "module.ans_embedding.weight  dot:  1215268.875    \n",
      "module.lstm.weight_ih_l0  dot:  94760.5546875    \n",
      "module.lstm.weight_hh_l0  dot:  4694.33984375    \n",
      "module.lstm.bias_ih_l0  dot:  7289.625    \n",
      "module.lstm.bias_hh_l0  dot:  7289.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  125707392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20662.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11625879.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11625879.0    \n",
      "module.adapter.frcn_linear.weight  dot:  173872976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  81011.203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  380042.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  214.3821563720703    \n",
      "module.attflat_img.mlp.linear.weight  dot:  446202.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  27377776.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  109946.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.3486948013305664    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.4085612893104553    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.1007936000823975    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.124100812432971e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121164.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  109946.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  229.39259338378906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  63.595947265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  239.24574279785156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55197020.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  77223792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452350.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  158/6933] Loss: -348.4568 [iq: 21.1583,ans: 34.1297,interp: 14.5779,fusion: -418.3228]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  106.08964538574219    \n",
      "module.ans_embedding.weight  dot:  1386221.75    \n",
      "module.lstm.weight_ih_l0  dot:  84572.09375    \n",
      "module.lstm.weight_hh_l0  dot:  4579.39111328125    \n",
      "module.lstm.bias_ih_l0  dot:  6591.5869140625    \n",
      "module.lstm.bias_hh_l0  dot:  6591.5869140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  124537112.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34958.95703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12236321.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12236321.0    \n",
      "module.adapter.frcn_linear.weight  dot:  178979488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  77298.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  993256.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  771.83984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1656858.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  27010540.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  106843.9921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.5923933982849121    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.19925549626350403    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.4842911660671234    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  120359.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  106843.9921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  111.43924713134766    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.25619888305664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  102.87416076660156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.46265255252365e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55969072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80347376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452351.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  159/6933] Loss: -337.7634 [iq: 19.3803,ans: 34.2281,interp: 16.3014,fusion: -407.6732]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125.42941284179688    \n",
      "module.ans_embedding.weight  dot:  1425452.25    \n",
      "module.lstm.weight_ih_l0  dot:  107457.8671875    \n",
      "module.lstm.weight_hh_l0  dot:  5256.779296875    \n",
      "module.lstm.bias_ih_l0  dot:  8971.923828125    \n",
      "module.lstm.bias_hh_l0  dot:  8971.923828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  124824472.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7769.5029296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11829799.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11829799.0    \n",
      "module.adapter.frcn_linear.weight  dot:  323148800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  138337.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1261721.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  823.3729248046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2089598.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  33585824.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  131489.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.7161492109298706    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2249285876750946    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.6127709150314331    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.418065747633591e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  134833.71875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  131489.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  44.09042739868164    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.80074691772461    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  89.45698547363281    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57595984.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  81043336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452351.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  160/6933] Loss: -340.7031 [iq: 23.1825,ans: 34.0500,interp: 12.6028,fusion: -410.5383]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  135.35011291503906    \n",
      "module.ans_embedding.weight  dot:  1337194.125    \n",
      "module.lstm.weight_ih_l0  dot:  94135.0390625    \n",
      "module.lstm.weight_hh_l0  dot:  4671.2724609375    \n",
      "module.lstm.bias_ih_l0  dot:  7334.943359375    \n",
      "module.lstm.bias_hh_l0  dot:  7334.943359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113121616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12186.0732421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11938312.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11938312.0    \n",
      "module.adapter.frcn_linear.weight  dot:  243224592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107443.3671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  510497.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  269.06158447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  523456.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  31205180.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  126177.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.8811452984809875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.30706483125686646    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7244770526885986    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.9960036108132044e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  136274.59375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  126177.4609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  30.60357666015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.623479843139648    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  36.96234893798828    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0746958878371515e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55262480.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84681808.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452352.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  161/6933] Loss: -347.9095 [iq: 20.3980,ans: 38.0522,interp: 10.8563,fusion: -417.2160]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  132.5811767578125    \n",
      "module.ans_embedding.weight  dot:  1431597.25    \n",
      "module.lstm.weight_ih_l0  dot:  114504.359375    \n",
      "module.lstm.weight_hh_l0  dot:  5464.4755859375    \n",
      "module.lstm.bias_ih_l0  dot:  9242.93359375    \n",
      "module.lstm.bias_hh_l0  dot:  9242.93359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  125032616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22126.546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12029675.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12029675.0    \n",
      "module.adapter.frcn_linear.weight  dot:  321910496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133897.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  799448.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  557.1287841796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1374144.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  35364384.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137356.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.453521728515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.45309728384017944    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.4352298974990845    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  142601.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137356.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  198.09930419921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  60.682411193847656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  294.6173095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59002592.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  85591296.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452352.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  162/6933] Loss: -349.9544 [iq: 21.3394,ans: 32.8534,interp: 11.2704,fusion: -415.4177]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  179.86373901367188    \n",
      "module.ans_embedding.weight  dot:  917989.0625    \n",
      "module.lstm.weight_ih_l0  dot:  105955.8671875    \n",
      "module.lstm.weight_hh_l0  dot:  5334.568359375    \n",
      "module.lstm.bias_ih_l0  dot:  8357.181640625    \n",
      "module.lstm.bias_hh_l0  dot:  8357.181640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  133990432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31760.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12799769.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12799769.0    \n",
      "module.adapter.frcn_linear.weight  dot:  224601248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92788.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1685594.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1209.4935302734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2644687.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  29807960.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124738.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.5789451599121094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.4984856843948364    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.2193877696990967    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.996003610813204e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  134001.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124738.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  283.9259033203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  93.84245300292969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  311.8504638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57021372.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88746912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452353.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  163/6933] Loss: -347.3064 [iq: 24.0798,ans: 35.6009,interp: 17.5207,fusion: -424.5078]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  162.2351837158203    \n",
      "module.ans_embedding.weight  dot:  1383842.75    \n",
      "module.lstm.weight_ih_l0  dot:  139239.125    \n",
      "module.lstm.weight_hh_l0  dot:  6612.0986328125    \n",
      "module.lstm.bias_ih_l0  dot:  11440.6953125    \n",
      "module.lstm.bias_hh_l0  dot:  11440.6953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  127454536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16967.0859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11874100.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11874100.0    \n",
      "module.adapter.frcn_linear.weight  dot:  533984544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  216681.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1517945.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  856.26513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2214092.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  47258584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  181249.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.2935681343078613    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7211700081825256    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.026219367980957    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7985612998927536e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  184994.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  181249.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  130.06472778320312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.058395385742188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  256.02154541015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  63336072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87542888.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452353.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  164/6933] Loss: -343.8809 [iq: 20.8465,ans: 33.3073,interp: 12.6065,fusion: -410.6413]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  157.89065551757812    \n",
      "module.ans_embedding.weight  dot:  1173670.75    \n",
      "module.lstm.weight_ih_l0  dot:  112031.9921875    \n",
      "module.lstm.weight_hh_l0  dot:  5276.173828125    \n",
      "module.lstm.bias_ih_l0  dot:  8735.8642578125    \n",
      "module.lstm.bias_hh_l0  dot:  8735.8642578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  127446096.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15625.6923828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11764076.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11764076.0    \n",
      "module.adapter.frcn_linear.weight  dot:  289942624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  134443.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  566278.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  222.97604370117188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  482127.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  30993152.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136980.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.350039482116699    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5879448652267456    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.8293511867523193    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143546.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136980.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  98.21708679199219    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.675506591796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  122.40364074707031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58969608.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84731040.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452353.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  165/6933] Loss: -361.1839 [iq: 20.8899,ans: 33.0447,interp: 11.0874,fusion: -426.2059]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  115.59715270996094    \n",
      "module.ans_embedding.weight  dot:  1369844.625    \n",
      "module.lstm.weight_ih_l0  dot:  85415.40625    \n",
      "module.lstm.weight_hh_l0  dot:  4180.26025390625    \n",
      "module.lstm.bias_ih_l0  dot:  6703.40673828125    \n",
      "module.lstm.bias_hh_l0  dot:  6703.40673828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  119330624.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12755.3759765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11298328.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11298328.0    \n",
      "module.adapter.frcn_linear.weight  dot:  208502688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  81300.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  716686.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  500.117431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1113210.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  26447956.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  101677.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.391633152961731    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.40495020151138306    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.3638803958892822    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0746958878371515e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  107495.6171875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  101677.6484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  72.98926544189453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.065181732177734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  65.62834167480469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56508128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82969640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452354.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  166/6933] Loss: -364.9878 [iq: 22.0084,ans: 33.1463,interp: 16.1033,fusion: -436.2457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  112.11105346679688    \n",
      "module.ans_embedding.weight  dot:  1913752.5    \n",
      "module.lstm.weight_ih_l0  dot:  91733.359375    \n",
      "module.lstm.weight_hh_l0  dot:  4308.4892578125    \n",
      "module.lstm.bias_ih_l0  dot:  7392.19091796875    \n",
      "module.lstm.bias_hh_l0  dot:  7392.19091796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  131661728.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26746.388671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11548584.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11548584.0    \n",
      "module.adapter.frcn_linear.weight  dot:  330953472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  128197.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1312152.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  977.1896362304688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2413858.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  32069656.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  130047.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.0778536796569824    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3412972092628479    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0613688230514526    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0725309529391325e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  132329.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  130047.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  221.22299194335938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  70.26210021972656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  359.6412353515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59568404.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80938144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452354.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  167/6933] Loss: -345.0194 [iq: 20.5067,ans: 27.7645,interp: 14.1534,fusion: -407.4440]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.35997009277344    \n",
      "module.ans_embedding.weight  dot:  1116174.125    \n",
      "module.lstm.weight_ih_l0  dot:  83583.9765625    \n",
      "module.lstm.weight_hh_l0  dot:  3878.110595703125    \n",
      "module.lstm.bias_ih_l0  dot:  6626.3740234375    \n",
      "module.lstm.bias_hh_l0  dot:  6626.3740234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  135120816.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13016.552734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11737652.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11737652.0    \n",
      "module.adapter.frcn_linear.weight  dot:  258499488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101273.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1589090.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1067.271728515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2495337.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26571876.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  112050.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.124106764793396    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3540797233581543    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.9390789866447449    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  114253.4140625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  112050.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  57.835235595703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.923797607421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  57.11468505859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59605068.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  85224816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452355.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  168/6933] Loss: -350.4663 [iq: 25.4184,ans: 31.7711,interp: 19.5256,fusion: -427.1815]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  120.29449462890625    \n",
      "module.ans_embedding.weight  dot:  1261656.25    \n",
      "module.lstm.weight_ih_l0  dot:  83925.984375    \n",
      "module.lstm.weight_hh_l0  dot:  4087.46728515625    \n",
      "module.lstm.bias_ih_l0  dot:  6441.7060546875    \n",
      "module.lstm.bias_hh_l0  dot:  6441.7060546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  126684944.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20710.125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11893388.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11893388.0    \n",
      "module.adapter.frcn_linear.weight  dot:  223232352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99668.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  474556.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.1196594238281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  606666.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24091464.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  112261.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.9052131175994873    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2834988236427307    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7236777544021606    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.01581023779363e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  119550.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  112261.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  156.39813232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.70093536376953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  136.60113525390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57718508.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87597696.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452355.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  169/6933] Loss: -368.0166 [iq: 20.5187,ans: 31.9481,interp: 15.1821,fusion: -435.6654]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  155.2416229248047    \n",
      "module.ans_embedding.weight  dot:  1294167.25    \n",
      "module.lstm.weight_ih_l0  dot:  86857.9296875    \n",
      "module.lstm.weight_hh_l0  dot:  3685.53125    \n",
      "module.lstm.bias_ih_l0  dot:  6615.4970703125    \n",
      "module.lstm.bias_hh_l0  dot:  6615.4970703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  114226840.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24855.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11402522.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11402522.0    \n",
      "module.adapter.frcn_linear.weight  dot:  226336864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  109784.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  292651.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  153.1774444580078    \n",
      "module.attflat_img.mlp.linear.weight  dot:  296209.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  24836708.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122122.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.9636603593826294    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3232927918434143    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7989133596420288    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  122632.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122122.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  52.24089050292969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.805135726928711    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  99.68437957763672    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3888446776254568e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56713028.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86925352.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452356.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  170/6933] Loss: -362.1629 [iq: 19.2647,ans: 33.8042,interp: 18.7071,fusion: -433.9389]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  121.88924407958984    \n",
      "module.ans_embedding.weight  dot:  1291620.875    \n",
      "module.lstm.weight_ih_l0  dot:  86097.0    \n",
      "module.lstm.weight_hh_l0  dot:  3903.96142578125    \n",
      "module.lstm.bias_ih_l0  dot:  6839.859375    \n",
      "module.lstm.bias_hh_l0  dot:  6839.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  107029968.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3550.69189453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11784416.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11784416.0    \n",
      "module.adapter.frcn_linear.weight  dot:  226166208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  100546.7109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1273454.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  660.828369140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1423296.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23717804.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  116403.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.1909586191177368    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.45726409554481506    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0784879922866821    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7985612998927536e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  116440.796875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  116403.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10.868223190307617    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.146125078201294    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  14.342982292175293    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  54150008.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  92758184.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452356.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  171/6933] Loss: -369.0922 [iq: 17.4221,ans: 39.8415,interp: 10.3253,fusion: -436.6812]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125.68570709228516    \n",
      "module.ans_embedding.weight  dot:  1601253.0    \n",
      "module.lstm.weight_ih_l0  dot:  75522.1484375    \n",
      "module.lstm.weight_hh_l0  dot:  3638.89501953125    \n",
      "module.lstm.bias_ih_l0  dot:  5822.7177734375    \n",
      "module.lstm.bias_hh_l0  dot:  5822.7177734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  139121824.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8277.73046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11215368.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11215368.0    \n",
      "module.adapter.frcn_linear.weight  dot:  186827136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80778.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  321772.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  150.65850830078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  291919.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20541260.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95430.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.3691799640655518    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.42887306213378906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.0967446565628052    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6042722705833512e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  100589.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95430.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  112.18887329101562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  37.846107482910156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  103.71421813964844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61676112.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80275824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452357.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  172/6933] Loss: -372.8026 [iq: 17.1626,ans: 25.3086,interp: 16.4536,fusion: -431.7274]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  120.68145751953125    \n",
      "module.ans_embedding.weight  dot:  1225718.5    \n",
      "module.lstm.weight_ih_l0  dot:  80372.3359375    \n",
      "module.lstm.weight_hh_l0  dot:  3738.753173828125    \n",
      "module.lstm.bias_ih_l0  dot:  6427.76953125    \n",
      "module.lstm.bias_hh_l0  dot:  6427.76953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  109897496.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  61377.48046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12226386.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12226386.0    \n",
      "module.adapter.frcn_linear.weight  dot:  227766496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101738.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1573730.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1208.6595458984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2629574.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  21799636.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107362.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.9571397304534912    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.362873375415802    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.8507706522941589    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.3520742565306136e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  106532.921875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107362.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  332.9750671386719    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  77.28092956542969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  646.8832397460938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.46265255252365e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  54158448.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87666808.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452357.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  173/6933] Loss: -368.8942 [iq: 18.3833,ans: 44.2469,interp: 11.9289,fusion: -443.4533]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  126.26409149169922    \n",
      "module.ans_embedding.weight  dot:  1387995.625    \n",
      "module.lstm.weight_ih_l0  dot:  78703.984375    \n",
      "module.lstm.weight_hh_l0  dot:  3687.375732421875    \n",
      "module.lstm.bias_ih_l0  dot:  6118.6708984375    \n",
      "module.lstm.bias_hh_l0  dot:  6118.6708984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  131712200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17970.7578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11906235.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11906235.0    \n",
      "module.adapter.frcn_linear.weight  dot:  209257824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92965.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  252761.296875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  120.14559173583984    \n",
      "module.attflat_img.mlp.linear.weight  dot:  262741.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22022962.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  102203.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.9138056039810181    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3330409526824951    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.8859093189239502    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.775380174100064e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  105283.0234375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  102203.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  236.09898376464844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.67503356933594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  280.96331787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61213548.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88589120.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452358.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  174/6933] Loss: -377.9442 [iq: 21.0045,ans: 29.2233,interp: 14.6842,fusion: -442.8561]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  146.94097900390625    \n",
      "module.ans_embedding.weight  dot:  1056069.0    \n",
      "module.lstm.weight_ih_l0  dot:  99323.6015625    \n",
      "module.lstm.weight_hh_l0  dot:  4915.7158203125    \n",
      "module.lstm.bias_ih_l0  dot:  7928.28662109375    \n",
      "module.lstm.bias_hh_l0  dot:  7928.28662109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  138249568.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8669.3134765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12186136.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12186136.0    \n",
      "module.adapter.frcn_linear.weight  dot:  271139200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  129619.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  453239.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  234.084716796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  454308.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24593116.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124211.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.00541353225708    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2579542398452759    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.9102045893669128    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  127047.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124211.7421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  49.39588928222656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.758774757385254    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  77.29936218261719    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60463588.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94539624.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452358.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  175/6933] Loss: -369.0586 [iq: 24.8741,ans: 33.7272,interp: 18.3593,fusion: -446.0191]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  129.35272216796875    \n",
      "module.ans_embedding.weight  dot:  1452442.875    \n",
      "module.lstm.weight_ih_l0  dot:  87768.265625    \n",
      "module.lstm.weight_hh_l0  dot:  3922.010009765625    \n",
      "module.lstm.bias_ih_l0  dot:  7171.736328125    \n",
      "module.lstm.bias_hh_l0  dot:  7171.736328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  116239920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7574.69580078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12748562.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12748562.0    \n",
      "module.adapter.frcn_linear.weight  dot:  268008880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127503.7109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  565856.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  317.2016296386719    \n",
      "module.attflat_img.mlp.linear.weight  dot:  534936.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23275828.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  126092.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.58821702003479    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.174525648355484    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.4538325071334839    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121084.328125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  126092.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  119.53512573242188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.476520538330078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  127.24702453613281    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58536352.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  98851088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452359.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  176/6933] Loss: -370.8559 [iq: 15.6802,ans: 33.7916,interp: 22.2129,fusion: -442.5406]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  141.2958984375    \n",
      "module.ans_embedding.weight  dot:  1408843.125    \n",
      "module.lstm.weight_ih_l0  dot:  90605.921875    \n",
      "module.lstm.weight_hh_l0  dot:  4153.99853515625    \n",
      "module.lstm.bias_ih_l0  dot:  7198.814453125    \n",
      "module.lstm.bias_hh_l0  dot:  7198.814453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  123152864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17748.814453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12507358.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12507358.0    \n",
      "module.adapter.frcn_linear.weight  dot:  260241392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  110968.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1801301.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1207.7017822265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2583431.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23851748.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  120843.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.8628255128860474    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2690396308898926    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7116653919219971    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.3520742565306136e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  121121.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  120843.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  60.53520965576172    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.24279022216797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  58.35743713378906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59864700.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97394464.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452359.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  177/6933] Loss: -378.2975 [iq: 20.2883,ans: 32.6269,interp: 14.8619,fusion: -446.0746]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  154.658203125    \n",
      "module.ans_embedding.weight  dot:  1429497.75    \n",
      "module.lstm.weight_ih_l0  dot:  86213.40625    \n",
      "module.lstm.weight_hh_l0  dot:  3976.69091796875    \n",
      "module.lstm.bias_ih_l0  dot:  6884.08984375    \n",
      "module.lstm.bias_hh_l0  dot:  6884.08984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  131077576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31132.884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12231658.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12231658.0    \n",
      "module.adapter.frcn_linear.weight  dot:  237685104.0    \n",
      "module.adapter.frcn_linear.bias  dot:  108434.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  626871.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  425.81097412109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  814312.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20713526.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107706.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.5487931966781616    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6106141805648804    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.5811446905136108    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  107123.921875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107706.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1760.5926513671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  494.77972412109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2358.69677734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.83853071655804e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61357316.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  90210560.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452360.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  178/6933] Loss: -390.9215 [iq: 17.5296,ans: 28.0279,interp: 12.5404,fusion: -449.0194]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  124.1138916015625    \n",
      "module.ans_embedding.weight  dot:  1251886.25    \n",
      "module.lstm.weight_ih_l0  dot:  70852.328125    \n",
      "module.lstm.weight_hh_l0  dot:  3384.43994140625    \n",
      "module.lstm.bias_ih_l0  dot:  5649.53515625    \n",
      "module.lstm.bias_hh_l0  dot:  5649.53515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  101217936.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23917.626953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12328086.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12328086.0    \n",
      "module.adapter.frcn_linear.weight  dot:  184403008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  77096.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  536179.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  297.14898681640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  543635.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18963096.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  89432.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.7964892387390137    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.28663399815559387    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7922518849372864    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  90640.34375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  89432.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  103.26996612548828    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.644004821777344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  112.63088989257812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.412026217120001e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  54160428.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97730512.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452360.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  179/6933] Loss: -389.3849 [iq: 18.2710,ans: 36.4875,interp: 10.6847,fusion: -454.8281]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  219.42193603515625    \n",
      "module.ans_embedding.weight  dot:  1323149.25    \n",
      "module.lstm.weight_ih_l0  dot:  152509.5625    \n",
      "module.lstm.weight_hh_l0  dot:  6757.1337890625    \n",
      "module.lstm.bias_ih_l0  dot:  11612.33984375    \n",
      "module.lstm.bias_hh_l0  dot:  11612.33984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  118822368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9300.9951171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11807492.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11807492.0    \n",
      "module.adapter.frcn_linear.weight  dot:  488337024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  215236.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3459368.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2857.817626953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5661469.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  33377778.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  182523.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.9054124355316162    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6546071171760559    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.690711259841919    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  186138.234375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  182523.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  26.993228912353516    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.6807098388671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  30.414459228515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59362296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99889896.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452361.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  180/6933] Loss: -390.2498 [iq: 17.5358,ans: 32.1322,interp: 13.4563,fusion: -453.3741]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  194.01609802246094    \n",
      "module.ans_embedding.weight  dot:  981372.1875    \n",
      "module.lstm.weight_ih_l0  dot:  82718.1953125    \n",
      "module.lstm.weight_hh_l0  dot:  3696.43408203125    \n",
      "module.lstm.bias_ih_l0  dot:  5999.02880859375    \n",
      "module.lstm.bias_hh_l0  dot:  5999.02880859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  114709040.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20319.74609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11760870.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11760870.0    \n",
      "module.adapter.frcn_linear.weight  dot:  209473552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  93404.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  759022.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  665.39404296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1187177.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19646894.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99465.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.1442642211914062    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9983701109886169    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.205698251724243    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.44064793217558e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  105354.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99465.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  201.41949462890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.06122589111328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  248.93161010742188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58026088.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  98455000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452361.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  181/6933] Loss: -395.9050 [iq: 22.2427,ans: 33.3143,interp: 14.3113,fusion: -465.7732]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  170.3139190673828    \n",
      "module.ans_embedding.weight  dot:  1546162.0    \n",
      "module.lstm.weight_ih_l0  dot:  82435.109375    \n",
      "module.lstm.weight_hh_l0  dot:  3731.63525390625    \n",
      "module.lstm.bias_ih_l0  dot:  6254.0380859375    \n",
      "module.lstm.bias_hh_l0  dot:  6254.0380859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  136401200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28190.29296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11460292.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11460292.0    \n",
      "module.adapter.frcn_linear.weight  dot:  207082128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  95661.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  383568.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  223.9436798095703    \n",
      "module.attflat_img.mlp.linear.weight  dot:  453314.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  19750202.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107066.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.0457334518432617    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7314289808273315    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.0640902519226074    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.381384558082573e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  109841.484375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107066.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  222.95179748535156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.12004089355469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  289.9718017578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  63003808.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86901952.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452362.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  182/6933] Loss: -395.4079 [iq: 19.4659,ans: 25.2098,interp: 10.0758,fusion: -450.1594]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  168.47793579101562    \n",
      "module.ans_embedding.weight  dot:  1421832.75    \n",
      "module.lstm.weight_ih_l0  dot:  75943.640625    \n",
      "module.lstm.weight_hh_l0  dot:  3454.490966796875    \n",
      "module.lstm.bias_ih_l0  dot:  5632.60107421875    \n",
      "module.lstm.bias_hh_l0  dot:  5632.60107421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  128054016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7057.544921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11026659.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11026659.0    \n",
      "module.adapter.frcn_linear.weight  dot:  200590400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  85556.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  901049.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  585.4601440429688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1156669.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  18257410.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  90688.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.7784149646759033    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9058163166046143    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.966174602508545    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.004086117172847e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  94311.890625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  90688.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  53.691246032714844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.066852569580078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  74.32731628417969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60854212.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88673288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452362.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  183/6933] Loss: -398.0518 [iq: 18.0809,ans: 25.9378,interp: 13.4799,fusion: -455.5504]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  127.07054138183594    \n",
      "module.ans_embedding.weight  dot:  1231706.375    \n",
      "module.lstm.weight_ih_l0  dot:  67437.0859375    \n",
      "module.lstm.weight_hh_l0  dot:  3299.85546875    \n",
      "module.lstm.bias_ih_l0  dot:  5330.3583984375    \n",
      "module.lstm.bias_hh_l0  dot:  5330.3583984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  124474072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7079.6640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11028203.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11028203.0    \n",
      "module.adapter.frcn_linear.weight  dot:  219197296.0    \n",
      "module.adapter.frcn_linear.bias  dot:  89409.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1194545.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1187.52880859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2511222.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19354248.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95379.6015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.4202978610992432    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.444631963968277    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.1725865602493286    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  96253.1328125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95379.6015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  24.454742431640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.447264671325684    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  26.35010528564453    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59143156.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  92717632.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452362.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  184/6933] Loss: -403.6184 [iq: 17.3052,ans: 27.4685,interp: 12.0473,fusion: -460.4394]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  143.83169555664062    \n",
      "module.ans_embedding.weight  dot:  1629303.125    \n",
      "module.lstm.weight_ih_l0  dot:  81222.265625    \n",
      "module.lstm.weight_hh_l0  dot:  3848.43408203125    \n",
      "module.lstm.bias_ih_l0  dot:  6733.6201171875    \n",
      "module.lstm.bias_hh_l0  dot:  6733.6201171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  129501688.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31789.115234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11352658.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11352658.0    \n",
      "module.adapter.frcn_linear.weight  dot:  230964480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  98840.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  587724.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  644.8460693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1260999.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20819194.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  105243.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.8991599082946777    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.2950873076915741    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.7959021925926208    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  102200.0546875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  105243.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  269.42071533203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  77.41679382324219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  443.0589294433594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60579860.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87164560.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452363.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  185/6933] Loss: -398.4106 [iq: 16.7651,ans: 25.8102,interp: 11.1553,fusion: -452.1412]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  190.04177856445312    \n",
      "module.ans_embedding.weight  dot:  1044520.3125    \n",
      "module.lstm.weight_ih_l0  dot:  101434.6015625    \n",
      "module.lstm.weight_hh_l0  dot:  5131.23095703125    \n",
      "module.lstm.bias_ih_l0  dot:  8081.33056640625    \n",
      "module.lstm.bias_hh_l0  dot:  8081.33056640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  122090504.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37223.3671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12000644.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12000644.0    \n",
      "module.adapter.frcn_linear.weight  dot:  298920864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  115729.421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4611256.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4079.942138671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8429712.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24118212.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124193.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.293076992034912    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.875603437423706    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.123098850250244    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.551115123125783e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  126589.0078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124193.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.381591796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  97.32489013671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  293.7158203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.206324095117452e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59676472.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94741472.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452364.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  186/6933] Loss: -385.0474 [iq: 20.9117,ans: 29.8237,interp: 20.5474,fusion: -456.3302]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  174.49795532226562    \n",
      "module.ans_embedding.weight  dot:  1206430.375    \n",
      "module.lstm.weight_ih_l0  dot:  115557.3359375    \n",
      "module.lstm.weight_hh_l0  dot:  5629.6025390625    \n",
      "module.lstm.bias_ih_l0  dot:  9130.9453125    \n",
      "module.lstm.bias_hh_l0  dot:  9130.9453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  132793224.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12935.775390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11375572.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11375572.0    \n",
      "module.adapter.frcn_linear.weight  dot:  345492384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  162012.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1005572.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  711.4896240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1255058.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  25734736.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  148147.703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.1773277521133423    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.28364333510398865    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.9064874649047852    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  151033.765625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  148147.703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  22.523330688476562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.991728782653809    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  29.19154930114746    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  62623832.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94528096.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452364.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  187/6933] Loss: -400.9186 [iq: 22.4243,ans: 25.7293,interp: 13.1892,fusion: -462.2614]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  116.15219116210938    \n",
      "module.ans_embedding.weight  dot:  1514512.25    \n",
      "module.lstm.weight_ih_l0  dot:  65339.0234375    \n",
      "module.lstm.weight_hh_l0  dot:  2755.9521484375    \n",
      "module.lstm.bias_ih_l0  dot:  5081.26416015625    \n",
      "module.lstm.bias_hh_l0  dot:  5081.26416015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  135699184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14922.6748046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11323484.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11323484.0    \n",
      "module.adapter.frcn_linear.weight  dot:  192684736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  85143.7265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  725058.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  646.46533203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1305780.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18449064.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  97070.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  0.867656946182251    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3393784165382385    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.8997877240180969    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  92405.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  97070.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  149.00015258789062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.494632720947266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  382.73944091796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  63229940.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  91354928.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452365.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  188/6933] Loss: -404.6662 [iq: 16.5754,ans: 24.6211,interp: 11.7935,fusion: -457.6563]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  137.10324096679688    \n",
      "module.ans_embedding.weight  dot:  1439615.5    \n",
      "module.lstm.weight_ih_l0  dot:  78319.5625    \n",
      "module.lstm.weight_hh_l0  dot:  3571.23193359375    \n",
      "module.lstm.bias_ih_l0  dot:  6267.412109375    \n",
      "module.lstm.bias_hh_l0  dot:  6267.412109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  124727200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  57209.36328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11807324.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11807324.0    \n",
      "module.adapter.frcn_linear.weight  dot:  241105680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102623.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  315875.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  219.32505798339844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  489349.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20817026.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  103627.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.3291220664978027    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.4445542097091675    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.206613302230835    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  101255.6953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  103627.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  329.6739196777344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.59002685546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  435.2687683105469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61648888.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94625504.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452365.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  189/6933] Loss: -396.0587 [iq: 19.9022,ans: 27.9240,interp: 17.5277,fusion: -461.4126]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  175.638671875    \n",
      "module.ans_embedding.weight  dot:  1405351.875    \n",
      "module.lstm.weight_ih_l0  dot:  78933.2109375    \n",
      "module.lstm.weight_hh_l0  dot:  3480.458740234375    \n",
      "module.lstm.bias_ih_l0  dot:  5917.9833984375    \n",
      "module.lstm.bias_hh_l0  dot:  5917.9833984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  122626280.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17665.98046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10821410.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10821410.0    \n",
      "module.adapter.frcn_linear.weight  dot:  219898960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  95755.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1871163.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1422.162109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3016926.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  19116832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  105776.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.161526679992676    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.613417387008667    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.8936642408370972    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.3520742565306136e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  104823.71875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  105776.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  212.027099609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.5430679321289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  216.29150390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58734760.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88969696.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452365.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  190/6933] Loss: -406.2632 [iq: 19.3165,ans: 24.8912,interp: 15.5486,fusion: -466.0194]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  252.49249267578125    \n",
      "module.ans_embedding.weight  dot:  1122082.5    \n",
      "module.lstm.weight_ih_l0  dot:  105267.375    \n",
      "module.lstm.weight_hh_l0  dot:  4675.748046875    \n",
      "module.lstm.bias_ih_l0  dot:  7981.779296875    \n",
      "module.lstm.bias_hh_l0  dot:  7981.779296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  125195288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28957.4375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11133331.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11133331.0    \n",
      "module.adapter.frcn_linear.weight  dot:  291037952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133912.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  899959.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  504.93304443359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1028090.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23932068.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137867.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.3716044425964355    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.185373067855835    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.9782886505126953    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  139610.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137867.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  223.009765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  60.981964111328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  413.82061767578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  62989800.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93414240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452366.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  191/6933] Loss: -412.5171 [iq: 17.1968,ans: 26.3502,interp: 13.4176,fusion: -469.4818]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  259.52252197265625    \n",
      "module.ans_embedding.weight  dot:  1620742.0    \n",
      "module.lstm.weight_ih_l0  dot:  119744.3046875    \n",
      "module.lstm.weight_hh_l0  dot:  5207.69580078125    \n",
      "module.lstm.bias_ih_l0  dot:  9583.9150390625    \n",
      "module.lstm.bias_hh_l0  dot:  9583.9150390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  123003920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10460.20703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10604898.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10604898.0    \n",
      "module.adapter.frcn_linear.weight  dot:  347432864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159088.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  721600.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  552.8970336914062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1047079.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  25645062.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  149366.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.164951801300049    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.6648553609848022    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.9240812063217163    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143003.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  149366.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  117.3001480102539    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.31945037841797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  147.7784423828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60439936.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  91187552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452367.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  192/6933] Loss: -402.1651 [iq: 18.5978,ans: 27.4756,interp: 10.0100,fusion: -458.2485]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  192.4429473876953    \n",
      "module.ans_embedding.weight  dot:  1418856.75    \n",
      "module.lstm.weight_ih_l0  dot:  99726.640625    \n",
      "module.lstm.weight_hh_l0  dot:  4697.576171875    \n",
      "module.lstm.bias_ih_l0  dot:  7798.4111328125    \n",
      "module.lstm.bias_hh_l0  dot:  7798.4111328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  120604312.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33490.0078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11882680.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11882680.0    \n",
      "module.adapter.frcn_linear.weight  dot:  321567008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  146188.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  608099.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  384.5635070800781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  622604.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  24276016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133288.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.650020956993103    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5408163070678711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.509519338607788    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  133573.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133288.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  206.113037109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.61531448364258    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  341.88043212890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61662832.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99401800.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452367.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  193/6933] Loss: -401.0662 [iq: 21.7858,ans: 28.5566,interp: 14.1118,fusion: -465.5202]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  218.27430725097656    \n",
      "module.ans_embedding.weight  dot:  1045315.0    \n",
      "module.lstm.weight_ih_l0  dot:  90546.609375    \n",
      "module.lstm.weight_hh_l0  dot:  4169.57470703125    \n",
      "module.lstm.bias_ih_l0  dot:  7034.80859375    \n",
      "module.lstm.bias_hh_l0  dot:  7034.80859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  97474832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11401.724609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10565590.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10565590.0    \n",
      "module.adapter.frcn_linear.weight  dot:  247743072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  116581.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  396692.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  248.5194549560547    \n",
      "module.attflat_img.mlp.linear.weight  dot:  412262.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19554068.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  108556.4296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.7966060638427734    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5768105983734131    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.582901120185852    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  107679.1328125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  108556.4296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  853.045654296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  302.1730651855469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  804.9056396484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  52527000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97704072.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452368.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  194/6933] Loss: -408.6907 [iq: 18.9346,ans: 34.8181,interp: 20.7172,fusion: -483.1606]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  204.40896606445312    \n",
      "module.ans_embedding.weight  dot:  1402655.875    \n",
      "module.lstm.weight_ih_l0  dot:  106431.40625    \n",
      "module.lstm.weight_hh_l0  dot:  4728.0615234375    \n",
      "module.lstm.bias_ih_l0  dot:  8405.3994140625    \n",
      "module.lstm.bias_hh_l0  dot:  8405.3994140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  122813536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28047.18359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10894050.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10894050.0    \n",
      "module.adapter.frcn_linear.weight  dot:  262320928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  115263.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1303669.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1273.67041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2340618.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23019960.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133523.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.8854970932006836    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9396662712097168    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.673503875732422    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.9960036108132044e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  130093.984375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133523.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  493.7216796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  127.37045288085938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  431.59698486328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  60564568.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  91274384.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452368.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  195/6933] Loss: -406.5345 [iq: 14.6573,ans: 23.3832,interp: 10.2981,fusion: -454.8731]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  211.50881958007812    \n",
      "module.ans_embedding.weight  dot:  1353014.125    \n",
      "module.lstm.weight_ih_l0  dot:  113324.1875    \n",
      "module.lstm.weight_hh_l0  dot:  5411.0986328125    \n",
      "module.lstm.bias_ih_l0  dot:  8842.787109375    \n",
      "module.lstm.bias_hh_l0  dot:  8842.787109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  108821616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14242.2705078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10290007.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10290007.0    \n",
      "module.adapter.frcn_linear.weight  dot:  363867520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  165443.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  469453.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  324.1254577636719    \n",
      "module.attflat_img.mlp.linear.weight  dot:  472821.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  26182984.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  148139.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.323113441467285    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.8504694700241089    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.210108995437622    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  147462.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  148139.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  143.96633911132812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.108543395996094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  214.62535095214844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58251280.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93667232.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452369.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  196/6933] Loss: -413.6999 [iq: 17.1176,ans: 29.1747,interp: 11.9503,fusion: -471.9424]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  264.1324462890625    \n",
      "module.ans_embedding.weight  dot:  989357.875    \n",
      "module.lstm.weight_ih_l0  dot:  106665.390625    \n",
      "module.lstm.weight_hh_l0  dot:  5104.470703125    \n",
      "module.lstm.bias_ih_l0  dot:  7721.9404296875    \n",
      "module.lstm.bias_hh_l0  dot:  7721.9404296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  122518776.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15206.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10505510.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10505510.0    \n",
      "module.adapter.frcn_linear.weight  dot:  234622224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101465.6953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  853662.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  676.2778930664062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1361278.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20799476.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  114530.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.504767656326294    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.213057279586792    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.1060473918914795    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  123173.5234375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  114530.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  142.1028289794922    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  36.36486053466797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  182.72166442871094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61590556.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  96993592.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452369.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  197/6933] Loss: -414.7482 [iq: 21.9418,ans: 24.6685,interp: 16.0668,fusion: -477.4254]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  176.33511352539062    \n",
      "module.ans_embedding.weight  dot:  1587285.875    \n",
      "module.lstm.weight_ih_l0  dot:  82515.453125    \n",
      "module.lstm.weight_hh_l0  dot:  3468.595703125    \n",
      "module.lstm.bias_ih_l0  dot:  6587.3603515625    \n",
      "module.lstm.bias_hh_l0  dot:  6587.3603515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  127966024.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19202.716796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10451574.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10451574.0    \n",
      "module.adapter.frcn_linear.weight  dot:  213629280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  94429.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  268008.40625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  212.2350311279297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  438321.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20611688.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  110232.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.2285988330841064    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.5941321849822998    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.9975608587265015    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  101805.90625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  110232.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  148.8494873046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  37.44634246826172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  266.39593505859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.972111694063642e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61355944.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88020640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452370.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  198/6933] Loss: -423.0445 [iq: 16.9143,ans: 21.6620,interp: 8.5087,fusion: -470.1295]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  279.92510986328125    \n",
      "module.ans_embedding.weight  dot:  787177.25    \n",
      "module.lstm.weight_ih_l0  dot:  166204.765625    \n",
      "module.lstm.weight_hh_l0  dot:  7393.92431640625    \n",
      "module.lstm.bias_ih_l0  dot:  12499.4140625    \n",
      "module.lstm.bias_hh_l0  dot:  12499.4140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  94025552.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24975.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10358256.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10358256.0    \n",
      "module.adapter.frcn_linear.weight  dot:  504043680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  226631.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1492743.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1171.1220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2474317.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  33070898.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  199036.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.655143737792969    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.6705498695373535    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.301510810852051    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.206324095117452e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  201886.296875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  199036.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  903.2052001953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  172.18875122070312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1610.828857421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.46265255252365e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55125728.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  109620896.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452370.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  199/6933] Loss: -404.5577 [iq: 22.4601,ans: 34.2264,interp: 19.8977,fusion: -481.1419]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  317.8357849121094    \n",
      "module.ans_embedding.weight  dot:  1201477.625    \n",
      "module.lstm.weight_ih_l0  dot:  114031.7578125    \n",
      "module.lstm.weight_hh_l0  dot:  5309.82666015625    \n",
      "module.lstm.bias_ih_l0  dot:  8250.232421875    \n",
      "module.lstm.bias_hh_l0  dot:  8250.232421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  107480584.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24682.3984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9814433.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9814433.0    \n",
      "module.adapter.frcn_linear.weight  dot:  321359872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  153765.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1146785.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  585.5264892578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  888878.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23355900.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  148852.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.410800933837891    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.236619234085083    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.5283639430999756    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  152397.390625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  148852.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1036.3870849609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  357.60107421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1027.1748046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0746958878371515e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57934736.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93174112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452371.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  200/6933] Loss: -422.0907 [iq: 17.1948,ans: 27.3448,interp: 14.5802,fusion: -481.2105]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  353.007568359375    \n",
      "module.ans_embedding.weight  dot:  819356.6875    \n",
      "module.lstm.weight_ih_l0  dot:  141491.375    \n",
      "module.lstm.weight_hh_l0  dot:  6752.587890625    \n",
      "module.lstm.bias_ih_l0  dot:  10767.646484375    \n",
      "module.lstm.bias_hh_l0  dot:  10767.646484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  95141632.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28681.8984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11242533.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11242533.0    \n",
      "module.adapter.frcn_linear.weight  dot:  298438208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  123345.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2636607.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2254.284423828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4518330.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23094796.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  138167.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.7203474044799805    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.3735933303833008    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.105717658996582    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.194245199571014e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  144272.234375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  138167.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  240.72821044921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  81.86343383789062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  304.70892333984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.6556181132473284e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55910192.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  108542912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452371.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  201/6933] Loss: -423.3637 [iq: 18.3934,ans: 33.4002,interp: 11.6182,fusion: -486.7755]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  229.29727172851562    \n",
      "module.ans_embedding.weight  dot:  1811165.25    \n",
      "module.lstm.weight_ih_l0  dot:  92884.890625    \n",
      "module.lstm.weight_hh_l0  dot:  4644.25146484375    \n",
      "module.lstm.bias_ih_l0  dot:  7354.4521484375    \n",
      "module.lstm.bias_hh_l0  dot:  7354.4521484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  121355072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40287.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9865028.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9865028.0    \n",
      "module.adapter.frcn_linear.weight  dot:  242702672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107168.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1329542.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1197.789794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2133042.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20266684.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  117545.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.245859146118164    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.1310863494873047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.364185333251953    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  117504.078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  117545.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  470.5162658691406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  110.06397247314453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  722.7173461914062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  61893080.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86318816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452371.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  202/6933] Loss: -430.1010 [iq: 14.3464,ans: 21.6454,interp: 10.5309,fusion: -476.6237]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  295.4605712890625    \n",
      "module.ans_embedding.weight  dot:  1448517.875    \n",
      "module.lstm.weight_ih_l0  dot:  114626.15625    \n",
      "module.lstm.weight_hh_l0  dot:  5256.29736328125    \n",
      "module.lstm.bias_ih_l0  dot:  8727.69921875    \n",
      "module.lstm.bias_hh_l0  dot:  8727.69921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  101456288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23592.17578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9342753.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9342753.0    \n",
      "module.adapter.frcn_linear.weight  dot:  321293632.0    \n",
      "module.adapter.frcn_linear.bias  dot:  154530.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  680909.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  381.68658447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  578227.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22649720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  146361.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.158048152923584    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9138985872268677    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.8035964965820312    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.214229214014267e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  143325.40625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  146361.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  296.13262939453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  52.602325439453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  399.3755798339844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55985440.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88795152.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452372.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  203/6933] Loss: -433.1963 [iq: 16.5222,ans: 25.1929,interp: 10.2931,fusion: -485.2045]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  333.66473388671875    \n",
      "module.ans_embedding.weight  dot:  798185.25    \n",
      "module.lstm.weight_ih_l0  dot:  174569.453125    \n",
      "module.lstm.weight_hh_l0  dot:  7934.9697265625    \n",
      "module.lstm.bias_ih_l0  dot:  13808.7421875    \n",
      "module.lstm.bias_hh_l0  dot:  13808.7421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  114346640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  63538.94921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  11859837.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  11859837.0    \n",
      "module.adapter.frcn_linear.weight  dot:  574037248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  247222.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5783343.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5813.435546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10754198.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  36160120.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241238.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.033641338348389    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.48651123046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.494610071182251    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  234477.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241238.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  632.442138671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  134.9202880859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  889.64599609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.672262990534364e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  61436496.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  109730216.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452373.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  204/6933] Loss: -405.1739 [iq: 22.5914,ans: 27.9175,interp: 18.6250,fusion: -474.3078]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  242.41653442382812    \n",
      "module.ans_embedding.weight  dot:  1299165.5    \n",
      "module.lstm.weight_ih_l0  dot:  116604.921875    \n",
      "module.lstm.weight_hh_l0  dot:  5184.32666015625    \n",
      "module.lstm.bias_ih_l0  dot:  9241.59765625    \n",
      "module.lstm.bias_hh_l0  dot:  9241.59765625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  115880208.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14143.28125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10681042.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10681042.0    \n",
      "module.adapter.frcn_linear.weight  dot:  324117344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  150612.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1930282.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2094.605712890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3727470.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22375086.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  147507.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.119807243347168    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.3388771712779999    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  0.9212133884429932    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0880185641326534e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  142008.734375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  147507.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  156.43853759765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.01324462890625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  159.02288818359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60380312.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  104704112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452373.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  205/6933] Loss: -427.6335 [iq: 16.3718,ans: 27.1800,interp: 13.2394,fusion: -484.4247]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  266.54766845703125    \n",
      "module.ans_embedding.weight  dot:  1320303.125    \n",
      "module.lstm.weight_ih_l0  dot:  107840.5625    \n",
      "module.lstm.weight_hh_l0  dot:  4981.6494140625    \n",
      "module.lstm.bias_ih_l0  dot:  7971.79345703125    \n",
      "module.lstm.bias_hh_l0  dot:  7971.79345703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  112658832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18481.703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9920150.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9920150.0    \n",
      "module.adapter.frcn_linear.weight  dot:  294148192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  129950.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1336702.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1488.8887939453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2720034.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  21829714.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  138848.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.7228593826293945    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.200366735458374    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.618229866027832    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.551115123125783e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  140266.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  138848.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  525.01904296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  147.35540771484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  425.2679748535156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  59162128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94724200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452374.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  206/6933] Loss: -437.8400 [iq: 15.1928,ans: 23.1077,interp: 11.8555,fusion: -487.9961]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  278.83355712890625    \n",
      "module.ans_embedding.weight  dot:  1076958.125    \n",
      "module.lstm.weight_ih_l0  dot:  101031.734375    \n",
      "module.lstm.weight_hh_l0  dot:  4804.70703125    \n",
      "module.lstm.bias_ih_l0  dot:  7477.5390625    \n",
      "module.lstm.bias_hh_l0  dot:  7477.5390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  102584856.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19872.96484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9827840.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9827840.0    \n",
      "module.adapter.frcn_linear.weight  dot:  288857760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127282.453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2929433.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2563.248779296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5175694.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19119984.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122631.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.42250394821167    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.097639560699463    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.7863354682922363    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0880185641326534e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  124460.203125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122631.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  261.4559326171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.029701232910156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  383.9833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56617400.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  98895072.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452374.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  207/6933] Loss: -442.3467 [iq: 16.3089,ans: 26.7323,interp: 11.2886,fusion: -496.6765]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  359.51025390625    \n",
      "module.ans_embedding.weight  dot:  1137204.5    \n",
      "module.lstm.weight_ih_l0  dot:  190214.25    \n",
      "module.lstm.weight_hh_l0  dot:  8905.1298828125    \n",
      "module.lstm.bias_ih_l0  dot:  15114.3984375    \n",
      "module.lstm.bias_hh_l0  dot:  15114.3984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  123341968.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31465.279296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10164596.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10164596.0    \n",
      "module.adapter.frcn_linear.weight  dot:  492032000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  234927.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2778782.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2544.550537109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4841928.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  30223168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  212356.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.218911170959473    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.6012979745864868    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.84739875793457    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.9168668308775523e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  208089.09375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  212356.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  646.6450805664062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  186.72593688964844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  543.97607421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  63007504.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97182160.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452375.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  208/6933] Loss: -430.0233 [iq: 18.7856,ans: 24.3287,interp: 12.7810,fusion: -485.9186]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  304.27447509765625    \n",
      "module.ans_embedding.weight  dot:  1098266.5    \n",
      "module.lstm.weight_ih_l0  dot:  158896.65625    \n",
      "module.lstm.weight_hh_l0  dot:  6876.59326171875    \n",
      "module.lstm.bias_ih_l0  dot:  12186.71875    \n",
      "module.lstm.bias_hh_l0  dot:  12186.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  112341048.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  42559.18359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10084452.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10084452.0    \n",
      "module.adapter.frcn_linear.weight  dot:  421245632.0    \n",
      "module.adapter.frcn_linear.bias  dot:  201445.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3414588.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3402.58154296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5606535.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  25291220.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  178113.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.332764148712158    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7112666368484497    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.4113755226135254    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  174294.921875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  178113.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  333.7485046386719    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.91690063476562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  747.7318115234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60873788.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101551088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452375.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  209/6933] Loss: -429.4752 [iq: 22.4760,ans: 25.5662,interp: 13.0240,fusion: -490.5414]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  327.0975646972656    \n",
      "module.ans_embedding.weight  dot:  1115329.0    \n",
      "module.lstm.weight_ih_l0  dot:  186782.59375    \n",
      "module.lstm.weight_hh_l0  dot:  8085.5927734375    \n",
      "module.lstm.bias_ih_l0  dot:  14484.626953125    \n",
      "module.lstm.bias_hh_l0  dot:  14484.626953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  97183072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9659.03515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9108130.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9108130.0    \n",
      "module.adapter.frcn_linear.weight  dot:  519882400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  266964.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  901358.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  534.4260864257812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  723820.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  29181212.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  225548.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2.5089871883392334    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.7818647027015686    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2.1916751861572266    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  218639.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  225548.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1243.0394287109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  266.2768859863281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1721.835693359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56140032.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  100513232.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452376.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  210/6933] Loss: -435.3709 [iq: 17.6437,ans: 26.5894,interp: 12.4125,fusion: -492.0165]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  337.1799621582031    \n",
      "module.ans_embedding.weight  dot:  1072477.5    \n",
      "module.lstm.weight_ih_l0  dot:  157126.171875    \n",
      "module.lstm.weight_hh_l0  dot:  7365.2431640625    \n",
      "module.lstm.bias_ih_l0  dot:  11844.2021484375    \n",
      "module.lstm.bias_hh_l0  dot:  11844.2021484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  102135984.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35154.3359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8894088.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8894088.0    \n",
      "module.adapter.frcn_linear.weight  dot:  452440704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  212523.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1815764.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1738.6190185546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2814819.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  25856520.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  184717.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.3466434478759766    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.1002318859100342    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.222451686859131    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  187310.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  184717.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  277.7934265136719    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  53.24960708618164    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  452.80584716796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57272640.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94180688.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452376.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  211/6933] Loss: -435.7277 [iq: 20.4856,ans: 23.9053,interp: 16.2369,fusion: -496.3555]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  351.10150146484375    \n",
      "module.ans_embedding.weight  dot:  1163076.875    \n",
      "module.lstm.weight_ih_l0  dot:  136869.640625    \n",
      "module.lstm.weight_hh_l0  dot:  6450.451171875    \n",
      "module.lstm.bias_ih_l0  dot:  10587.1845703125    \n",
      "module.lstm.bias_hh_l0  dot:  10587.1845703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  98539048.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29507.42578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9411387.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9411387.0    \n",
      "module.adapter.frcn_linear.weight  dot:  366733376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  178211.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  641515.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  308.138916015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  572544.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23514276.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  161713.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.234076976776123    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2189518213272095    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5.110374927520752    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  160510.515625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  161713.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  575.0728759765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  141.90101623535156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1314.243408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56861048.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99208512.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452377.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  212/6933] Loss: -439.9144 [iq: 17.2665,ans: 27.2600,interp: 14.8688,fusion: -499.3098]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  394.90081787109375    \n",
      "module.ans_embedding.weight  dot:  1365710.625    \n",
      "module.lstm.weight_ih_l0  dot:  119180.90625    \n",
      "module.lstm.weight_hh_l0  dot:  5544.890625    \n",
      "module.lstm.bias_ih_l0  dot:  9014.0712890625    \n",
      "module.lstm.bias_hh_l0  dot:  9014.0712890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  120130576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37326.94140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9278123.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9278123.0    \n",
      "module.adapter.frcn_linear.weight  dot:  328012544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  160988.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  936261.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  773.9000244140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1177122.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21387512.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  153905.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.740333557128906    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.2632029056549072    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.770719528198242    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.417089082333405e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  150010.578125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  153905.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  202.60606384277344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.3477840423584    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  371.92376708984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.382049840518821e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  62137056.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  89895952.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452377.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  213/6933] Loss: -446.4048 [iq: 15.5293,ans: 20.7171,interp: 10.9941,fusion: -493.6454]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  390.70257568359375    \n",
      "module.ans_embedding.weight  dot:  942229.375    \n",
      "module.lstm.weight_ih_l0  dot:  141158.796875    \n",
      "module.lstm.weight_hh_l0  dot:  7128.24072265625    \n",
      "module.lstm.bias_ih_l0  dot:  10240.7744140625    \n",
      "module.lstm.bias_hh_l0  dot:  10240.7744140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  104281328.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4718.37890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8974746.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8974746.0    \n",
      "module.adapter.frcn_linear.weight  dot:  364423040.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159157.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2422057.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2284.01953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4507012.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24081972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  154903.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.8640971183776855    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.090272903442383    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.69448184967041    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.2660098504020425e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  165070.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  154903.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  170.3477783203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  55.48577880859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  227.52133178710938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  58397404.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101621360.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452378.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  214/6933] Loss: -449.5532 [iq: 17.4774,ans: 23.5541,interp: 14.3549,fusion: -504.9396]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  371.340576171875    \n",
      "module.ans_embedding.weight  dot:  925772.9375    \n",
      "module.lstm.weight_ih_l0  dot:  140408.28125    \n",
      "module.lstm.weight_hh_l0  dot:  5891.1328125    \n",
      "module.lstm.bias_ih_l0  dot:  10632.7177734375    \n",
      "module.lstm.bias_hh_l0  dot:  10632.7177734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  87618656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28360.96484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9067936.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9067936.0    \n",
      "module.adapter.frcn_linear.weight  dot:  364956512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  179703.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  344354.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  224.8258514404297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  286439.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  24126930.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  170206.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1.5139445066452026    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.39695581793785095    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1.3500709533691406    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3101520696400257e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  164657.796875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  170206.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  122.72557067871094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.427892684936523    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  253.1467742919922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.105871115849368e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  52898748.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  106529848.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452378.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  215/6933] Loss: -445.4285 [iq: 19.7853,ans: 29.0504,interp: 14.0465,fusion: -508.3107]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  532.286865234375    \n",
      "module.ans_embedding.weight  dot:  1108949.5    \n",
      "module.lstm.weight_ih_l0  dot:  175473.515625    \n",
      "module.lstm.weight_hh_l0  dot:  8287.078125    \n",
      "module.lstm.bias_ih_l0  dot:  13298.16796875    \n",
      "module.lstm.bias_hh_l0  dot:  13298.16796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  113243880.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4494.48974609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8471608.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8471608.0    \n",
      "module.adapter.frcn_linear.weight  dot:  464812384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  227752.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  470502.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  336.8438720703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  417097.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27482536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  206328.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.095732688903809    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.135056495666504    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.84975814819336    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  202877.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  206328.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  82.92752075195312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.925880432128906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  117.45233154296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  60175564.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  90841984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452379.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  216/6933] Loss: -458.1747 [iq: 14.4022,ans: 19.7780,interp: 11.6604,fusion: -504.0152]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  403.026123046875    \n",
      "module.ans_embedding.weight  dot:  1001046.0    \n",
      "module.lstm.weight_ih_l0  dot:  123701.6953125    \n",
      "module.lstm.weight_hh_l0  dot:  5978.0234375    \n",
      "module.lstm.bias_ih_l0  dot:  9118.958984375    \n",
      "module.lstm.bias_hh_l0  dot:  9118.958984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  102418424.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16929.60546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8707823.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8707823.0    \n",
      "module.adapter.frcn_linear.weight  dot:  312741056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  144975.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2267887.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2340.93017578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4170707.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21897976.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  150654.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.385741710662842    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.4712305068969727    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.970430374145508    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_lang.linear_merge.weight  dot:  154073.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  150654.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  55.815895080566406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.473037719726562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  89.68555450439453    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.231304030431147e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  57576700.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97562576.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452380.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  217/6933] Loss: -449.9620 [iq: 17.9249,ans: 24.4609,interp: 12.6896,fusion: -505.0374]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  396.0445251464844    \n",
      "module.ans_embedding.weight  dot:  990865.8125    \n",
      "module.lstm.weight_ih_l0  dot:  130589.859375    \n",
      "module.lstm.weight_hh_l0  dot:  5939.482421875    \n",
      "module.lstm.bias_ih_l0  dot:  10065.259765625    \n",
      "module.lstm.bias_hh_l0  dot:  10065.259765625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  86912544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18845.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  9127198.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  9127198.0    \n",
      "module.adapter.frcn_linear.weight  dot:  307310784.0    \n",
      "module.adapter.frcn_linear.bias  dot:  132285.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2658681.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2886.4296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4940286.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  21391444.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  143775.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.656452178955078    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.0810751914978027    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7.083134651184082    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.206324095117452e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  140451.984375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  143775.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  117.35847473144531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.43059539794922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  167.100341796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53875840.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  105570648.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452380.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  218/6933] Loss: -452.8577 [iq: 16.7661,ans: 28.3122,interp: 9.8958,fusion: -507.8318]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  325.15106201171875    \n",
      "module.ans_embedding.weight  dot:  938605.4375    \n",
      "module.lstm.weight_ih_l0  dot:  116494.265625    \n",
      "module.lstm.weight_hh_l0  dot:  5563.1630859375    \n",
      "module.lstm.bias_ih_l0  dot:  8617.974609375    \n",
      "module.lstm.bias_hh_l0  dot:  8617.974609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  91601832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  44744.78125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8393453.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8393453.0    \n",
      "module.adapter.frcn_linear.weight  dot:  287758912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  123589.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2222899.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2583.17919921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4647609.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  21106060.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  131271.140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.5844202041625977    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.2815003395080566    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.1517133712768555    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  135711.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  131271.140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  400.5518493652344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.42190551757812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  857.9578247070312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53962128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93497072.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452380.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  219/6933] Loss: -454.7981 [iq: 19.3888,ans: 24.8598,interp: 13.8864,fusion: -512.9331]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  371.91552734375    \n",
      "module.ans_embedding.weight  dot:  886806.75    \n",
      "module.lstm.weight_ih_l0  dot:  108822.03125    \n",
      "module.lstm.weight_hh_l0  dot:  5525.36279296875    \n",
      "module.lstm.bias_ih_l0  dot:  7736.298828125    \n",
      "module.lstm.bias_hh_l0  dot:  7736.298828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  78066688.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32426.01171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7955122.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7955122.0    \n",
      "module.adapter.frcn_linear.weight  dot:  262920960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133995.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  316809.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  231.28628540039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  368814.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19225752.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  140802.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7.5536041259765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.9225802421569824    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8.891194343566895    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  145393.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  140802.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  394.58319091796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  79.31629943847656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  390.1613464355469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49167024.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94853040.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452381.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  220/6933] Loss: -460.0626 [iq: 17.0606,ans: 29.4829,interp: 14.9191,fusion: -521.5252]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  350.4264831542969    \n",
      "module.ans_embedding.weight  dot:  891307.875    \n",
      "module.lstm.weight_ih_l0  dot:  142372.96875    \n",
      "module.lstm.weight_hh_l0  dot:  6239.2177734375    \n",
      "module.lstm.bias_ih_l0  dot:  11272.3671875    \n",
      "module.lstm.bias_hh_l0  dot:  11272.3671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  85626656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37276.265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8807385.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8807385.0    \n",
      "module.adapter.frcn_linear.weight  dot:  438573120.0    \n",
      "module.adapter.frcn_linear.bias  dot:  205115.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1102837.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  622.159912109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  985092.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  25464402.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  181021.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3.236530303955078    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  0.9339509010314941    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4.144472122192383    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  171364.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  181021.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  340.97998046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  99.67438507080078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  421.40350341796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.323741778644035e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53403496.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  104731824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452382.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  221/6933] Loss: -458.0228 [iq: 16.4296,ans: 26.9025,interp: 13.8351,fusion: -515.1899]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  566.8176879882812    \n",
      "module.ans_embedding.weight  dot:  625928.5    \n",
      "module.lstm.weight_ih_l0  dot:  204579.4375    \n",
      "module.lstm.weight_hh_l0  dot:  10556.662109375    \n",
      "module.lstm.bias_ih_l0  dot:  14936.7744140625    \n",
      "module.lstm.bias_hh_l0  dot:  14936.7744140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  89408096.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34463.91796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8363488.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8363488.5    \n",
      "module.adapter.frcn_linear.weight  dot:  401043872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  187924.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1113188.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  731.0328369140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1204189.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26940192.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  200255.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.485269546508789    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.81594181060791    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.080821990966797    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6187051699034782e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  211081.546875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  200255.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  393.6558532714844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  106.17237091064453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  501.81353759765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53587800.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101206032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452382.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  222/6933] Loss: -466.6337 [iq: 17.3947,ans: 25.3930,interp: 12.3170,fusion: -521.7385]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  795.1167602539062    \n",
      "module.ans_embedding.weight  dot:  873158.875    \n",
      "module.lstm.weight_ih_l0  dot:  297885.375    \n",
      "module.lstm.weight_hh_l0  dot:  14518.630859375    \n",
      "module.lstm.bias_ih_l0  dot:  23428.03515625    \n",
      "module.lstm.bias_hh_l0  dot:  23428.03515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  83613904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1342.6259765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7854709.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7854709.5    \n",
      "module.adapter.frcn_linear.weight  dot:  817505792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  390995.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1713714.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1181.3345947265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2017512.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  43384328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  345481.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.436529159545898    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.942488670349121    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10.986654281616211    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  340856.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  345481.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  52.18733215332031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.668787002563477    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  84.9478988647461    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  54666480.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  106942912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452383.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  223/6933] Loss: -458.1111 [iq: 14.9184,ans: 25.9018,interp: 9.0483,fusion: -507.9796]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  554.2286987304688    \n",
      "module.ans_embedding.weight  dot:  821809.625    \n",
      "module.lstm.weight_ih_l0  dot:  157273.921875    \n",
      "module.lstm.weight_hh_l0  dot:  7784.09765625    \n",
      "module.lstm.bias_ih_l0  dot:  11374.537109375    \n",
      "module.lstm.bias_hh_l0  dot:  11374.537109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  85691184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20456.43359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7860418.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7860418.5    \n",
      "module.adapter.frcn_linear.weight  dot:  410665440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  185463.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  989594.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  903.0269165039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1414496.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  25740160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  177189.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.7041654586792    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.422112464904785    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.6026611328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  184057.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  177189.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  487.12567138671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  125.69998168945312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1004.0400390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53636100.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  96736112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452383.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  224/6933] Loss: -466.3723 [iq: 15.9257,ans: 24.7943,interp: 13.4344,fusion: -520.5267]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  519.9598388671875    \n",
      "module.ans_embedding.weight  dot:  1083772.125    \n",
      "module.lstm.weight_ih_l0  dot:  213909.625    \n",
      "module.lstm.weight_hh_l0  dot:  9251.25390625    \n",
      "module.lstm.bias_ih_l0  dot:  16515.591796875    \n",
      "module.lstm.bias_hh_l0  dot:  16515.591796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  92683592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31010.30859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8046096.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8046096.0    \n",
      "module.adapter.frcn_linear.weight  dot:  619366528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  277604.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5951750.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5716.7548828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9521499.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  29694860.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241280.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8.908764839172363    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7491586208343506    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  9.95679759979248    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  232005.71875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241280.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  523.5682983398438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  72.51220703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1268.544921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56067060.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99060584.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452384.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  225/6933] Loss: -463.7445 [iq: 12.7304,ans: 24.9018,interp: 10.3441,fusion: -511.7208]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  560.0120849609375    \n",
      "module.ans_embedding.weight  dot:  1019553.875    \n",
      "module.lstm.weight_ih_l0  dot:  163117.4375    \n",
      "module.lstm.weight_hh_l0  dot:  7299.86865234375    \n",
      "module.lstm.bias_ih_l0  dot:  11858.892578125    \n",
      "module.lstm.bias_hh_l0  dot:  11858.892578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  93515568.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  0.0    NOT UPDATING\n",
      "module.ans_lstm.bias_ih_l0  dot:  7422522.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7422522.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387527232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  180586.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2840577.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2881.75439453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5084001.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  22387512.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  169070.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.867486000061035    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.849715232849121    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.987638473510742    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  171594.515625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  169070.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  55999804.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93745552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  21  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452385.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  226/6933] Loss: -479.4260 [iq: 11.2923,ans: 22.6359,interp: 9.6021,fusion: -522.9564]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  468.24639892578125    \n",
      "module.ans_embedding.weight  dot:  772041.75    \n",
      "module.lstm.weight_ih_l0  dot:  178201.25    \n",
      "module.lstm.weight_hh_l0  dot:  8620.5751953125    \n",
      "module.lstm.bias_ih_l0  dot:  13675.1103515625    \n",
      "module.lstm.bias_hh_l0  dot:  13675.1103515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  80164256.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2507.537841796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7243474.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7243474.5    \n",
      "module.adapter.frcn_linear.weight  dot:  504577280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  228524.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4597646.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4725.421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8617096.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27068070.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  206437.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.142080307006836    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.437130928039551    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.069108963012695    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  206216.34375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  206437.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  227.8624267578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  37.791316986083984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  316.89898681640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53149364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  98647072.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452385.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  227/6933] Loss: -475.5763 [iq: 15.3484,ans: 24.6521,interp: 10.4268,fusion: -526.0035]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  525.5235595703125    \n",
      "module.ans_embedding.weight  dot:  714793.25    \n",
      "module.lstm.weight_ih_l0  dot:  142566.59375    \n",
      "module.lstm.weight_hh_l0  dot:  6792.81689453125    \n",
      "module.lstm.bias_ih_l0  dot:  10482.0556640625    \n",
      "module.lstm.bias_hh_l0  dot:  10482.0556640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  94195608.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36978.4921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8257324.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8257324.0    \n",
      "module.adapter.frcn_linear.weight  dot:  344648320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159147.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3698358.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4105.673828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7154062.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22426696.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  171768.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.987527847290039    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.4101643562316895    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  14.701799392700195    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4948931809376518e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  170639.09375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  171768.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  265.40960693359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.91524887084961    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  543.5195922851562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  56440812.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101837760.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452386.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  228/6933] Loss: -475.9250 [iq: 14.6576,ans: 23.5671,interp: 11.2584,fusion: -525.4081]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  540.3555297851562    \n",
      "module.ans_embedding.weight  dot:  756912.375    \n",
      "module.lstm.weight_ih_l0  dot:  126705.5234375    \n",
      "module.lstm.weight_hh_l0  dot:  6545.03173828125    \n",
      "module.lstm.bias_ih_l0  dot:  9142.8349609375    \n",
      "module.lstm.bias_hh_l0  dot:  9142.8349609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  78195344.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6340.1669921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6988732.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6988732.0    \n",
      "module.adapter.frcn_linear.weight  dot:  272822976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133308.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  738642.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  465.0019226074219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  918216.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  18563048.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  148083.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.274148941040039    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.9682652950286865    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  17.490657806396484    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4141578453272814e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  151781.515625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  148083.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  109.58785247802734    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  25.549732208251953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  119.94246673583984    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49593392.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97517760.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452386.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  229/6933] Loss: -476.5357 [iq: 15.1743,ans: 24.7942,interp: 10.7681,fusion: -527.2723]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  820.7254638671875    \n",
      "module.ans_embedding.weight  dot:  551914.4375    \n",
      "module.lstm.weight_ih_l0  dot:  280103.5    \n",
      "module.lstm.weight_hh_l0  dot:  13919.21484375    \n",
      "module.lstm.bias_ih_l0  dot:  20686.927734375    \n",
      "module.lstm.bias_hh_l0  dot:  20686.927734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  89243240.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12643.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8525515.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8525515.0    \n",
      "module.adapter.frcn_linear.weight  dot:  582606912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  292020.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3126721.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2814.339111328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4359773.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32353944.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  290176.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18.565563201904297    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.5613226890563965    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  25.909107208251953    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.496403249731884e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  303597.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  290176.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  291.21527099609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  100.6617431640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  354.0564880371094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.555378710501827e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  53143528.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  108153616.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452387.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  230/6933] Loss: -460.6985 [iq: 21.6949,ans: 25.0226,interp: 16.7512,fusion: -524.1672]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  534.654052734375    \n",
      "module.ans_embedding.weight  dot:  724238.4375    \n",
      "module.lstm.weight_ih_l0  dot:  112534.375    \n",
      "module.lstm.weight_hh_l0  dot:  5613.0078125    \n",
      "module.lstm.bias_ih_l0  dot:  7944.1630859375    \n",
      "module.lstm.bias_hh_l0  dot:  7944.1630859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  73452976.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10400.59375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6805873.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6805873.0    \n",
      "module.adapter.frcn_linear.weight  dot:  277272640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  126989.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1541849.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1426.4410400390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2435293.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17843946.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133967.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5.341026782989502    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.7457795143127441    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6.2671966552734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  140356.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133967.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  263.84967041015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  87.37451171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  346.4968566894531    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46560680.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  92191136.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452388.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  231/6933] Loss: -475.0854 [iq: 16.8878,ans: 25.0556,interp: 13.6914,fusion: -530.7202]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  871.707275390625    \n",
      "module.ans_embedding.weight  dot:  744466.5    \n",
      "module.lstm.weight_ih_l0  dot:  172218.25    \n",
      "module.lstm.weight_hh_l0  dot:  9094.369140625    \n",
      "module.lstm.bias_ih_l0  dot:  11133.2861328125    \n",
      "module.lstm.bias_hh_l0  dot:  11133.2861328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  76457392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19926.9140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6899148.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6899148.5    \n",
      "module.adapter.frcn_linear.weight  dot:  398083424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  189684.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2249800.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2243.13232421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3562889.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22513088.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  182210.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24.55208969116211    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  5.309918403625488    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  33.669578552246094    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  203535.21875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  182210.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  298.62139892578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.97909545898438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  553.6878662109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49835840.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94101952.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452389.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  232/6933] Loss: -480.0764 [iq: 15.1130,ans: 23.7436,interp: 11.0463,fusion: -529.9793]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  667.2046508789062    \n",
      "module.ans_embedding.weight  dot:  669263.0    \n",
      "module.lstm.weight_ih_l0  dot:  172648.84375    \n",
      "module.lstm.weight_hh_l0  dot:  8460.4111328125    \n",
      "module.lstm.bias_ih_l0  dot:  12845.7138671875    \n",
      "module.lstm.bias_hh_l0  dot:  12845.7138671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  70468544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16616.44140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7160950.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7160950.5    \n",
      "module.adapter.frcn_linear.weight  dot:  413157568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  215380.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1388114.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1846.6239013671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2724419.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22799556.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  212668.703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.585585594177246    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.5954504013061523    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15.567281723022461    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.695044270557446e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  215548.078125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  212668.703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  173.42861938476562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.372135162353516    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  204.93577575683594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  47084948.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101076656.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452389.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  233/6933] Loss: -462.4238 [iq: 19.5977,ans: 27.4777,interp: 19.5324,fusion: -529.0316]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  677.1041259765625    \n",
      "module.ans_embedding.weight  dot:  853127.9375    \n",
      "module.lstm.weight_ih_l0  dot:  135834.6875    \n",
      "module.lstm.weight_hh_l0  dot:  6653.75048828125    \n",
      "module.lstm.bias_ih_l0  dot:  9841.044921875    \n",
      "module.lstm.bias_hh_l0  dot:  9841.044921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  82330240.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10423.216796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7273512.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7273512.5    \n",
      "module.adapter.frcn_linear.weight  dot:  270940832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133492.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  555079.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  509.7450256347656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  859425.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20351140.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160726.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6.933780193328857    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.3670470714569092    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  9.076133728027344    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.220446049250313e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  161431.28125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160726.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  40.60908508300781    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.207115173339844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  62.238502502441406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49061336.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  89911288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452390.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  234/6933] Loss: -480.2398 [iq: 13.4607,ans: 22.3634,interp: 9.5750,fusion: -525.6389]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  737.6287841796875    \n",
      "module.ans_embedding.weight  dot:  750911.3125    \n",
      "module.lstm.weight_ih_l0  dot:  136028.84375    \n",
      "module.lstm.weight_hh_l0  dot:  6905.58056640625    \n",
      "module.lstm.bias_ih_l0  dot:  9611.60546875    \n",
      "module.lstm.bias_hh_l0  dot:  9611.60546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  66390972.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10690.6357421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6057274.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6057274.0    \n",
      "module.adapter.frcn_linear.weight  dot:  244883408.0    \n",
      "module.adapter.frcn_linear.bias  dot:  122125.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  225805.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  135.47552490234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  162279.671875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19020808.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  143319.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.97138786315918    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.0656638145446777    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.901418685913086    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  151292.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  143319.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  39.08039093017578    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.020185470581055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  67.79645538330078    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  44438472.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  83992112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452390.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  235/6933] Loss: -491.2316 [iq: 15.4106,ans: 22.9874,interp: 8.9298,fusion: -538.5593]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  714.6357421875    \n",
      "module.ans_embedding.weight  dot:  470741.625    \n",
      "module.lstm.weight_ih_l0  dot:  155893.5625    \n",
      "module.lstm.weight_hh_l0  dot:  8133.34375    \n",
      "module.lstm.bias_ih_l0  dot:  10429.7265625    \n",
      "module.lstm.bias_hh_l0  dot:  10429.7265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  67027680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12193.3984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7034961.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7034961.0    \n",
      "module.adapter.frcn_linear.weight  dot:  273516352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  130050.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  644898.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  645.54736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  963828.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19082284.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  148531.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21.29401969909668    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.642341136932373    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  29.63026237487793    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.987032926033862e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  162426.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  148531.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  664.9659423828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  204.7794189453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  747.626953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  47245720.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  107305936.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452391.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  236/6933] Loss: -481.5972 [iq: 19.0810,ans: 27.2071,interp: 13.7992,fusion: -541.6844]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  821.280029296875    \n",
      "module.ans_embedding.weight  dot:  895108.125    \n",
      "module.lstm.weight_ih_l0  dot:  246085.921875    \n",
      "module.lstm.weight_hh_l0  dot:  10656.9140625    \n",
      "module.lstm.bias_ih_l0  dot:  18859.19140625    \n",
      "module.lstm.bias_hh_l0  dot:  18859.19140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  92023904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  52733.46875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8334895.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8334895.0    \n",
      "module.adapter.frcn_linear.weight  dot:  494859584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  259431.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1074152.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1391.388671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2040001.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  27858378.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  267631.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4.559453010559082    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1.0240223407745361    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3.7968921661376953    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  254378.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  267631.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  115.01828002929688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.81623840332031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  96.05619812011719    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.912070832891914e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  55053832.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  101180480.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452392.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  237/6933] Loss: -462.5613 [iq: 17.6757,ans: 22.6381,interp: 17.3397,fusion: -520.2148]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  556.986572265625    \n",
      "module.ans_embedding.weight  dot:  737827.625    \n",
      "module.lstm.weight_ih_l0  dot:  124946.796875    \n",
      "module.lstm.weight_hh_l0  dot:  5537.33056640625    \n",
      "module.lstm.bias_ih_l0  dot:  8958.3828125    \n",
      "module.lstm.bias_hh_l0  dot:  8958.3828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  68879072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10312.2607421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6322019.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6322019.5    \n",
      "module.adapter.frcn_linear.weight  dot:  274976032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  134439.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  912339.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  857.5894775390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1480083.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16970106.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137194.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.25729751586914    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.3030014038085938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  13.117753028869629    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.695044270557446e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  136850.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137194.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  286.3799743652344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  67.53932189941406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  498.1738586425781    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  45661632.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93199808.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452392.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  238/6933] Loss: -492.2211 [iq: 12.9485,ans: 23.4390,interp: 9.7703,fusion: -538.3789]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  650.0624389648438    \n",
      "module.ans_embedding.weight  dot:  493845.46875    \n",
      "module.lstm.weight_ih_l0  dot:  169993.671875    \n",
      "module.lstm.weight_hh_l0  dot:  9015.4833984375    \n",
      "module.lstm.bias_ih_l0  dot:  12248.44140625    \n",
      "module.lstm.bias_hh_l0  dot:  12248.44140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60568660.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21961.10546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6173494.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6173494.5    \n",
      "module.adapter.frcn_linear.weight  dot:  365363136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  176341.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1455564.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1685.7451171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2432010.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20306404.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  175656.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12.250616073608398    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.532634973526001    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.499996185302734    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.568967592102126e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  184774.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  175656.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  349.98748779296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.2081069946289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  813.6197509765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  43761696.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99547200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452393.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  239/6933] Loss: -490.7462 [iq: 17.4135,ans: 25.3944,interp: 11.6116,fusion: -545.1658]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  782.0081787109375    \n",
      "module.ans_embedding.weight  dot:  627633.75    \n",
      "module.lstm.weight_ih_l0  dot:  152732.25    \n",
      "module.lstm.weight_hh_l0  dot:  7367.134765625    \n",
      "module.lstm.bias_ih_l0  dot:  10732.82421875    \n",
      "module.lstm.bias_hh_l0  dot:  10732.82421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  81149096.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19761.96484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6674244.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6674244.0    \n",
      "module.adapter.frcn_linear.weight  dot:  312367808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159306.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  986991.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  752.6792602539062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1088516.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19155124.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  168296.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  27.740142822265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  6.891947269439697    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  35.98375701904297    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  171828.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  168296.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  475.563720703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  58.09693908691406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  997.28857421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  50963556.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  91616096.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452393.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  240/6933] Loss: -488.9330 [iq: 16.2153,ans: 21.2403,interp: 11.1577,fusion: -537.5463]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  744.7929077148438    \n",
      "module.ans_embedding.weight  dot:  475571.34375    \n",
      "module.lstm.weight_ih_l0  dot:  190278.4375    \n",
      "module.lstm.weight_hh_l0  dot:  9851.5830078125    \n",
      "module.lstm.bias_ih_l0  dot:  13988.453125    \n",
      "module.lstm.bias_hh_l0  dot:  13988.453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  67629840.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9623.078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6385791.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6385791.0    \n",
      "module.adapter.frcn_linear.weight  dot:  499775616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  235556.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2268985.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2221.75830078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3586050.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26090824.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  224530.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.992918014526367    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.9382643699645996    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  25.105772018432617    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  232159.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  224530.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  220.39088439941406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  78.23580932617188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  388.5457763671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46201560.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  100721088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452394.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  241/6933] Loss: -492.0856 [iq: 16.8565,ans: 24.1206,interp: 11.0062,fusion: -544.0689]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  769.9263916015625    \n",
      "module.ans_embedding.weight  dot:  751905.0    \n",
      "module.lstm.weight_ih_l0  dot:  151349.90625    \n",
      "module.lstm.weight_hh_l0  dot:  7320.384765625    \n",
      "module.lstm.bias_ih_l0  dot:  10639.4140625    \n",
      "module.lstm.bias_hh_l0  dot:  10639.4140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  71991488.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20705.1875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6296089.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6296089.5    \n",
      "module.adapter.frcn_linear.weight  dot:  285077664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  140896.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  649022.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  653.4193115234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  995545.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17995830.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  152933.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9.6035795211792    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.051258087158203    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12.610396385192871    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  154926.015625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  152933.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  360.44970703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  79.67637634277344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  430.1866149902344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46709072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86850856.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452395.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  242/6933] Loss: -491.4578 [iq: 14.7748,ans: 20.8587,interp: 12.9846,fusion: -540.0759]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  842.21044921875    \n",
      "module.ans_embedding.weight  dot:  685212.125    \n",
      "module.lstm.weight_ih_l0  dot:  119303.03125    \n",
      "module.lstm.weight_hh_l0  dot:  6174.99755859375    \n",
      "module.lstm.bias_ih_l0  dot:  7941.32421875    \n",
      "module.lstm.bias_hh_l0  dot:  7941.32421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60155276.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35778.6328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5959426.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5959426.5    \n",
      "module.adapter.frcn_linear.weight  dot:  284620992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127283.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1849451.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2150.301513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3495630.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18000362.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  135526.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14.280592918395996    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.8648643493652344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  23.62483024597168    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2159162565694714e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  142166.03125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  135526.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  628.0025634765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  123.7053451538086    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1181.732421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42582424.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82153592.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452396.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  243/6933] Loss: -497.1166 [iq: 12.8220,ans: 20.3759,interp: 10.9216,fusion: -541.2361]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  889.8275146484375    \n",
      "module.ans_embedding.weight  dot:  551120.125    \n",
      "module.lstm.weight_ih_l0  dot:  230405.796875    \n",
      "module.lstm.weight_hh_l0  dot:  11987.359375    \n",
      "module.lstm.bias_ih_l0  dot:  16639.99609375    \n",
      "module.lstm.bias_hh_l0  dot:  16639.99609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  62103432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23146.6953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6034887.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6034887.0    \n",
      "module.adapter.frcn_linear.weight  dot:  533590336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  246502.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2089658.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1928.388427734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2929204.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27421262.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  232984.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21.05859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  4.305081844329834    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  36.764068603515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.566746732351021e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  240347.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  232984.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  544.66064453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  135.84933471679688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1067.0640869140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  45979404.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  97598000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452396.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  244/6933] Loss: -488.5370 [iq: 16.1518,ans: 22.6610,interp: 12.8048,fusion: -540.1546]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  962.73876953125    \n",
      "module.ans_embedding.weight  dot:  671250.625    \n",
      "module.lstm.weight_ih_l0  dot:  179755.96875    \n",
      "module.lstm.weight_hh_l0  dot:  9522.802734375    \n",
      "module.lstm.bias_ih_l0  dot:  12803.173828125    \n",
      "module.lstm.bias_hh_l0  dot:  12803.173828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  68199472.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14049.75390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5917521.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5917521.5    \n",
      "module.adapter.frcn_linear.weight  dot:  469979104.0    \n",
      "module.adapter.frcn_linear.bias  dot:  204122.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6024502.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  7257.56982421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  12007745.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23568608.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  199146.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11.667058944702148    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.338207483291626    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.275135040283203    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  207996.09375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  199146.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  180.698974609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.35615158081055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  220.427978515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46872760.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  90664528.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452397.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  245/6933] Loss: -496.7060 [iq: 13.9900,ans: 20.5293,interp: 11.8839,fusion: -543.1092]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1354.7259521484375    \n",
      "module.ans_embedding.weight  dot:  580181.625    \n",
      "module.lstm.weight_ih_l0  dot:  216794.0625    \n",
      "module.lstm.weight_hh_l0  dot:  12599.220703125    \n",
      "module.lstm.bias_ih_l0  dot:  14317.9521484375    \n",
      "module.lstm.bias_hh_l0  dot:  14317.9521484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  76038640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  27222.50390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6411351.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6411351.0    \n",
      "module.adapter.frcn_linear.weight  dot:  330468352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  165359.671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1309001.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1079.2139892578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1544662.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21342536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  193715.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53.96709060668945    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.916970252990723    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  90.09877014160156    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  220840.359375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  193715.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  165.44808959960938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.751415252685547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  297.5747985839844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.94040388996109e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46070584.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82589808.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452398.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  246/6933] Loss: -501.5881 [iq: 13.6324,ans: 19.3558,interp: 11.2096,fusion: -545.7860]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  825.810791015625    \n",
      "module.ans_embedding.weight  dot:  596983.875    \n",
      "module.lstm.weight_ih_l0  dot:  213890.71875    \n",
      "module.lstm.weight_hh_l0  dot:  10826.294921875    \n",
      "module.lstm.bias_ih_l0  dot:  15576.4501953125    \n",
      "module.lstm.bias_hh_l0  dot:  15576.4501953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  69011248.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14234.681640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5967007.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5967007.5    \n",
      "module.adapter.frcn_linear.weight  dot:  546067200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  277194.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1960789.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1562.779052734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2042135.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  26003382.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  250658.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13.799822807312012    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7765321731567383    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  26.299381256103516    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.8689051962137455e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  257209.703125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  250658.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  549.3189086914062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  122.26364135742188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1060.375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  48152540.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  94654336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452398.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  247/6933] Loss: -490.1440 [iq: 18.2254,ans: 20.8030,interp: 12.1974,fusion: -541.3697]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  943.2442016601562    \n",
      "module.ans_embedding.weight  dot:  651036.5625    \n",
      "module.lstm.weight_ih_l0  dot:  211529.5625    \n",
      "module.lstm.weight_hh_l0  dot:  10148.595703125    \n",
      "module.lstm.bias_ih_l0  dot:  15252.3671875    \n",
      "module.lstm.bias_hh_l0  dot:  15252.3671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  74114144.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2746.00390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6219795.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6219795.0    \n",
      "module.adapter.frcn_linear.weight  dot:  452471680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  215906.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5563702.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5987.13134765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9709611.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23657906.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  225764.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10.789163589477539    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2.7338614463806152    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  19.00298309326172    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  230959.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  225764.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18.211143493652344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.20001220703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  31.18895721435547    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  49115424.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99760944.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452399.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  248/6933] Loss: -488.8155 [iq: 15.6724,ans: 20.7645,interp: 10.6960,fusion: -535.9483]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  987.9464721679688    \n",
      "module.ans_embedding.weight  dot:  468762.46875    \n",
      "module.lstm.weight_ih_l0  dot:  152367.375    \n",
      "module.lstm.weight_hh_l0  dot:  8290.48828125    \n",
      "module.lstm.bias_ih_l0  dot:  10356.5224609375    \n",
      "module.lstm.bias_hh_l0  dot:  10356.5224609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50046960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10167.5029296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4959448.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4959448.5    \n",
      "module.adapter.frcn_linear.weight  dot:  316717632.0    \n",
      "module.adapter.frcn_linear.bias  dot:  141037.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1695277.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1481.794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2446875.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18518808.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  145658.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15.523506164550781    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  4.90064811706543    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  28.93498420715332    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  159390.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  145658.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  68.74060821533203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.3407039642334    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  65.44794464111328    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38527416.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  83405752.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452400.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  249/6933] Loss: -504.2362 [iq: 15.9168,ans: 22.9185,interp: 10.1053,fusion: -553.1768]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1000.5220947265625    \n",
      "module.ans_embedding.weight  dot:  730683.375    \n",
      "module.lstm.weight_ih_l0  dot:  177999.9375    \n",
      "module.lstm.weight_hh_l0  dot:  8874.849609375    \n",
      "module.lstm.bias_ih_l0  dot:  12598.380859375    \n",
      "module.lstm.bias_hh_l0  dot:  12598.380859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  59823456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16231.275390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5458983.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5458983.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387249568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  170052.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4130849.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5118.66748046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8329722.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.007016857736744e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21259186.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  177472.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20.17034339904785    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  6.416426658630371    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  38.412696838378906    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  180385.984375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  177472.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  101.1558837890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  25.77251625061035    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  111.40524291992188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41480088.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  79673848.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452400.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  250/6933] Loss: -506.1640 [iq: 12.4795,ans: 19.5890,interp: 9.3647,fusion: -547.5972]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1384.90869140625    \n",
      "module.ans_embedding.weight  dot:  526910.375    \n",
      "module.lstm.weight_ih_l0  dot:  201788.1875    \n",
      "module.lstm.weight_hh_l0  dot:  12900.3564453125    \n",
      "module.lstm.bias_ih_l0  dot:  12563.7890625    \n",
      "module.lstm.bias_hh_l0  dot:  12563.7890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  60418244.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13942.921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5130798.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5130798.5    \n",
      "module.adapter.frcn_linear.weight  dot:  335883968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  158686.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1853224.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2010.5948486328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3250647.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19769286.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  170828.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  40.81486892700195    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  7.673496246337891    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  76.08290100097656    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  199342.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  170828.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  207.7547607421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  40.44304275512695    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  549.9788818359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42872364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80330736.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452401.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  251/6933] Loss: -509.7182 [iq: 14.5685,ans: 18.9500,interp: 13.5848,fusion: -556.8215]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1051.18701171875    \n",
      "module.ans_embedding.weight  dot:  544974.9375    \n",
      "module.lstm.weight_ih_l0  dot:  150411.0625    \n",
      "module.lstm.weight_hh_l0  dot:  8373.177734375    \n",
      "module.lstm.bias_ih_l0  dot:  10334.90625    \n",
      "module.lstm.bias_hh_l0  dot:  10334.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54287064.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19620.0390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5219141.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5219141.0    \n",
      "module.adapter.frcn_linear.weight  dot:  307712704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  147754.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  515207.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  530.4362182617188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  727906.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20618160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  155955.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23.14922332763672    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  4.680243015289307    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  31.482223510742188    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  167668.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  155955.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  235.87054443359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.61787414550781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  577.5073852539062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  40011300.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84028144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452402.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  252/6933] Loss: -502.6166 [iq: 16.7375,ans: 22.8772,interp: 10.6443,fusion: -552.8756]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1116.5626220703125    \n",
      "module.ans_embedding.weight  dot:  492625.03125    \n",
      "module.lstm.weight_ih_l0  dot:  224823.1875    \n",
      "module.lstm.weight_hh_l0  dot:  12574.078125    \n",
      "module.lstm.bias_ih_l0  dot:  15292.62109375    \n",
      "module.lstm.bias_hh_l0  dot:  15292.62109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  53190784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10528.4638671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4979923.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4979923.0    \n",
      "module.adapter.frcn_linear.weight  dot:  537686400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  264792.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1529423.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1354.6790771484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1941117.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26070808.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  230277.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19.648359298706055    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  3.929159164428711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  42.062599182128906    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  249234.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  230277.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  189.34326171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.59601593017578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  505.08807373046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  43715112.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93082944.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452403.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  253/6933] Loss: -498.8843 [iq: 17.3804,ans: 21.5960,interp: 16.5505,fusion: -554.4112]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1697.3729248046875    \n",
      "module.ans_embedding.weight  dot:  402369.25    \n",
      "module.lstm.weight_ih_l0  dot:  305421.46875    \n",
      "module.lstm.weight_hh_l0  dot:  18008.7890625    \n",
      "module.lstm.bias_ih_l0  dot:  21733.22265625    \n",
      "module.lstm.bias_hh_l0  dot:  21733.22265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  61327832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4457.84423828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6746356.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6746356.5    \n",
      "module.adapter.frcn_linear.weight  dot:  326878912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  151594.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1603259.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1772.583984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2816027.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23171050.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211645.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  51.29655456542969    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.88990306854248    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  105.56651306152344    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.51754214434186e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  245849.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211645.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  129.5667266845703    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.06317138671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  328.1783752441406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  43019976.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  106578056.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452403.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  254/6933] Loss: -491.8077 [iq: 20.8875,ans: 26.4117,interp: 12.3303,fusion: -551.4372]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1129.1619873046875    \n",
      "module.ans_embedding.weight  dot:  509811.8125    \n",
      "module.lstm.weight_ih_l0  dot:  189210.765625    \n",
      "module.lstm.weight_hh_l0  dot:  10641.6201171875    \n",
      "module.lstm.bias_ih_l0  dot:  13482.62109375    \n",
      "module.lstm.bias_hh_l0  dot:  13482.62109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54802296.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17897.2265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4897975.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4897975.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387978432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  189959.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1484729.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1220.7843017578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1781384.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  20454666.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  180638.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46.827999114990234    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.311534881591797    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  78.97862243652344    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  187611.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  180638.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  59.00786590576172    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.164931297302246    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  140.2110137939453    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41551936.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84107936.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452404.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  255/6933] Loss: -501.8850 [iq: 17.9201,ans: 20.1341,interp: 13.6385,fusion: -553.5777]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1339.481201171875    \n",
      "module.ans_embedding.weight  dot:  410571.0    \n",
      "module.lstm.weight_ih_l0  dot:  271867.625    \n",
      "module.lstm.weight_hh_l0  dot:  15513.318359375    \n",
      "module.lstm.bias_ih_l0  dot:  18277.005859375    \n",
      "module.lstm.bias_hh_l0  dot:  18277.005859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  55870296.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15860.185546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4918202.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4918202.0    \n",
      "module.adapter.frcn_linear.weight  dot:  554143232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  283266.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3642199.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4258.52197265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6313571.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24766592.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  270114.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38.00209045410156    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  6.034476280212402    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  78.41185760498047    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  299250.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  270114.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  418.14111328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.80606079101562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  637.6578369140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41115888.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  83028272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452405.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  256/6933] Loss: -514.6350 [iq: 18.6012,ans: 19.6955,interp: 10.1645,fusion: -563.0962]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1668.038818359375    \n",
      "module.ans_embedding.weight  dot:  378418.34375    \n",
      "module.lstm.weight_ih_l0  dot:  249965.0625    \n",
      "module.lstm.weight_hh_l0  dot:  15193.025390625    \n",
      "module.lstm.bias_ih_l0  dot:  16370.31640625    \n",
      "module.lstm.bias_hh_l0  dot:  16370.31640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46345408.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8507.39453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4419849.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4419849.0    \n",
      "module.adapter.frcn_linear.weight  dot:  354058816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  178756.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  842382.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  548.8397216796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  793935.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20640888.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  183088.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  54.114784240722656    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.142699241638184    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  114.96160888671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_lang.linear_merge.weight  dot:  217747.84375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  183088.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  121.68794250488281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.46698760986328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  231.33566284179688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36278268.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  80515408.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452406.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  257/6933] Loss: -515.6729 [iq: 17.5680,ans: 21.3176,interp: 13.0315,fusion: -567.5901]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1581.23974609375    \n",
      "module.ans_embedding.weight  dot:  400363.40625    \n",
      "module.lstm.weight_ih_l0  dot:  173169.75    \n",
      "module.lstm.weight_hh_l0  dot:  11430.376953125    \n",
      "module.lstm.bias_ih_l0  dot:  10351.822265625    \n",
      "module.lstm.bias_hh_l0  dot:  10351.822265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  52788152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11065.12890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4731166.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4731166.0    \n",
      "module.adapter.frcn_linear.weight  dot:  332420512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  151193.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2467514.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2919.350830078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4731916.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18540112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160193.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  61.548744201660156    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  11.688282012939453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  131.8160400390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  189015.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160193.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  563.1065673828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  106.70187377929688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1103.5162353515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  39102752.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  79068416.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452406.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  258/6933] Loss: -522.6302 [iq: 13.2902,ans: 18.6755,interp: 10.4655,fusion: -565.0614]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1545.5447998046875    \n",
      "module.ans_embedding.weight  dot:  580774.4375    \n",
      "module.lstm.weight_ih_l0  dot:  223755.875    \n",
      "module.lstm.weight_hh_l0  dot:  14078.9658203125    \n",
      "module.lstm.bias_ih_l0  dot:  15586.37109375    \n",
      "module.lstm.bias_hh_l0  dot:  15586.37109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  53556368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8361.818359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4675119.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4675119.0    \n",
      "module.adapter.frcn_linear.weight  dot:  412487936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  211823.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  832872.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  760.13037109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1062892.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  22276868.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  217737.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28.381145477294922    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  5.465673923492432    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  51.28411865234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  235826.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  217737.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  203.68296813964844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  52.80524826049805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  189.8592987060547    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  39454076.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  77209272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452407.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  259/6933] Loss: -515.0745 [iq: 14.1723,ans: 18.7464,interp: 12.8102,fusion: -560.8033]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1624.387451171875    \n",
      "module.ans_embedding.weight  dot:  397053.1875    \n",
      "module.lstm.weight_ih_l0  dot:  167068.859375    \n",
      "module.lstm.weight_hh_l0  dot:  10601.5576171875    \n",
      "module.lstm.bias_ih_l0  dot:  10818.41796875    \n",
      "module.lstm.bias_hh_l0  dot:  10818.41796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51357360.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19434.275390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4883459.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4883459.5    \n",
      "module.adapter.frcn_linear.weight  dot:  332076352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159384.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  779012.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  719.454345703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1234349.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17974654.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  161745.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  36.66681671142578    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  7.553485870361328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  70.7429428100586    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  181466.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  161745.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  377.4650573730469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  143.37057495117188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  423.1429138183594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.79805092826291e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  40244368.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  88001000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452408.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  260/6933] Loss: -508.3869 [iq: 17.1789,ans: 21.4752,interp: 16.1275,fusion: -563.1685]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1782.1275634765625    \n",
      "module.ans_embedding.weight  dot:  434622.875    \n",
      "module.lstm.weight_ih_l0  dot:  169577.21875    \n",
      "module.lstm.weight_hh_l0  dot:  11625.00390625    \n",
      "module.lstm.bias_ih_l0  dot:  10697.34375    \n",
      "module.lstm.bias_hh_l0  dot:  10697.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  55367028.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10919.31640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4854595.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4854595.0    \n",
      "module.adapter.frcn_linear.weight  dot:  303432896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  150784.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  809155.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  646.7786865234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1111644.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18905692.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  168966.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  47.316036224365234    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  11.535624504089355    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  104.03227233886719    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  194602.578125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  168966.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  686.9134521484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  176.44065856933594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  753.348876953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  41049688.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82404016.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452409.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  261/6933] Loss: -515.9246 [iq: 16.3503,ans: 19.1713,interp: 13.0852,fusion: -564.5313]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2096.109619140625    \n",
      "module.ans_embedding.weight  dot:  405625.375    \n",
      "module.lstm.weight_ih_l0  dot:  292814.4375    \n",
      "module.lstm.weight_hh_l0  dot:  17848.39453125    \n",
      "module.lstm.bias_ih_l0  dot:  19407.544921875    \n",
      "module.lstm.bias_hh_l0  dot:  19407.544921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  71742496.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31159.75    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6733314.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6733314.5    \n",
      "module.adapter.frcn_linear.weight  dot:  394394752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  188443.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3661286.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3803.474609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6264658.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20693056.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  216600.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  92.99560546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  20.06668472290039    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  223.6407928466797    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  254572.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  216600.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1675.08349609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  292.71002197265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2140.0185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  46339216.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  99542416.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452409.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  262/6933] Loss: -509.8131 [iq: 17.4292,ans: 20.3173,interp: 14.0326,fusion: -561.5923]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1631.4764404296875    \n",
      "module.ans_embedding.weight  dot:  337766.34375    \n",
      "module.lstm.weight_ih_l0  dot:  229536.609375    \n",
      "module.lstm.weight_hh_l0  dot:  14929.8779296875    \n",
      "module.lstm.bias_ih_l0  dot:  14661.201171875    \n",
      "module.lstm.bias_hh_l0  dot:  14661.201171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45632760.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2850.651123046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4331954.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4331954.5    \n",
      "module.adapter.frcn_linear.weight  dot:  356469504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  175773.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  457296.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  319.5149841308594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  472659.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19944140.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  187427.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46.2381706237793    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.311949729919434    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  113.78981018066406    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  220715.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  187427.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  245.9930419921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.613624572753906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  470.24725341796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37510672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  85750656.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452410.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  263/6933] Loss: -527.3441 [iq: 16.8859,ans: 21.2164,interp: 10.8701,fusion: -576.3166]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1814.10205078125    \n",
      "module.ans_embedding.weight  dot:  428212.71875    \n",
      "module.lstm.weight_ih_l0  dot:  268691.5625    \n",
      "module.lstm.weight_hh_l0  dot:  15993.8994140625    \n",
      "module.lstm.bias_ih_l0  dot:  18438.3203125    \n",
      "module.lstm.bias_hh_l0  dot:  18438.3203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  59521556.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22980.00390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5340807.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5340807.0    \n",
      "module.adapter.frcn_linear.weight  dot:  474706624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  233785.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3886575.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4710.728515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6824189.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23940156.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  256769.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  68.68641662597656    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  16.950092315673828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  155.68109130859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  281487.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  256769.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  148.4486083984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  36.470802307128906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  256.32000732421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  43085144.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84240312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452411.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  264/6933] Loss: -518.6147 [iq: 13.8466,ans: 19.4299,interp: 9.8157,fusion: -561.7070]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1953.978515625    \n",
      "module.ans_embedding.weight  dot:  428838.65625    \n",
      "module.lstm.weight_ih_l0  dot:  212021.109375    \n",
      "module.lstm.weight_hh_l0  dot:  13038.228515625    \n",
      "module.lstm.bias_ih_l0  dot:  14017.544921875    \n",
      "module.lstm.bias_hh_l0  dot:  14017.544921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49745960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3111.238525390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4191836.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4191836.75    \n",
      "module.adapter.frcn_linear.weight  dot:  380210496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  190506.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2293957.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2247.87158203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3687962.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  19439752.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  189984.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46.6546630859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.237266540527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  101.02429962158203    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.8689051962137455e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  217346.953125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  189984.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  32.36934280395508    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.262039184570312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  58.87656784057617    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38370512.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  75886224.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452411.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  265/6933] Loss: -521.8662 [iq: 14.3330,ans: 18.6602,interp: 11.6513,fusion: -566.5107]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2413.85009765625    \n",
      "module.ans_embedding.weight  dot:  367382.5625    \n",
      "module.lstm.weight_ih_l0  dot:  349844.25    \n",
      "module.lstm.weight_hh_l0  dot:  19207.642578125    \n",
      "module.lstm.bias_ih_l0  dot:  24934.98828125    \n",
      "module.lstm.bias_hh_l0  dot:  24934.98828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46608656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2707.25341796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4281129.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4281129.0    \n",
      "module.adapter.frcn_linear.weight  dot:  837102592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  449054.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3178555.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2785.56884765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2987070.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  30652944.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  388653.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  57.632118225097656    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  9.171932220458984    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  142.8380126953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  400415.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  388653.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  567.0478515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  94.19055938720703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  951.7406616210938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37861592.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  83632176.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452412.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  266/6933] Loss: -516.1927 [iq: 13.7232,ans: 19.2954,interp: 12.9183,fusion: -562.1296]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2152.4375    \n",
      "module.ans_embedding.weight  dot:  308506.75    \n",
      "module.lstm.weight_ih_l0  dot:  244475.9375    \n",
      "module.lstm.weight_hh_l0  dot:  16239.6875    \n",
      "module.lstm.bias_ih_l0  dot:  15434.3583984375    \n",
      "module.lstm.bias_hh_l0  dot:  15434.3583984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45182284.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10606.1640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4068892.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4068892.5    \n",
      "module.adapter.frcn_linear.weight  dot:  460989664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  232487.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1327616.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1624.485595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2044510.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22620712.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  235015.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  43.46955108642578    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  8.375975608825684    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  105.69947814941406    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.991829089500243e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  271618.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  235015.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.5208740234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  55.18303680419922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  641.611572265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38171132.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  84111016.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452413.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  267/6933] Loss: -527.2816 [iq: 14.6871,ans: 19.0718,interp: 14.3319,fusion: -575.3724]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1938.8890380859375    \n",
      "module.ans_embedding.weight  dot:  278911.375    \n",
      "module.lstm.weight_ih_l0  dot:  262688.84375    \n",
      "module.lstm.weight_hh_l0  dot:  16668.142578125    \n",
      "module.lstm.bias_ih_l0  dot:  17571.02734375    \n",
      "module.lstm.bias_hh_l0  dot:  17571.02734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48691792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31581.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4431806.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4431806.0    \n",
      "module.adapter.frcn_linear.weight  dot:  494926656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  250930.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2733040.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3159.9501953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4210235.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  25490592.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  258309.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  98.42922973632812    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  16.816482543945312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  264.078125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  297186.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  258309.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  174.26683044433594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.269039154052734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  305.7209777832031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.9878322038712213e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  39190664.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82299904.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452414.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  268/6933] Loss: -526.9009 [iq: 14.9745,ans: 19.2957,interp: 13.6422,fusion: -574.8132]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1910.296875    \n",
      "module.ans_embedding.weight  dot:  321274.9375    \n",
      "module.lstm.weight_ih_l0  dot:  228363.515625    \n",
      "module.lstm.weight_hh_l0  dot:  12185.1943359375    \n",
      "module.lstm.bias_ih_l0  dot:  15923.404296875    \n",
      "module.lstm.bias_hh_l0  dot:  15923.404296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35518864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6809.2236328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3763825.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3763825.0    \n",
      "module.adapter.frcn_linear.weight  dot:  430252800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  220895.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  867080.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  862.9249267578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1388949.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20247548.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  221116.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31.55112075805664    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  7.133857727050781    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74.93756103515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  232195.46875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  221116.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1145.201171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  243.63397216796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2650.701904296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  34054292.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  87158384.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452414.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  269/6933] Loss: -527.0491 [iq: 15.2533,ans: 22.5144,interp: 14.3829,fusion: -579.1998]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1832.2742919921875    \n",
      "module.ans_embedding.weight  dot:  335159.75    \n",
      "module.lstm.weight_ih_l0  dot:  300621.625    \n",
      "module.lstm.weight_hh_l0  dot:  14625.2529296875    \n",
      "module.lstm.bias_ih_l0  dot:  21894.873046875    \n",
      "module.lstm.bias_hh_l0  dot:  21894.873046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54665688.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14478.7314453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4558748.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4558748.0    \n",
      "module.adapter.frcn_linear.weight  dot:  678229376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  328957.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6248463.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  7513.2060546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11952110.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27531880.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  308942.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  32.051902770996094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  6.823158264160156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  71.75801086425781    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.8689051962137455e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  304130.09375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  308942.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  254.08624267578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.36674499511719    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  655.9949951171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  44656904.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93169344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452415.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  270/6933] Loss: -524.7928 [iq: 14.8593,ans: 18.7994,interp: 13.5184,fusion: -571.9699]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1647.896484375    \n",
      "module.ans_embedding.weight  dot:  336563.75    \n",
      "module.lstm.weight_ih_l0  dot:  215569.375    \n",
      "module.lstm.weight_hh_l0  dot:  11913.9375    \n",
      "module.lstm.bias_ih_l0  dot:  14566.837890625    \n",
      "module.lstm.bias_hh_l0  dot:  14566.837890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49208088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9927.505859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4340444.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4340444.0    \n",
      "module.adapter.frcn_linear.weight  dot:  445408960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  225338.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2060656.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2240.59716796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3690659.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21258708.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  218775.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53.2777214050293    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  10.201501846313477    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  114.43222045898438    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  237015.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  218775.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  695.703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  181.26393127441406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1544.5377197265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.991829089500243e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  42097508.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  93370624.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452416.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  271/6933] Loss: -529.3931 [iq: 13.3528,ans: 18.7366,interp: 10.0414,fusion: -571.5238]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2914.625    \n",
      "module.ans_embedding.weight  dot:  378526.375    \n",
      "module.lstm.weight_ih_l0  dot:  291495.625    \n",
      "module.lstm.weight_hh_l0  dot:  20959.87890625    \n",
      "module.lstm.bias_ih_l0  dot:  19509.03125    \n",
      "module.lstm.bias_hh_l0  dot:  19509.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  43038488.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5620.8564453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3705905.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3705905.5    \n",
      "module.adapter.frcn_linear.weight  dot:  610398208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  273422.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5698607.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  6667.69287109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  10405693.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  27286602.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  261531.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  103.10855102539062    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.598441123962402    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  272.2709655761719    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  293466.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  261531.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  136.15902709960938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.922122955322266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  218.64654541015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36494216.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73458736.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452416.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  272/6933] Loss: -533.3359 [iq: 10.8456,ans: 17.7090,interp: 8.9321,fusion: -570.8226]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3260.8095703125    \n",
      "module.ans_embedding.weight  dot:  298088.15625    \n",
      "module.lstm.weight_ih_l0  dot:  492477.5    \n",
      "module.lstm.weight_hh_l0  dot:  40211.3515625    \n",
      "module.lstm.bias_ih_l0  dot:  32499.498046875    \n",
      "module.lstm.bias_hh_l0  dot:  32499.498046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47591384.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23725.900390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4602391.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4602391.0    \n",
      "module.adapter.frcn_linear.weight  dot:  505008128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  250898.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4517470.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5630.7548828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7645537.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  27665792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  292592.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  128.22164916992188    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  18.610816955566406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  396.3066101074219    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  376853.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  292592.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  375.20184326171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  100.94436645507812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  848.6561279296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38643464.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86442864.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452417.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  273/6933] Loss: -524.3577 [iq: 15.9448,ans: 19.2042,interp: 13.2050,fusion: -572.7117]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2228.980712890625    \n",
      "module.ans_embedding.weight  dot:  314775.59375    \n",
      "module.lstm.weight_ih_l0  dot:  338184.9375    \n",
      "module.lstm.weight_hh_l0  dot:  20504.9453125    \n",
      "module.lstm.bias_ih_l0  dot:  24246.53515625    \n",
      "module.lstm.bias_hh_l0  dot:  24246.53515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39397292.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4474.97607421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3756319.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3756319.0    \n",
      "module.adapter.frcn_linear.weight  dot:  743064512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  364823.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3917980.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4195.2431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5931244.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  30285214.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  344124.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46.85843276977539    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  10.551239013671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  120.05787658691406    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  375506.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  344124.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  354.3525390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  124.71965026855469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  708.573486328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  35492624.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82889536.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452418.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  274/6933] Loss: -524.6211 [iq: 14.2532,ans: 19.3072,interp: 10.5357,fusion: -568.7172]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2910.767578125    \n",
      "module.ans_embedding.weight  dot:  343329.96875    \n",
      "module.lstm.weight_ih_l0  dot:  302350.8125    \n",
      "module.lstm.weight_hh_l0  dot:  17923.8984375    \n",
      "module.lstm.bias_ih_l0  dot:  19469.685546875    \n",
      "module.lstm.bias_hh_l0  dot:  19469.685546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44055848.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18103.38671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3650077.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3650077.25    \n",
      "module.adapter.frcn_linear.weight  dot:  458795584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  220956.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2280055.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2684.091796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3679903.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21414436.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  232603.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  103.03485107421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  22.808612823486328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  242.82955932617188    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  273027.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  232603.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  74.94039916992188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.155216217041016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  149.9429473876953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34883036.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  69323296.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452419.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  275/6933] Loss: -537.3971 [iq: 12.0915,ans: 16.7768,interp: 9.3240,fusion: -575.5894]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3103.34375    \n",
      "module.ans_embedding.weight  dot:  269898.5    \n",
      "module.lstm.weight_ih_l0  dot:  252721.4375    \n",
      "module.lstm.weight_hh_l0  dot:  18784.40625    \n",
      "module.lstm.bias_ih_l0  dot:  14997.103515625    \n",
      "module.lstm.bias_hh_l0  dot:  14997.103515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37070272.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21135.12109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3555469.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3555469.25    \n",
      "module.adapter.frcn_linear.weight  dot:  333737792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  179664.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  736013.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  645.2354736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  809096.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18113104.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  209463.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  89.45337677001953    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.63075065612793    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  263.5692138671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  256798.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  209463.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  424.0275573730469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  99.94013214111328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1095.398193359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  33855000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  79966336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452420.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  276/6933] Loss: -540.7128 [iq: 15.1470,ans: 18.6903,interp: 10.1774,fusion: -584.7274]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5269.6435546875    \n",
      "module.ans_embedding.weight  dot:  309417.4375    \n",
      "module.lstm.weight_ih_l0  dot:  461480.28125    \n",
      "module.lstm.weight_hh_l0  dot:  36660.2890625    \n",
      "module.lstm.bias_ih_l0  dot:  28527.58984375    \n",
      "module.lstm.bias_hh_l0  dot:  28527.58984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49779036.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16522.212890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4258648.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4258648.5    \n",
      "module.adapter.frcn_linear.weight  dot:  585286976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  323484.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  652715.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  534.9830932617188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  578251.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  25238408.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  328672.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  203.3653106689453    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  46.25038528442383    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  593.9679565429688    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  421741.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  328672.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  301.13739013671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  66.0425796508789    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  768.470947265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37758384.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73952472.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452420.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  277/6933] Loss: -526.2632 [iq: 15.4362,ans: 16.8804,interp: 13.2295,fusion: -571.8093]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2122.042236328125    \n",
      "module.ans_embedding.weight  dot:  226456.828125    \n",
      "module.lstm.weight_ih_l0  dot:  295864.15625    \n",
      "module.lstm.weight_hh_l0  dot:  19134.423828125    \n",
      "module.lstm.bias_ih_l0  dot:  20761.82421875    \n",
      "module.lstm.bias_hh_l0  dot:  20761.82421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32514070.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12885.76953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3179717.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3179717.0    \n",
      "module.adapter.frcn_linear.weight  dot:  520072672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  264121.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  929504.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  603.9268798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  749123.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23435676.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  270812.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  50.73002243041992    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  9.739748001098633    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  145.5392303466797    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.667022075935165e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  298776.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  270812.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  203.02418518066406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.63286590576172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  491.4470520019531    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  32014456.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  82762448.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452421.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  278/6933] Loss: -535.5613 [iq: 14.2682,ans: 21.1129,interp: 13.4448,fusion: -584.3872]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3106.064453125    \n",
      "module.ans_embedding.weight  dot:  276722.375    \n",
      "module.lstm.weight_ih_l0  dot:  265039.15625    \n",
      "module.lstm.weight_hh_l0  dot:  17302.365234375    \n",
      "module.lstm.bias_ih_l0  dot:  16768.859375    \n",
      "module.lstm.bias_hh_l0  dot:  16768.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29008316.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8887.14453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2885472.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2885472.0    \n",
      "module.adapter.frcn_linear.weight  dot:  334630912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  177886.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  918651.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1144.7720947265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1419098.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16873802.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  195284.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  76.34947204589844    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  14.852795600891113    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  212.99661254882812    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  236730.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  195284.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  304.5802001953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  72.19329833984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  606.32275390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28481622.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  66949992.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452422.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  279/6933] Loss: -548.2023 [iq: 12.6178,ans: 19.0986,interp: 9.9863,fusion: -589.9052]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4195.703125    \n",
      "module.ans_embedding.weight  dot:  253185.1875    \n",
      "module.lstm.weight_ih_l0  dot:  305254.4375    \n",
      "module.lstm.weight_hh_l0  dot:  31617.44921875    \n",
      "module.lstm.bias_ih_l0  dot:  16600.2421875    \n",
      "module.lstm.bias_hh_l0  dot:  16600.2421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32344294.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12202.83203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2901649.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2901649.0    \n",
      "module.adapter.frcn_linear.weight  dot:  375263488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  199162.203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  679363.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  524.53515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  704669.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19459336.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211485.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  167.0511474609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  30.78241539001465    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  512.1790771484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  290372.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211485.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  271.0662841796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  53.666507720947266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  754.9278564453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.007016857736744e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  32107416.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  69154096.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452423.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  280/6933] Loss: -554.5791 [iq: 12.8673,ans: 17.8532,interp: 10.2753,fusion: -595.5748]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3754.03076171875    \n",
      "module.ans_embedding.weight  dot:  303208.375    \n",
      "module.lstm.weight_ih_l0  dot:  326525.1875    \n",
      "module.lstm.weight_hh_l0  dot:  26157.1640625    \n",
      "module.lstm.bias_ih_l0  dot:  19123.396484375    \n",
      "module.lstm.bias_hh_l0  dot:  19123.396484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51160872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10407.9072265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4112492.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4112492.0    \n",
      "module.adapter.frcn_linear.weight  dot:  385991616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  207696.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1045348.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1115.2119140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1594531.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18243730.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  227055.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  209.14474487304688    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.70465850830078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  613.8963012695312    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  298434.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  227055.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  124.2728271484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.937910079956055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  264.52923583984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36188696.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  70585200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452423.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  281/6933] Loss: -536.5661 [iq: 15.5411,ans: 17.1939,interp: 11.8209,fusion: -581.1221]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4403.48779296875    \n",
      "module.ans_embedding.weight  dot:  434712.34375    \n",
      "module.lstm.weight_ih_l0  dot:  454360.59375    \n",
      "module.lstm.weight_hh_l0  dot:  23577.525390625    \n",
      "module.lstm.bias_ih_l0  dot:  32385.619140625    \n",
      "module.lstm.bias_hh_l0  dot:  32385.619140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39523124.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8480.80859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3833986.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3833986.5    \n",
      "module.adapter.frcn_linear.weight  dot:  347409152.0    \n",
      "module.adapter.frcn_linear.bias  dot:  193706.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  757198.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  980.6289672851562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1233790.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  19123564.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  247943.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  49.211769104003906    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  9.485328674316406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  115.16996765136719    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.594813170413545e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  268299.90625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  247943.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  151.85775756835938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.19435119628906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  270.4432678222656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.1391778065881226e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29340308.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  65483828.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452424.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  282/6933] Loss: -538.8192 [iq: 14.3600,ans: 17.8641,interp: 10.1112,fusion: -581.1545]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3718.70166015625    \n",
      "module.ans_embedding.weight  dot:  211055.84375    \n",
      "module.lstm.weight_ih_l0  dot:  329354.8125    \n",
      "module.lstm.weight_hh_l0  dot:  30140.7734375    \n",
      "module.lstm.bias_ih_l0  dot:  20455.169921875    \n",
      "module.lstm.bias_hh_l0  dot:  20455.169921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39654056.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26093.322265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3612968.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3612968.5    \n",
      "module.adapter.frcn_linear.weight  dot:  524468480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  281455.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1021568.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1110.9732666015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1396382.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23173210.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  282893.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  85.82823181152344    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  11.251859664916992    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  237.8331298828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  347059.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  282893.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1161.618408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  200.78488159179688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3145.021484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  38323296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  89902368.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452425.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  283/6933] Loss: -536.8102 [iq: 17.6612,ans: 18.7424,interp: 13.0408,fusion: -586.2546]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4686.4150390625    \n",
      "module.ans_embedding.weight  dot:  228266.875    \n",
      "module.lstm.weight_ih_l0  dot:  298959.1875    \n",
      "module.lstm.weight_hh_l0  dot:  26975.6796875    \n",
      "module.lstm.bias_ih_l0  dot:  16852.5234375    \n",
      "module.lstm.bias_hh_l0  dot:  16852.5234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27202948.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1182.9993896484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2441579.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2441579.5    \n",
      "module.adapter.frcn_linear.weight  dot:  465968800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  238976.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1317743.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1099.2135009765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1588304.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20359456.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  237769.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  200.8455810546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  31.70153045654297    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  651.919677734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.035971308738226e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  314272.90625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  237769.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2.5475082397460938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.7689928412437439    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2.9276480674743652    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  26986000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  63162296.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452426.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  284/6933] Loss: -545.3545 [iq: 14.0063,ans: 18.3004,interp: 11.6518,fusion: -589.3130]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3859.612548828125    \n",
      "module.ans_embedding.weight  dot:  203824.484375    \n",
      "module.lstm.weight_ih_l0  dot:  258298.75    \n",
      "module.lstm.weight_hh_l0  dot:  27225.603515625    \n",
      "module.lstm.bias_ih_l0  dot:  14235.537109375    \n",
      "module.lstm.bias_hh_l0  dot:  14235.537109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28074920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5167.23046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2611327.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2611327.5    \n",
      "module.adapter.frcn_linear.weight  dot:  332204416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  175773.421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1499667.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1267.78857421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1612348.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16251312.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  195742.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  143.64434814453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  24.11104393005371    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  475.6478271484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  255573.328125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  195742.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  14.454263687133789    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.073000907897949    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  17.90843963623047    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28648294.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  69174344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452426.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  285/6933] Loss: -549.8463 [iq: 14.9953,ans: 18.1638,interp: 11.6874,fusion: -594.6927]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3326.5009765625    \n",
      "module.ans_embedding.weight  dot:  220637.796875    \n",
      "module.lstm.weight_ih_l0  dot:  275789.71875    \n",
      "module.lstm.weight_hh_l0  dot:  18818.66796875    \n",
      "module.lstm.bias_ih_l0  dot:  17378.294921875    \n",
      "module.lstm.bias_hh_l0  dot:  17378.294921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34325680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15675.4052734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3120760.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3120760.5    \n",
      "module.adapter.frcn_linear.weight  dot:  470321056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  235711.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2930385.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3486.936767578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5301465.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20769404.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241813.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  71.78559112548828    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  17.700042724609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  239.3687744140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  293481.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241813.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  152.93946838378906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.26332473754883    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  276.6337585449219    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4784973245696165e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  34492360.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  81237280.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452428.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  286/6933] Loss: -545.4818 [iq: 14.3670,ans: 17.6632,interp: 12.9560,fusion: -590.4680]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4576.046875    \n",
      "module.ans_embedding.weight  dot:  211621.40625    \n",
      "module.lstm.weight_ih_l0  dot:  309792.6875    \n",
      "module.lstm.weight_hh_l0  dot:  27572.041015625    \n",
      "module.lstm.bias_ih_l0  dot:  16936.8046875    \n",
      "module.lstm.bias_hh_l0  dot:  16936.8046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31461814.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10953.3173828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2937720.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2937720.0    \n",
      "module.adapter.frcn_linear.weight  dot:  407595872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  219239.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1689468.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1794.44970703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2487043.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16330218.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211341.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  126.14076232910156    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  21.496387481689453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  464.8135986328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.645884008728899e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  292259.65625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211341.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  554.17431640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  148.27500915527344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  900.2611083984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31282674.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  73441840.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452428.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  287/6933] Loss: -549.4780 [iq: 14.2026,ans: 17.4800,interp: 11.0437,fusion: -592.2043]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4472.94189453125    \n",
      "module.ans_embedding.weight  dot:  433801.3125    \n",
      "module.lstm.weight_ih_l0  dot:  635425.9375    \n",
      "module.lstm.weight_hh_l0  dot:  29263.341796875    \n",
      "module.lstm.bias_ih_l0  dot:  52462.20703125    \n",
      "module.lstm.bias_hh_l0  dot:  52462.20703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  52785088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13019.37890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4633101.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4633101.0    \n",
      "module.adapter.frcn_linear.weight  dot:  745227392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  336996.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  11845554.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  14729.80078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  23061832.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  27098528.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  356748.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  45.67348098754883    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  11.099851608276367    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74.55392456054688    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  358013.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  356748.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  162.52024841308594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.11668014526367    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  295.56524658203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  36907776.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  78576360.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452429.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  288/6933] Loss: -536.9990 [iq: 12.4186,ans: 16.7200,interp: 9.4619,fusion: -575.5995]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5995.779296875    \n",
      "module.ans_embedding.weight  dot:  242393.8125    \n",
      "module.lstm.weight_ih_l0  dot:  408677.625    \n",
      "module.lstm.weight_hh_l0  dot:  30068.546875    \n",
      "module.lstm.bias_ih_l0  dot:  24324.20703125    \n",
      "module.lstm.bias_hh_l0  dot:  24324.20703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30070412.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8415.05859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2649232.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2649232.5    \n",
      "module.adapter.frcn_linear.weight  dot:  590241536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  289650.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4484096.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5416.29150390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8210383.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  23502094.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  290253.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  213.29629516601562    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  46.38352966308594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  703.2860107421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  394365.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  290253.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  250.33837890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.66081237792969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  638.5363159179688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29939356.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  63729424.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452430.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  289/6933] Loss: -552.9138 [iq: 10.2648,ans: 16.2251,interp: 8.0590,fusion: -587.4626]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6026.07763671875    \n",
      "module.ans_embedding.weight  dot:  267898.15625    \n",
      "module.lstm.weight_ih_l0  dot:  357410.0625    \n",
      "module.lstm.weight_hh_l0  dot:  35849.25    \n",
      "module.lstm.bias_ih_l0  dot:  19039.888671875    \n",
      "module.lstm.bias_hh_l0  dot:  19039.888671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30591882.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9972.2841796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2654746.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2654746.5    \n",
      "module.adapter.frcn_linear.weight  dot:  404924928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  232081.203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  416016.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  296.6990966796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  354782.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  17570004.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  237102.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  203.5755157470703    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  29.574321746826172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  767.405517578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  315253.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  237102.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  302.00128173828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  59.0098991394043    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  870.141357421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27347804.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58121532.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452431.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  290/6933] Loss: -549.8713 [iq: 12.5933,ans: 15.9015,interp: 11.4796,fusion: -589.8457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5773.451171875    \n",
      "module.ans_embedding.weight  dot:  203841.6875    \n",
      "module.lstm.weight_ih_l0  dot:  396290.75    \n",
      "module.lstm.weight_hh_l0  dot:  49311.234375    \n",
      "module.lstm.bias_ih_l0  dot:  23219.76171875    \n",
      "module.lstm.bias_hh_l0  dot:  23219.76171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33935904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18731.87890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3272334.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3272334.75    \n",
      "module.adapter.frcn_linear.weight  dot:  385283776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  211841.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  510275.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  616.3486328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  732890.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17905486.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  222544.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  269.4861755371094    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.17506408691406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1060.53515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  337226.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  222544.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  161.81224060058594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.208656311035156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  363.1501159667969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  29097360.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  65497648.0    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452431.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  291/6933] Loss: -564.2881 [iq: 13.3087,ans: 17.5261,interp: 9.6702,fusion: -604.7930]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7142.66796875    \n",
      "module.ans_embedding.weight  dot:  199108.46875    \n",
      "module.lstm.weight_ih_l0  dot:  376097.0    \n",
      "module.lstm.weight_hh_l0  dot:  38391.0703125    \n",
      "module.lstm.bias_ih_l0  dot:  18821.890625    \n",
      "module.lstm.bias_hh_l0  dot:  18821.890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26984568.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19634.29296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2403770.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2403770.25    \n",
      "module.adapter.frcn_linear.weight  dot:  363561600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  185695.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2306625.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3042.6630859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3920835.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  17806762.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  194695.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  320.7066345214844    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.75114440917969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1173.451904296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  324309.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  194695.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  287.0059814453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.179744720458984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  909.6304931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28267182.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60009276.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452432.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  292/6933] Loss: -568.1734 [iq: 10.4728,ans: 15.8100,interp: 11.3076,fusion: -605.7638]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6063.21240234375    \n",
      "module.ans_embedding.weight  dot:  488274.65625    \n",
      "module.lstm.weight_ih_l0  dot:  449794.25    \n",
      "module.lstm.weight_hh_l0  dot:  33221.0859375    \n",
      "module.lstm.bias_ih_l0  dot:  31520.07421875    \n",
      "module.lstm.bias_hh_l0  dot:  31520.07421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42977768.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10189.7783203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3762073.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3762073.5    \n",
      "module.adapter.frcn_linear.weight  dot:  465095296.0    \n",
      "module.adapter.frcn_linear.bias  dot:  238158.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2497900.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2826.62939453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3764868.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20524382.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  260490.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  160.2657470703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.151391983032227    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  635.6190185546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  317266.71875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  260490.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  97.37744140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.150409698486328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  169.1387939453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  30249960.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59474116.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452433.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  293/6933] Loss: -545.0500 [iq: 10.6832,ans: 15.6133,interp: 9.0292,fusion: -580.3757]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4083.10498046875    \n",
      "module.ans_embedding.weight  dot:  200375.8125    \n",
      "module.lstm.weight_ih_l0  dot:  312315.375    \n",
      "module.lstm.weight_hh_l0  dot:  26241.390625    \n",
      "module.lstm.bias_ih_l0  dot:  20971.814453125    \n",
      "module.lstm.bias_hh_l0  dot:  20971.814453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32566018.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9347.7529296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3220557.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3220557.0    \n",
      "module.adapter.frcn_linear.weight  dot:  569520384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  292538.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  871813.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  945.8391723632812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1139305.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22371664.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  295344.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  151.0476531982422    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  20.623899459838867    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  539.667724609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  358565.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  295344.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  239.264892578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  68.03153991699219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  346.040283203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  30016736.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  77708000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452434.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  294/6933] Loss: -552.8057 [iq: 11.7688,ans: 16.9741,interp: 9.7011,fusion: -591.2496]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5613.91748046875    \n",
      "module.ans_embedding.weight  dot:  261440.46875    \n",
      "module.lstm.weight_ih_l0  dot:  396367.25    \n",
      "module.lstm.weight_hh_l0  dot:  34410.8515625    \n",
      "module.lstm.bias_ih_l0  dot:  27214.83984375    \n",
      "module.lstm.bias_hh_l0  dot:  27214.83984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42392464.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9209.2109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3680004.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3680004.75    \n",
      "module.adapter.frcn_linear.weight  dot:  556736000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  307097.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1401781.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1547.653076171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1843467.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22449090.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  313366.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  144.10047912597656    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  31.25023651123047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  491.5757751464844    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  394358.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  313366.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  560.7133178710938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  154.02517700195312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1154.732177734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  37591924.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  86253760.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452435.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  295/6933] Loss: -547.0823 [iq: 14.1167,ans: 16.9490,interp: 12.4428,fusion: -590.5908]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6690.8203125    \n",
      "module.ans_embedding.weight  dot:  271835.875    \n",
      "module.lstm.weight_ih_l0  dot:  419059.65625    \n",
      "module.lstm.weight_hh_l0  dot:  34296.0390625    \n",
      "module.lstm.bias_ih_l0  dot:  27719.671875    \n",
      "module.lstm.bias_hh_l0  dot:  27719.671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28340208.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18045.037109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2575779.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2575779.0    \n",
      "module.adapter.frcn_linear.weight  dot:  366952704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  197507.203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2188932.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2504.05029296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3135940.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  16517876.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  224325.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  261.8702087402344    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.02519989013672    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  810.0157470703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  289930.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  224325.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  858.5660400390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  149.6912078857422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2464.081787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25535172.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  54842724.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452436.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  296/6933] Loss: -554.2571 [iq: 10.8864,ans: 15.9585,interp: 12.0267,fusion: -593.1285]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5723.068359375    \n",
      "module.ans_embedding.weight  dot:  204533.484375    \n",
      "module.lstm.weight_ih_l0  dot:  424587.5625    \n",
      "module.lstm.weight_hh_l0  dot:  38753.21875    \n",
      "module.lstm.bias_ih_l0  dot:  26979.39453125    \n",
      "module.lstm.bias_hh_l0  dot:  26979.39453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26270872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4296.2197265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2542099.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2542099.5    \n",
      "module.adapter.frcn_linear.weight  dot:  687226688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  390790.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1149213.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  819.6947021484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  960227.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24919334.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  364452.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  100.90492248535156    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.492144584655762    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  402.99285888671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  428522.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  364452.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  167.59115600585938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.064151763916016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  303.75091552734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  28963992.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  74351504.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452436.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  297/6933] Loss: -551.1005 [iq: 14.4189,ans: 17.8292,interp: 12.3012,fusion: -595.6497]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  12645.533203125    \n",
      "module.ans_embedding.weight  dot:  345117.65625    \n",
      "module.lstm.weight_ih_l0  dot:  699382.25    \n",
      "module.lstm.weight_hh_l0  dot:  71655.453125    \n",
      "module.lstm.bias_ih_l0  dot:  41909.7578125    \n",
      "module.lstm.bias_hh_l0  dot:  41909.7578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33706904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1524.4676513671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2855625.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2855625.25    \n",
      "module.adapter.frcn_linear.weight  dot:  380721600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  206956.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  707774.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  474.2176513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  565936.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18075572.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  246459.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  354.88018798828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  61.88029479980469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1434.947998046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  406320.53125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  246459.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  70.89559173583984    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.90152359008789    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  87.36050415039062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26168620.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53535464.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452437.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  298/6933] Loss: -560.0886 [iq: 11.6415,ans: 14.9626,interp: 10.3745,fusion: -597.0671]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7760.81982421875    \n",
      "module.ans_embedding.weight  dot:  163561.3125    \n",
      "module.lstm.weight_ih_l0  dot:  472456.5625    \n",
      "module.lstm.weight_hh_l0  dot:  55169.921875    \n",
      "module.lstm.bias_ih_l0  dot:  27879.45703125    \n",
      "module.lstm.bias_hh_l0  dot:  27879.45703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26356030.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13274.189453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2417224.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2417224.25    \n",
      "module.adapter.frcn_linear.weight  dot:  567099072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  327453.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1153674.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1142.2327880859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1282519.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19290240.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  317524.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  392.60321044921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  58.97132110595703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1626.214111328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  444101.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  317524.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  89.01133728027344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.853065490722656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  180.1573486328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3219647598816664e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27834090.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  62416856.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452438.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  299/6933] Loss: -555.8971 [iq: 13.3928,ans: 16.1574,interp: 10.5990,fusion: -596.0463]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7391.68115234375    \n",
      "module.ans_embedding.weight  dot:  268722.875    \n",
      "module.lstm.weight_ih_l0  dot:  402043.3125    \n",
      "module.lstm.weight_hh_l0  dot:  37981.90625    \n",
      "module.lstm.bias_ih_l0  dot:  22565.1953125    \n",
      "module.lstm.bias_hh_l0  dot:  22565.1953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27331658.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13883.33203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2595082.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2595082.75    \n",
      "module.adapter.frcn_linear.weight  dot:  375297536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  203163.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2428300.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3110.71044921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3661159.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18033778.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  234442.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  134.68960571289062    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  18.134418487548828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  539.1357421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  308810.375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  234442.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  347.4779052734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.98377990722656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  620.2501220703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26972256.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59988968.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452439.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  300/6933] Loss: -556.2764 [iq: 11.4006,ans: 15.6888,interp: 9.5679,fusion: -592.9337]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8556.95703125    \n",
      "module.ans_embedding.weight  dot:  180107.296875    \n",
      "module.lstm.weight_ih_l0  dot:  484696.03125    \n",
      "module.lstm.weight_hh_l0  dot:  70563.859375    \n",
      "module.lstm.bias_ih_l0  dot:  23352.76171875    \n",
      "module.lstm.bias_hh_l0  dot:  23352.76171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27680152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20871.568359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2605525.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2605525.5    \n",
      "module.adapter.frcn_linear.weight  dot:  379529056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  208130.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  604415.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  351.11822509765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  474070.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18834590.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  231114.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  321.00048828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  37.76449966430664    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1497.917236328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.176800878965878e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  364498.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  231114.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  598.7307739257812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  141.37742614746094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1791.842041015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.780531526193954e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27331636.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  60019848.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452440.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  301/6933] Loss: -569.5599 [iq: 12.1523,ans: 15.9622,interp: 9.0916,fusion: -606.7661]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11261.6005859375    \n",
      "module.ans_embedding.weight  dot:  185349.203125    \n",
      "module.lstm.weight_ih_l0  dot:  609507.0625    \n",
      "module.lstm.weight_hh_l0  dot:  78479.7421875    \n",
      "module.lstm.bias_ih_l0  dot:  27368.82421875    \n",
      "module.lstm.bias_hh_l0  dot:  27368.82421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26315356.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17268.17578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2455971.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2455971.0    \n",
      "module.adapter.frcn_linear.weight  dot:  349445760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  194386.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  577903.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  603.2484130859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  754824.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  17409220.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  237168.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  430.1128845214844    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.8207778930664    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2016.6046142578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  448753.59375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  237168.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  542.7098388671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  117.17170715332031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1052.1900634765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26283132.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  63100024.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452440.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  302/6933] Loss: -570.4852 [iq: 10.2907,ans: 15.4941,interp: 8.1543,fusion: -604.4243]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11021.38671875    \n",
      "module.ans_embedding.weight  dot:  185353.015625    \n",
      "module.lstm.weight_ih_l0  dot:  511233.21875    \n",
      "module.lstm.weight_hh_l0  dot:  69995.609375    \n",
      "module.lstm.bias_ih_l0  dot:  25315.01171875    \n",
      "module.lstm.bias_hh_l0  dot:  25315.01171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31125576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5094.2470703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2882399.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2882399.5    \n",
      "module.adapter.frcn_linear.weight  dot:  426061504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  233459.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2524245.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3440.811767578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3935607.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20537570.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  282350.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  597.3845825195312    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.43954086303711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2224.787353515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  493723.15625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  282350.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  98.17800903320312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.56987762451172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  136.74502563476562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.007016857736744e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28284280.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  67844240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452441.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  303/6933] Loss: -561.7501 [iq: 15.2753,ans: 16.9225,interp: 11.3145,fusion: -605.2624]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  9339.7021484375    \n",
      "module.ans_embedding.weight  dot:  194201.53125    \n",
      "module.lstm.weight_ih_l0  dot:  429432.4375    \n",
      "module.lstm.weight_hh_l0  dot:  63297.3359375    \n",
      "module.lstm.bias_ih_l0  dot:  19679.015625    \n",
      "module.lstm.bias_hh_l0  dot:  19679.015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24331308.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4184.14404296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2331374.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2331374.0    \n",
      "module.adapter.frcn_linear.weight  dot:  465075808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  244722.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1973771.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2457.935546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2768068.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19697556.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  252378.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  320.66363525390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  42.770591735839844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1491.22314453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  398482.90625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  252378.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  134.36746215820312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.618812561035156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  322.55413818359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25662108.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61764968.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452442.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  304/6933] Loss: -564.7085 [iq: 11.8797,ans: 15.6600,interp: 8.8416,fusion: -601.0898]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14379.6025390625    \n",
      "module.ans_embedding.weight  dot:  162045.40625    \n",
      "module.lstm.weight_ih_l0  dot:  617741.5    \n",
      "module.lstm.weight_hh_l0  dot:  91957.265625    \n",
      "module.lstm.bias_ih_l0  dot:  28089.8046875    \n",
      "module.lstm.bias_hh_l0  dot:  28089.8046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22276504.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4234.599609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1974870.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1974870.75    \n",
      "module.adapter.frcn_linear.weight  dot:  574365056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  322087.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  958519.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  674.1438598632812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  910171.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21403184.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  334880.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  815.0150146484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  98.724853515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3913.190185546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  554631.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  334880.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  306.6282958984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  95.1306381225586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  673.2705078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  23377644.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53490764.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452443.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  305/6933] Loss: -560.3998 [iq: 13.1217,ans: 15.2095,interp: 9.3329,fusion: -598.0639]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  12659.5859375    \n",
      "module.ans_embedding.weight  dot:  164417.5625    \n",
      "module.lstm.weight_ih_l0  dot:  522596.78125    \n",
      "module.lstm.weight_hh_l0  dot:  91486.546875    \n",
      "module.lstm.bias_ih_l0  dot:  21556.017578125    \n",
      "module.lstm.bias_hh_l0  dot:  21556.017578125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26740088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2484.222900390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2228501.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2228501.25    \n",
      "module.adapter.frcn_linear.weight  dot:  367633088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  210138.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  743764.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  592.8409423828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  762649.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15664160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  233890.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  699.0758056640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  68.77923583984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3028.10791015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  450825.96875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  233890.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.756528377532959    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.008725643157959    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11.316650390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.6275870368408505e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25793384.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61759680.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452444.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  306/6933] Loss: -565.7700 [iq: 15.1729,ans: 16.5882,interp: 12.7861,fusion: -610.3171]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8100.73779296875    \n",
      "module.ans_embedding.weight  dot:  175573.4375    \n",
      "module.lstm.weight_ih_l0  dot:  431718.78125    \n",
      "module.lstm.weight_hh_l0  dot:  43095.3828125    \n",
      "module.lstm.bias_ih_l0  dot:  25144.95703125    \n",
      "module.lstm.bias_hh_l0  dot:  25144.95703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29534740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18611.095703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2442649.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2442649.5    \n",
      "module.adapter.frcn_linear.weight  dot:  518564544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  288569.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1117960.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1010.5012817382812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1237522.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19454740.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  309967.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  291.77288818359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.01805114746094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1120.425048828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1951328815484885e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  415058.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  309967.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  252.68344116210938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  55.04277038574219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  639.39404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.02936581270319e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27732434.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  58125216.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452445.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  307/6933] Loss: -563.0848 [iq: 12.0601,ans: 14.9351,interp: 11.0722,fusion: -601.1522]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8471.0478515625    \n",
      "module.ans_embedding.weight  dot:  170066.40625    \n",
      "module.lstm.weight_ih_l0  dot:  353508.0625    \n",
      "module.lstm.weight_hh_l0  dot:  41282.9375    \n",
      "module.lstm.bias_ih_l0  dot:  19989.05859375    \n",
      "module.lstm.bias_hh_l0  dot:  19989.05859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23866454.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28239.8828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2400014.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2400014.75    \n",
      "module.adapter.frcn_linear.weight  dot:  462683520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  246460.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5665359.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  6404.12060546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8986176.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18440552.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  270141.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  194.05953979492188    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  21.402240753173828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  761.3936767578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  374990.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  270141.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  583.7039794921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  188.2488555908203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1948.361572265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26056254.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  61631128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452446.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  308/6933] Loss: -567.9941 [iq: 11.3818,ans: 14.8957,interp: 9.3017,fusion: -603.5733]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  18131.068359375    \n",
      "module.ans_embedding.weight  dot:  172050.75    \n",
      "module.lstm.weight_ih_l0  dot:  755201.1875    \n",
      "module.lstm.weight_hh_l0  dot:  179874.140625    \n",
      "module.lstm.bias_ih_l0  dot:  31409.73828125    \n",
      "module.lstm.bias_hh_l0  dot:  31409.73828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31079744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5545.87060546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2454894.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2454894.75    \n",
      "module.adapter.frcn_linear.weight  dot:  345257472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  194747.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  621897.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  413.7507629394531    \n",
      "module.attflat_img.mlp.linear.weight  dot:  494160.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17710666.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  233165.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  838.2654418945312    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  92.55636596679688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3883.3701171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  522289.09375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  233165.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  79.74876403808594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.46042251586914    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  211.98001098632812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26836956.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56001752.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452447.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  309/6933] Loss: -574.4891 [iq: 14.2781,ans: 15.1896,interp: 11.4756,fusion: -615.4324]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14035.1083984375    \n",
      "module.ans_embedding.weight  dot:  131192.875    \n",
      "module.lstm.weight_ih_l0  dot:  454320.0    \n",
      "module.lstm.weight_hh_l0  dot:  82998.8828125    \n",
      "module.lstm.bias_ih_l0  dot:  18128.6875    \n",
      "module.lstm.bias_hh_l0  dot:  18128.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20436088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14251.2197265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1880789.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1880789.25    \n",
      "module.adapter.frcn_linear.weight  dot:  297874752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  163343.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  921246.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  947.6966552734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1203211.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15223170.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  191665.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  255.3125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  55.60973358154297    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1133.3011474609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  394700.78125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  191665.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  123.5364761352539    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.602622985839844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  238.3700408935547    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21734626.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  51110404.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452447.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  310/6933] Loss: -567.5811 [iq: 13.5271,ans: 16.1758,interp: 11.7542,fusion: -609.0381]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11346.9970703125    \n",
      "module.ans_embedding.weight  dot:  408972.40625    \n",
      "module.lstm.weight_ih_l0  dot:  1009487.25    \n",
      "module.lstm.weight_hh_l0  dot:  47627.37890625    \n",
      "module.lstm.bias_ih_l0  dot:  74271.6953125    \n",
      "module.lstm.bias_hh_l0  dot:  74271.6953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45324680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9721.5732421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4209971.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4209971.5    \n",
      "module.adapter.frcn_linear.weight  dot:  849781248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  416223.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6998709.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  8985.404296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  11947775.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  28089020.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  423731.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  143.1440887451172    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.2335147857666    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  475.7355651855469    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  479537.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  423731.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  20.966510772705078    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.144731521606445    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  34.79567337036133    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  31341060.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  68834368.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452448.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  311/6933] Loss: -552.7241 [iq: 11.1462,ans: 14.7050,interp: 9.3419,fusion: -587.9172]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14740.7548828125    \n",
      "module.ans_embedding.weight  dot:  258075.59375    \n",
      "module.lstm.weight_ih_l0  dot:  624224.625    \n",
      "module.lstm.weight_hh_l0  dot:  75080.953125    \n",
      "module.lstm.bias_ih_l0  dot:  32239.703125    \n",
      "module.lstm.bias_hh_l0  dot:  32239.703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29979188.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  0.0    NOT UPDATING\n",
      "module.ans_lstm.bias_ih_l0  dot:  2893567.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2893567.5    \n",
      "module.adapter.frcn_linear.weight  dot:  394768928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  231092.015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  652949.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  536.672607421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  559176.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18534994.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  289734.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  544.4947509765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  92.56094360351562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2432.884765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  424955.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  289734.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  22883582.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52165188.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  21  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452449.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  312/6933] Loss: -570.2033 [iq: 9.5100,ans: 14.2959,interp: 7.4151,fusion: -601.4243]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11691.7939453125    \n",
      "module.ans_embedding.weight  dot:  176484.125    \n",
      "module.lstm.weight_ih_l0  dot:  663497.5    \n",
      "module.lstm.weight_hh_l0  dot:  64463.359375    \n",
      "module.lstm.bias_ih_l0  dot:  40400.27734375    \n",
      "module.lstm.bias_hh_l0  dot:  40400.27734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21606388.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10451.306640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1924310.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1924310.125    \n",
      "module.adapter.frcn_linear.weight  dot:  466770752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  261305.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2299392.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2122.0263671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2914758.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17515220.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  273178.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  383.86767578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  57.359947204589844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1739.513916015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  408520.3125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  273178.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  106.50328063964844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.407377243041992    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  219.15228271484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  23652106.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52935200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452450.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  313/6933] Loss: -575.4661 [iq: 11.9865,ans: 14.5753,interp: 11.0485,fusion: -613.0764]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  17052.240234375    \n",
      "module.ans_embedding.weight  dot:  148091.65625    \n",
      "module.lstm.weight_ih_l0  dot:  588188.5    \n",
      "module.lstm.weight_hh_l0  dot:  99520.265625    \n",
      "module.lstm.bias_ih_l0  dot:  27565.73046875    \n",
      "module.lstm.bias_hh_l0  dot:  27565.73046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25224828.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1059.9886474609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2341528.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2341528.0    \n",
      "module.adapter.frcn_linear.weight  dot:  435922208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  230288.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2370970.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3209.6806640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3785283.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17838072.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  257172.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  919.312744140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  123.70513153076172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4921.9072265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  488081.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  257172.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  163.16571044921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  59.51580810546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  349.4251708984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25605798.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  64296420.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452451.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  314/6933] Loss: -571.0801 [iq: 13.0118,ans: 15.5159,interp: 10.3659,fusion: -609.9737]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  12340.646484375    \n",
      "module.ans_embedding.weight  dot:  138889.1875    \n",
      "module.lstm.weight_ih_l0  dot:  455783.375    \n",
      "module.lstm.weight_hh_l0  dot:  77038.25    \n",
      "module.lstm.bias_ih_l0  dot:  24987.583984375    \n",
      "module.lstm.bias_hh_l0  dot:  24987.583984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23052532.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2489.91552734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1998812.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1998812.75    \n",
      "module.adapter.frcn_linear.weight  dot:  510970432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  306871.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  721946.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  548.6158447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  662633.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19487116.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  304563.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  340.561279296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  38.51390075683594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1147.840576171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  450466.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  304563.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6.563132286071777    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.1626520156860352    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5.867566108703613    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26852314.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  66333288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452452.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  315/6933] Loss: -570.4171 [iq: 13.8866,ans: 16.5394,interp: 11.8564,fusion: -612.6995]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  17881.650390625    \n",
      "module.ans_embedding.weight  dot:  287038.28125    \n",
      "module.lstm.weight_ih_l0  dot:  753158.375    \n",
      "module.lstm.weight_hh_l0  dot:  108524.5234375    \n",
      "module.lstm.bias_ih_l0  dot:  43105.58203125    \n",
      "module.lstm.bias_hh_l0  dot:  43105.58203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34162176.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11445.197265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3192961.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3192961.75    \n",
      "module.adapter.frcn_linear.weight  dot:  646626816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  361106.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5087123.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  6051.02783203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7696760.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23405772.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  392535.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  384.087890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  70.64871215820312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1428.267578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  555996.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  392535.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  63.805423736572266    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.639131546020508    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  177.845947265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27001452.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59238912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452453.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  316/6933] Loss: -561.4420 [iq: 11.1562,ans: 14.1249,interp: 11.0583,fusion: -597.7814]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  22146.26171875    \n",
      "module.ans_embedding.weight  dot:  131510.609375    \n",
      "module.lstm.weight_ih_l0  dot:  812299.625    \n",
      "module.lstm.weight_hh_l0  dot:  127140.75    \n",
      "module.lstm.bias_ih_l0  dot:  41143.71875    \n",
      "module.lstm.bias_hh_l0  dot:  41143.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20293454.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11651.5703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1864631.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1864631.75    \n",
      "module.adapter.frcn_linear.weight  dot:  649855232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  375986.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4140099.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5047.736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5914200.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23612548.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  395618.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  872.637451171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  153.64584350585938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4138.8203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  672128.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  395618.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  308.76409912109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  65.65107727050781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  685.6597900390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21674684.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  46720608.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452454.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  317/6933] Loss: -575.7062 [iq: 11.3072,ans: 14.5256,interp: 9.9597,fusion: -611.4988]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  17113.744140625    \n",
      "module.ans_embedding.weight  dot:  134722.78125    \n",
      "module.lstm.weight_ih_l0  dot:  522640.5625    \n",
      "module.lstm.weight_hh_l0  dot:  95873.5625    \n",
      "module.lstm.bias_ih_l0  dot:  23692.0390625    \n",
      "module.lstm.bias_hh_l0  dot:  23692.0390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26048724.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16189.388671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2120836.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2120836.0    \n",
      "module.adapter.frcn_linear.weight  dot:  498197312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  275507.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1022290.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  865.8636474609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1020025.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18933350.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  268648.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  499.59088134765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  71.67296600341797    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2256.190673828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  457627.03125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  268648.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1861.3623046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  373.3851623535156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3794.43359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  26952716.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  59096240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452455.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  318/6933] Loss: -574.7954 [iq: 13.1130,ans: 15.0131,interp: 10.4591,fusion: -613.3806]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  17514.662109375    \n",
      "module.ans_embedding.weight  dot:  151357.890625    \n",
      "module.lstm.weight_ih_l0  dot:  509652.875    \n",
      "module.lstm.weight_hh_l0  dot:  85660.6953125    \n",
      "module.lstm.bias_ih_l0  dot:  28064.21484375    \n",
      "module.lstm.bias_hh_l0  dot:  28064.21484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22887718.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6820.6826171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2015900.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2015900.25    \n",
      "module.adapter.frcn_linear.weight  dot:  403638176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  227821.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1631641.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1521.8751220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2243304.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17172346.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  252886.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  346.7684326171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  53.441925048828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1654.15380859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  432260.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  252886.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  553.9411010742188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  163.28192138671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2110.46826171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27234114.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  64474464.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452455.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  319/6933] Loss: -574.7988 [iq: 11.5595,ans: 14.3743,interp: 9.6197,fusion: -610.3522]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  20778.18359375    \n",
      "module.ans_embedding.weight  dot:  186615.40625    \n",
      "module.lstm.weight_ih_l0  dot:  1313314.625    \n",
      "module.lstm.weight_hh_l0  dot:  243178.578125    \n",
      "module.lstm.bias_ih_l0  dot:  71810.9921875    \n",
      "module.lstm.bias_hh_l0  dot:  71810.9921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32624734.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12835.869140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3163728.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3163728.0    \n",
      "module.adapter.frcn_linear.weight  dot:  390161184.0    \n",
      "module.adapter.frcn_linear.bias  dot:  217139.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1776172.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2002.064697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2402739.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17006616.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  244927.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  930.6898193359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  96.84780883789062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4881.982421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.46265255252365e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  574959.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  244927.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1984.516845703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  375.03729248046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4037.803466796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  27028318.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  64629112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452456.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  320/6933] Loss: -584.2379 [iq: 10.7256,ans: 14.8007,interp: 10.1383,fusion: -619.9024]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  19190.611328125    \n",
      "module.ans_embedding.weight  dot:  114289.390625    \n",
      "module.lstm.weight_ih_l0  dot:  590091.4375    \n",
      "module.lstm.weight_hh_l0  dot:  89170.53125    \n",
      "module.lstm.bias_ih_l0  dot:  28579.90234375    \n",
      "module.lstm.bias_hh_l0  dot:  28579.90234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16907278.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7746.427734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1337723.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1337723.625    \n",
      "module.adapter.frcn_linear.weight  dot:  473916480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  269140.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  831923.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  532.3802490234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  670691.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17503168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  275747.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  634.5527954101562    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  113.00454711914062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2919.52880859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4784973245696165e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  497253.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  275747.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  107.10125732421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.687755584716797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  298.78143310546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.341815961263819e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20888824.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  44696720.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452457.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  321/6933] Loss: -576.9917 [iq: 13.2851,ans: 15.2749,interp: 12.9250,fusion: -618.4766]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  24994.01953125    \n",
      "module.ans_embedding.weight  dot:  110056.8671875    \n",
      "module.lstm.weight_ih_l0  dot:  726181.0    \n",
      "module.lstm.weight_hh_l0  dot:  142555.546875    \n",
      "module.lstm.bias_ih_l0  dot:  32582.490234375    \n",
      "module.lstm.bias_hh_l0  dot:  32582.490234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15759598.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12315.88671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1364293.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1364293.0    \n",
      "module.adapter.frcn_linear.weight  dot:  696351680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  409918.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2229895.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2735.93798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3076317.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21977540.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  386818.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  626.1038818359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  83.60209655761719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3251.5048828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  697738.4375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  386818.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  256.14849853515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  52.64565658569336    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  620.7132568359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1951328815484885e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19497484.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  41776584.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452458.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  322/6933] Loss: -582.0160 [iq: 11.3682,ans: 13.8643,interp: 8.5919,fusion: -615.8405]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  18466.244140625    \n",
      "module.ans_embedding.weight  dot:  147564.828125    \n",
      "module.lstm.weight_ih_l0  dot:  932808.9375    \n",
      "module.lstm.weight_hh_l0  dot:  56278.0546875    \n",
      "module.lstm.bias_ih_l0  dot:  59795.5546875    \n",
      "module.lstm.bias_hh_l0  dot:  59795.5546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17839036.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4255.291015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1445011.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1445011.375    \n",
      "module.adapter.frcn_linear.weight  dot:  556545280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  296304.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1549043.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1714.6556396484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2259095.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  20000972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  314577.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  394.8818359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  56.73949432373047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1048.9678955078125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  540436.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  314577.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  177.4170379638672    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.60685729980469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  397.86016845703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  21530078.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49167432.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452459.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  323/6933] Loss: -580.3611 [iq: 12.6512,ans: 14.7369,interp: 10.2150,fusion: -617.9643]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  24973.38671875    \n",
      "module.ans_embedding.weight  dot:  174924.09375    \n",
      "module.lstm.weight_ih_l0  dot:  757076.1875    \n",
      "module.lstm.weight_hh_l0  dot:  140619.015625    \n",
      "module.lstm.bias_ih_l0  dot:  38206.91796875    \n",
      "module.lstm.bias_hh_l0  dot:  38206.91796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20905660.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11514.712890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2026674.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2026674.375    \n",
      "module.adapter.frcn_linear.weight  dot:  395686720.0    \n",
      "module.adapter.frcn_linear.bias  dot:  223033.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1117242.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1190.68603515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1477317.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17908632.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  261126.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  605.6739501953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.716094970703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3246.60546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  464707.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  261126.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  954.4198608398438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  189.56338500976562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3009.98583984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  23142866.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  51792248.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452460.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  324/6933] Loss: -585.9407 [iq: 11.2367,ans: 13.8947,interp: 10.5223,fusion: -621.5945]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  30985.54296875    \n",
      "module.ans_embedding.weight  dot:  143478.125    \n",
      "module.lstm.weight_ih_l0  dot:  999697.75    \n",
      "module.lstm.weight_hh_l0  dot:  279856.375    \n",
      "module.lstm.bias_ih_l0  dot:  38969.78515625    \n",
      "module.lstm.bias_hh_l0  dot:  38969.78515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18161768.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2799.82958984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1771710.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1771710.375    \n",
      "module.adapter.frcn_linear.weight  dot:  381147968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  219852.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2088527.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2120.899658203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2939800.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15491444.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  249428.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1988.0379638671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  180.7105712890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  10698.107421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  641042.9375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  249428.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  162.63787841796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  48.264766693115234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  428.513671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19964164.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49519652.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452460.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  325/6933] Loss: -591.9234 [iq: 12.9518,ans: 14.7263,interp: 11.3727,fusion: -630.9742]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  29425.11328125    \n",
      "module.ans_embedding.weight  dot:  158473.15625    \n",
      "module.lstm.weight_ih_l0  dot:  897177.6875    \n",
      "module.lstm.weight_hh_l0  dot:  186496.53125    \n",
      "module.lstm.bias_ih_l0  dot:  49756.45703125    \n",
      "module.lstm.bias_hh_l0  dot:  49756.45703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27398424.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14385.26953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2296513.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2296513.75    \n",
      "module.adapter.frcn_linear.weight  dot:  427267456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  245796.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2165136.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2912.31591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3438673.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18781724.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  282557.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1257.696533203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  137.7507781982422    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5866.06640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  621515.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  282557.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1058.743408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  183.03953552246094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2747.400634765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25739804.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52881128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452461.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  326/6933] Loss: -580.1147 [iq: 15.0847,ans: 14.5554,interp: 10.9909,fusion: -620.7457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  21978.7578125    \n",
      "module.ans_embedding.weight  dot:  204066.296875    \n",
      "module.lstm.weight_ih_l0  dot:  692647.875    \n",
      "module.lstm.weight_hh_l0  dot:  86179.234375    \n",
      "module.lstm.bias_ih_l0  dot:  42359.6328125    \n",
      "module.lstm.bias_hh_l0  dot:  42359.6328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30636808.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17905.45703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2753849.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2753849.5    \n",
      "module.adapter.frcn_linear.weight  dot:  486346112.0    \n",
      "module.adapter.frcn_linear.bias  dot:  251710.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5689932.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  7604.14111328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  9881149.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21070624.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  305449.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  678.7899169921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  75.3216552734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3386.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  480022.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  305449.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  863.6217041015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  187.93521118164062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3208.48779296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  27384180.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56217736.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452463.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  327/6933] Loss: -582.6220 [iq: 11.2147,ans: 13.5886,interp: 8.8543,fusion: -616.2796]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  30858.24609375    \n",
      "module.ans_embedding.weight  dot:  182137.53125    \n",
      "module.lstm.weight_ih_l0  dot:  830848.9375    \n",
      "module.lstm.weight_hh_l0  dot:  228231.84375    \n",
      "module.lstm.bias_ih_l0  dot:  39530.6171875    \n",
      "module.lstm.bias_hh_l0  dot:  39530.6171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26274440.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10541.4541015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2591607.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2591607.5    \n",
      "module.adapter.frcn_linear.weight  dot:  504255744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  283563.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2378658.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2663.537841796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3198791.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20553134.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  329756.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1412.0382080078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  96.08268737792969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7402.2470703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  702144.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  329756.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  198.26727294921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.235313415527344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  321.7408752441406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  22842200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  53043092.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452463.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  328/6933] Loss: -579.2075 [iq: 12.3432,ans: 13.2982,interp: 9.9717,fusion: -614.8206]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  23975.28125    \n",
      "module.ans_embedding.weight  dot:  281792.5    \n",
      "module.lstm.weight_ih_l0  dot:  936371.125    \n",
      "module.lstm.weight_hh_l0  dot:  122482.84375    \n",
      "module.lstm.bias_ih_l0  dot:  60566.9296875    \n",
      "module.lstm.bias_hh_l0  dot:  60566.9296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27250792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8350.6806640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2578537.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2578537.5    \n",
      "module.adapter.frcn_linear.weight  dot:  235110912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  124408.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1133710.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1346.1512451171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1812221.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  12999921.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  176947.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  607.2188720703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  70.21443176269531    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2897.561767578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  349076.5625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  176947.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1810.3966064453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  409.7272033691406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3955.62060546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20514912.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  44345568.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452464.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  329/6933] Loss: -576.8917 [iq: 13.3046,ans: 13.3419,interp: 9.7716,fusion: -613.3098]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  33068.53125    \n",
      "module.ans_embedding.weight  dot:  339958.0    \n",
      "module.lstm.weight_ih_l0  dot:  830599.0625    \n",
      "module.lstm.weight_hh_l0  dot:  173911.40625    \n",
      "module.lstm.bias_ih_l0  dot:  45588.7265625    \n",
      "module.lstm.bias_hh_l0  dot:  45588.7265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22854868.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7496.16357421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2035298.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2035298.625    \n",
      "module.adapter.frcn_linear.weight  dot:  256246816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  135079.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  517261.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  442.91510009765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  496783.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16692167.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  187947.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  417.59912109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  72.26322937011719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1626.1982421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  393064.4375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  187947.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  410.7584228515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  84.02364349365234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1139.103759765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18446788.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  35335640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452465.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  330/6933] Loss: -578.7985 [iq: 9.9792,ans: 11.6956,interp: 8.3890,fusion: -608.8622]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  36932.52734375    \n",
      "module.ans_embedding.weight  dot:  123813.5625    \n",
      "module.lstm.weight_ih_l0  dot:  997813.6875    \n",
      "module.lstm.weight_hh_l0  dot:  419134.1875    \n",
      "module.lstm.bias_ih_l0  dot:  35661.5234375    \n",
      "module.lstm.bias_hh_l0  dot:  35661.5234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18800388.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9776.103515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1683698.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1683698.375    \n",
      "module.adapter.frcn_linear.weight  dot:  323617888.0    \n",
      "module.adapter.frcn_linear.bias  dot:  178669.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1363471.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1315.78369140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1708303.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.022684490540996e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15267694.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  207229.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2521.72216796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  165.04446411132812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12576.642578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  650262.4375    \n",
      "module.attflat_lang.linear_merge.bias  dot:  207229.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.06341552734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  63.459014892578125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  760.2952880859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20590876.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  48417792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452466.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  331/6933] Loss: -588.4113 [iq: 12.4297,ans: 12.9871,interp: 10.4464,fusion: -624.2745]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  39912.19140625    \n",
      "module.ans_embedding.weight  dot:  149189.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1124409.25    \n",
      "module.lstm.weight_hh_l0  dot:  282196.375    \n",
      "module.lstm.bias_ih_l0  dot:  53503.33984375    \n",
      "module.lstm.bias_hh_l0  dot:  53503.33984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22173940.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6287.85791015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1912740.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1912740.5    \n",
      "module.adapter.frcn_linear.weight  dot:  478636096.0    \n",
      "module.adapter.frcn_linear.bias  dot:  273200.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  795878.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  469.5238952636719    \n",
      "module.attflat_img.mlp.linear.weight  dot:  647543.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18744626.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  297076.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2196.62841796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  154.34765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11001.939453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  706722.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  297076.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  424.3753662109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  86.23236846923828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1165.343994140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22410336.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49504056.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452467.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  332/6933] Loss: -588.9599 [iq: 11.7604,ans: 12.7037,interp: 11.5312,fusion: -624.9551]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  32773.69140625    \n",
      "module.ans_embedding.weight  dot:  199214.53125    \n",
      "module.lstm.weight_ih_l0  dot:  679693.3125    \n",
      "module.lstm.weight_hh_l0  dot:  200691.015625    \n",
      "module.lstm.bias_ih_l0  dot:  27510.66015625    \n",
      "module.lstm.bias_hh_l0  dot:  27510.66015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21346760.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12895.3740234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1847064.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1847064.125    \n",
      "module.adapter.frcn_linear.weight  dot:  369354016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  209793.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  837061.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  650.1405029296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  780814.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16826906.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  237002.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1174.35107421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  111.66539001464844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  5720.4560546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  485120.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  237002.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  239.33921813964844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.290340423583984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  685.71142578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20478008.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42131956.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452468.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  333/6933] Loss: -580.7576 [iq: 13.1076,ans: 12.5635,interp: 11.5062,fusion: -617.9350]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  66936.9140625    \n",
      "module.ans_embedding.weight  dot:  166345.828125    \n",
      "module.lstm.weight_ih_l0  dot:  2496945.0    \n",
      "module.lstm.weight_hh_l0  dot:  732292.125    \n",
      "module.lstm.bias_ih_l0  dot:  125933.234375    \n",
      "module.lstm.bias_hh_l0  dot:  125933.234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26779962.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11592.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2416365.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2416365.5    \n",
      "module.adapter.frcn_linear.weight  dot:  653282688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  376673.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1512043.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1633.1456298828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1643454.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21500628.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  365682.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3732.306640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  243.40643310546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  21443.76171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1207259.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  365682.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  331.361083984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  74.67160034179688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  452.39532470703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20780484.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  45795640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452469.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  334/6933] Loss: -591.2253 [iq: 12.7083,ans: 13.5350,interp: 9.8348,fusion: -627.3033]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  38657.90625    \n",
      "module.ans_embedding.weight  dot:  132418.3125    \n",
      "module.lstm.weight_ih_l0  dot:  966178.0    \n",
      "module.lstm.weight_hh_l0  dot:  281170.0    \n",
      "module.lstm.bias_ih_l0  dot:  35165.95703125    \n",
      "module.lstm.bias_hh_l0  dot:  35165.95703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26949350.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8257.111328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2271399.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2271399.75    \n",
      "module.adapter.frcn_linear.weight  dot:  542771840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  327358.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  739599.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  556.5503540039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  686639.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19888728.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  345205.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2243.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  148.84176635742188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  11882.017578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  795008.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  345205.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  114.9424057006836    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.105562210083008    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  222.589599609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  23663074.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52189960.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452470.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  335/6933] Loss: -592.8856 [iq: 13.2389,ans: 12.8775,interp: 9.1942,fusion: -628.1962]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  40566.07421875    \n",
      "module.ans_embedding.weight  dot:  153804.65625    \n",
      "module.lstm.weight_ih_l0  dot:  956701.4375    \n",
      "module.lstm.weight_hh_l0  dot:  301960.375    \n",
      "module.lstm.bias_ih_l0  dot:  38417.3984375    \n",
      "module.lstm.bias_hh_l0  dot:  38417.3984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23257460.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5672.8154296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2071943.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2071943.875    \n",
      "module.adapter.frcn_linear.weight  dot:  416318656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  239043.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  533079.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  371.6200866699219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  396675.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17190062.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  270630.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1411.4805908203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  150.9221954345703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8269.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4190391084412113e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  682486.875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  270630.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  67.58341979980469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.875640869140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  191.19644165039062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19817726.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  41656288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452471.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  336/6933] Loss: -590.7073 [iq: 11.0671,ans: 12.7528,interp: 8.0870,fusion: -622.6141]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  73442.234375    \n",
      "module.ans_embedding.weight  dot:  106234.8515625    \n",
      "module.lstm.weight_ih_l0  dot:  1660887.375    \n",
      "module.lstm.weight_hh_l0  dot:  705761.5625    \n",
      "module.lstm.bias_ih_l0  dot:  49865.40625    \n",
      "module.lstm.bias_hh_l0  dot:  49865.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15907030.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10621.642578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1218461.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1218461.75    \n",
      "module.adapter.frcn_linear.weight  dot:  422306048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  227819.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  720824.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  575.8148193359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  796217.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19916880.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  258242.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4689.123046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  303.900634765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  27186.03515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1068828.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  258242.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  306.49334716796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.665809631347656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  933.4082641601562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19980300.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  38069312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452472.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  337/6933] Loss: -606.9948 [iq: 9.9658,ans: 12.0507,interp: 8.0979,fusion: -637.1092]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  42050.64453125    \n",
      "module.ans_embedding.weight  dot:  106669.796875    \n",
      "module.lstm.weight_ih_l0  dot:  964288.625    \n",
      "module.lstm.weight_hh_l0  dot:  325699.25    \n",
      "module.lstm.bias_ih_l0  dot:  36361.46484375    \n",
      "module.lstm.bias_hh_l0  dot:  36361.46484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21353256.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8456.43359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1735258.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1735258.75    \n",
      "module.adapter.frcn_linear.weight  dot:  581907968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  329382.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1768129.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1806.7127685546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1914392.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23269376.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  365586.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2861.4404296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  181.9575653076172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  15715.6572265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  958589.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  365586.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  162.7889404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.020130157470703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  400.0139465332031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  24701100.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  56210256.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452473.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  338/6933] Loss: -586.4147 [iq: 11.8189,ans: 13.3434,interp: 10.6218,fusion: -622.1989]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  51962.42578125    \n",
      "module.ans_embedding.weight  dot:  177406.6875    \n",
      "module.lstm.weight_ih_l0  dot:  1240695.5    \n",
      "module.lstm.weight_hh_l0  dot:  226907.375    \n",
      "module.lstm.bias_ih_l0  dot:  61648.6640625    \n",
      "module.lstm.bias_hh_l0  dot:  61648.6640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22833920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6241.765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2099109.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2099109.5    \n",
      "module.adapter.frcn_linear.weight  dot:  421897984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  244050.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2197678.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2745.7900390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3209346.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18153250.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  312788.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2109.87109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  202.08628845214844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12074.6513671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  791432.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  312788.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  89.53189086914062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.633174896240234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  291.3067321777344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19439364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  43576552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452473.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  339/6933] Loss: -591.0844 [iq: 11.1468,ans: 12.6301,interp: 9.8479,fusion: -624.7092]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  54893.17578125    \n",
      "module.ans_embedding.weight  dot:  150059.875    \n",
      "module.lstm.weight_ih_l0  dot:  1746224.25    \n",
      "module.lstm.weight_hh_l0  dot:  510656.28125    \n",
      "module.lstm.bias_ih_l0  dot:  69162.484375    \n",
      "module.lstm.bias_hh_l0  dot:  69162.484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24704990.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21115.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2362445.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2362445.5    \n",
      "module.adapter.frcn_linear.weight  dot:  381258176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  216373.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  942393.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  856.4758911132812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1028829.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17079034.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  266467.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7367.25439453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  321.14056396484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  41681.2421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1107281.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  266467.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  151.1805419921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  22.64883804321289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  277.89581298828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21868346.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  51072288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452474.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  340/6933] Loss: -599.7247 [iq: 13.3276,ans: 13.8432,interp: 11.1607,fusion: -638.0562]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  72212.953125    \n",
      "module.ans_embedding.weight  dot:  153344.921875    \n",
      "module.lstm.weight_ih_l0  dot:  1652805.0    \n",
      "module.lstm.weight_hh_l0  dot:  297777.625    \n",
      "module.lstm.bias_ih_l0  dot:  81854.6484375    \n",
      "module.lstm.bias_hh_l0  dot:  81854.6484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21682406.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2725.035400390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2033159.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2033159.0    \n",
      "module.adapter.frcn_linear.weight  dot:  500891392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  281074.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2469734.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2517.5654296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3187226.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20124594.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  322492.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1589.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  137.29464721679688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  8057.0068359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5967316358000971e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  780845.8125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  322492.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  41.71776580810547    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.239904403686523    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  65.44227600097656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  19378212.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  45711884.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452476.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  341/6933] Loss: -584.1693 [iq: 11.7044,ans: 12.6563,interp: 11.4069,fusion: -619.9368]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  60057.4609375    \n",
      "module.ans_embedding.weight  dot:  272313.8125    \n",
      "module.lstm.weight_ih_l0  dot:  1314111.5    \n",
      "module.lstm.weight_hh_l0  dot:  386946.6875    \n",
      "module.lstm.bias_ih_l0  dot:  55479.03515625    \n",
      "module.lstm.bias_hh_l0  dot:  55479.03515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35261704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15223.39453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3176713.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3176713.5    \n",
      "module.adapter.frcn_linear.weight  dot:  608160384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  334616.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5933629.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  6309.5849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  8159217.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  21465436.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  409827.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3542.448486328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  283.67218017578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  20295.787109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1025106.1875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  409827.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  259.37017822265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.910045623779297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  901.3795166015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22555558.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  46382628.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452476.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  342/6933] Loss: -575.4822 [iq: 12.0061,ans: 13.2799,interp: 10.3278,fusion: -611.0961]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  43246.203125    \n",
      "module.ans_embedding.weight  dot:  269874.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1778654.75    \n",
      "module.lstm.weight_hh_l0  dot:  207259.78125    \n",
      "module.lstm.bias_ih_l0  dot:  120677.5    \n",
      "module.lstm.bias_hh_l0  dot:  120677.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28206122.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5660.8623046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2776740.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2776740.0    \n",
      "module.adapter.frcn_linear.weight  dot:  361663136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  211089.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1126364.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1143.0762939453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1426430.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  16943536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  270708.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  519.1952514648438    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  59.631507873535156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  430.57244873046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  530504.0625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  270708.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  177.7635955810547    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.53022766113281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  505.9140319824219    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.589928271845565e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20819282.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  48228172.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452477.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  343/6933] Loss: -584.7982 [iq: 12.3929,ans: 12.9732,interp: 11.2891,fusion: -621.4534]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  69013.6953125    \n",
      "module.ans_embedding.weight  dot:  206408.171875    \n",
      "module.lstm.weight_ih_l0  dot:  1182298.375    \n",
      "module.lstm.weight_hh_l0  dot:  252753.90625    \n",
      "module.lstm.bias_ih_l0  dot:  45290.96875    \n",
      "module.lstm.bias_hh_l0  dot:  45290.96875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31774432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21299.99609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2861777.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2861777.25    \n",
      "module.adapter.frcn_linear.weight  dot:  564238592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  336722.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1337273.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1002.6561889648438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1257419.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20524598.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  359705.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1042.4737548828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  155.06143188476562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3767.98486328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  861591.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  359705.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  169.07855224609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.12815856933594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  191.47946166992188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5475620784854982e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  24134104.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  47662328.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452478.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  344/6933] Loss: -589.5369 [iq: 10.8498,ans: 12.5549,interp: 10.0618,fusion: -623.0033]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  97985.5234375    \n",
      "module.ans_embedding.weight  dot:  213696.59375    \n",
      "module.lstm.weight_ih_l0  dot:  2413440.0    \n",
      "module.lstm.weight_hh_l0  dot:  547606.25    \n",
      "module.lstm.bias_ih_l0  dot:  126254.84375    \n",
      "module.lstm.bias_hh_l0  dot:  126254.84375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25438952.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17443.25390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2508785.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2508785.5    \n",
      "module.adapter.frcn_linear.weight  dot:  667004672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  393413.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4943768.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  6016.927734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7756283.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23046722.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  459226.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4107.759765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  278.4927978515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  23108.19921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1358508.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  459226.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  152.01162719726562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.47190856933594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  542.4329833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22659972.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49821632.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452479.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  345/6933] Loss: -593.5197 [iq: 10.6186,ans: 12.2181,interp: 9.4681,fusion: -625.8244]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  67485.375    \n",
      "module.ans_embedding.weight  dot:  222779.890625    \n",
      "module.lstm.weight_ih_l0  dot:  1430249.125    \n",
      "module.lstm.weight_hh_l0  dot:  517907.5625    \n",
      "module.lstm.bias_ih_l0  dot:  61019.22265625    \n",
      "module.lstm.bias_hh_l0  dot:  61019.22265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26730864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8617.4755859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2504002.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2504002.25    \n",
      "module.adapter.frcn_linear.weight  dot:  513442880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  297260.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2649028.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3063.245849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3678091.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19400116.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  358465.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4436.876953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  211.84136962890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  24396.623046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  858037.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  358465.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  270.3486328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.84099197387695    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  525.0236206054688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17593608.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  35490440.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452480.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  346/6933] Loss: -585.5622 [iq: 10.7398,ans: 11.8348,interp: 9.0102,fusion: -617.1470]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  120043.8828125    \n",
      "module.ans_embedding.weight  dot:  190035.390625    \n",
      "module.lstm.weight_ih_l0  dot:  2054399.75    \n",
      "module.lstm.weight_hh_l0  dot:  848602.9375    \n",
      "module.lstm.bias_ih_l0  dot:  56474.21875    \n",
      "module.lstm.bias_hh_l0  dot:  56474.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22954380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7965.06787109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2236578.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2236578.0    \n",
      "module.adapter.frcn_linear.weight  dot:  440197952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  249234.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4267908.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5114.71826171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6372043.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19085718.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  316447.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7710.83056640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  526.7860107421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  44245.1171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3648104868480004e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1385486.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  316447.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  385.6082763671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.10169982910156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  715.5538330078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20056006.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  39666040.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452481.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  347/6933] Loss: -598.3936 [iq: 9.2398,ans: 12.2021,interp: 8.8571,fusion: -628.6926]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  97414.46875    \n",
      "module.ans_embedding.weight  dot:  103531.1484375    \n",
      "module.lstm.weight_ih_l0  dot:  2897438.25    \n",
      "module.lstm.weight_hh_l0  dot:  1155570.875    \n",
      "module.lstm.bias_ih_l0  dot:  114683.546875    \n",
      "module.lstm.bias_hh_l0  dot:  114683.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17558642.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13662.654296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1537088.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1537088.125    \n",
      "module.adapter.frcn_linear.weight  dot:  615301376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  374050.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1414211.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1127.42041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1485586.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20888508.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  380676.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13584.8955078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  478.54901123046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  77140.6015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1422611.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  380676.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  409.757080078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  102.49041748046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1153.8525390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20412440.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50825944.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452482.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  348/6933] Loss: -602.7287 [iq: 10.9997,ans: 12.0961,interp: 8.8648,fusion: -634.6893]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  93301.4296875    \n",
      "module.ans_embedding.weight  dot:  120241.8984375    \n",
      "module.lstm.weight_ih_l0  dot:  1483454.25    \n",
      "module.lstm.weight_hh_l0  dot:  756799.125    \n",
      "module.lstm.bias_ih_l0  dot:  54061.56640625    \n",
      "module.lstm.bias_hh_l0  dot:  54061.56640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17774212.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5324.2626953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1531697.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1531697.125    \n",
      "module.adapter.frcn_linear.weight  dot:  392997792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  238858.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  629701.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  418.4706726074219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  540733.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16965368.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  285159.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6163.060546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  253.72157287597656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  34235.5546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1010321.625    \n",
      "module.attflat_lang.linear_merge.bias  dot:  285159.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  68.27525329589844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.699527740478516    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  180.46466064453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.331290964250002e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18991048.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42619352.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452483.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  349/6933] Loss: -615.5948 [iq: 10.5745,ans: 11.7165,interp: 8.1626,fusion: -646.0484]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  113434.3125    \n",
      "module.ans_embedding.weight  dot:  167149.84375    \n",
      "module.lstm.weight_ih_l0  dot:  1826291.5    \n",
      "module.lstm.weight_hh_l0  dot:  925385.125    \n",
      "module.lstm.bias_ih_l0  dot:  57295.078125    \n",
      "module.lstm.bias_hh_l0  dot:  57295.078125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23986280.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13883.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2005219.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2005219.625    \n",
      "module.adapter.frcn_linear.weight  dot:  517408000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  317390.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1574442.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1024.9130859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1540935.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19246680.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  340998.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5100.654296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  200.69203186035156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  30604.28515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1160814.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  340998.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  126.66795349121094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  34.388248443603516    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  217.24671936035156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20613590.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42611000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452484.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  350/6933] Loss: -604.5298 [iq: 13.0486,ans: 12.2601,interp: 10.5879,fusion: -640.4265]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  76619.28125    \n",
      "module.ans_embedding.weight  dot:  208980.546875    \n",
      "module.lstm.weight_ih_l0  dot:  2280075.0    \n",
      "module.lstm.weight_hh_l0  dot:  297789.875    \n",
      "module.lstm.bias_ih_l0  dot:  109119.28125    \n",
      "module.lstm.bias_hh_l0  dot:  109119.28125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28042668.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15554.30078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2247370.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2247370.75    \n",
      "module.adapter.frcn_linear.weight  dot:  370225792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  207024.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  731823.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  496.89398193359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  607876.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18643300.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  273333.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2053.46484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  286.5699462890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  7648.1171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  817389.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  273333.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  183.18003845214844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.24043655395508    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  457.41766357421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22078208.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42792280.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452485.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  351/6933] Loss: -600.9882 [iq: 12.1526,ans: 11.8993,interp: 9.7436,fusion: -634.7836]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  125415.703125    \n",
      "module.ans_embedding.weight  dot:  242671.953125    \n",
      "module.lstm.weight_ih_l0  dot:  3267573.0    \n",
      "module.lstm.weight_hh_l0  dot:  1708696.75    \n",
      "module.lstm.bias_ih_l0  dot:  141968.171875    \n",
      "module.lstm.bias_hh_l0  dot:  141968.171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39493600.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19940.5546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3587566.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3587566.5    \n",
      "module.adapter.frcn_linear.weight  dot:  517288416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  297039.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2311514.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2276.924560546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3179590.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6575540939811617e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20410208.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  326454.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24970.771484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  671.9486083984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144368.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2081886.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  326454.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  122.3109130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.36964225769043    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  382.0535888671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1951328815484885e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  23150264.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50374856.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452486.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  352/6933] Loss: -614.0646 [iq: 14.6934,ans: 13.2258,interp: 9.8282,fusion: -651.8120]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  80855.984375    \n",
      "module.ans_embedding.weight  dot:  99511.03125    \n",
      "module.lstm.weight_ih_l0  dot:  1170982.75    \n",
      "module.lstm.weight_hh_l0  dot:  664955.625    \n",
      "module.lstm.bias_ih_l0  dot:  43017.734375    \n",
      "module.lstm.bias_hh_l0  dot:  43017.734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14653151.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8667.8828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1115531.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1115531.5    \n",
      "module.adapter.frcn_linear.weight  dot:  454842240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  272099.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1429123.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1132.906005859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1337935.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  18351524.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  296390.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3465.85302734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  227.48898315429688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12867.873046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  844200.6875    \n",
      "module.attflat_lang.linear_merge.bias  dot:  296390.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  127.11041259765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.629451751708984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  371.5220947265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16588704.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31163484.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452487.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  353/6933] Loss: -602.1281 [iq: 12.7241,ans: 11.4688,interp: 9.4762,fusion: -635.7972]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  199530.703125    \n",
      "module.ans_embedding.weight  dot:  223287.09375    \n",
      "module.lstm.weight_ih_l0  dot:  6308131.0    \n",
      "module.lstm.weight_hh_l0  dot:  1942909.375    \n",
      "module.lstm.bias_ih_l0  dot:  372645.28125    \n",
      "module.lstm.bias_hh_l0  dot:  372645.28125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26788852.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13322.0244140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2182133.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2182133.0    \n",
      "module.adapter.frcn_linear.weight  dot:  529618752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  306212.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2008197.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1497.1473388671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1669205.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21292792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  337238.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14656.150390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  522.0068359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  87894.0859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1742814.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  337238.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  69.4098129272461    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.048954963684082    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  115.15572357177734    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19991672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  37406992.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452488.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  354/6933] Loss: -597.2003 [iq: 15.0241,ans: 12.7058,interp: 13.8753,fusion: -638.8055]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  93161.2421875    \n",
      "module.ans_embedding.weight  dot:  122439.140625    \n",
      "module.lstm.weight_ih_l0  dot:  1790946.625    \n",
      "module.lstm.weight_hh_l0  dot:  1005376.0    \n",
      "module.lstm.bias_ih_l0  dot:  73595.4375    \n",
      "module.lstm.bias_hh_l0  dot:  73595.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21310624.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10073.4296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1663355.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1663355.5    \n",
      "module.adapter.frcn_linear.weight  dot:  431384416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  261480.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1274029.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1334.332763671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1586021.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16921584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  302517.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10043.5908203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  282.226806640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50625.953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1066637.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  302517.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  64.21882629394531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.821281433105469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  188.77386474609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22245236.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50673552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452489.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  355/6933] Loss: -596.8127 [iq: 13.5368,ans: 12.3682,interp: 11.8736,fusion: -634.5914]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  197489.015625    \n",
      "module.ans_embedding.weight  dot:  191485.34375    \n",
      "module.lstm.weight_ih_l0  dot:  4129792.5    \n",
      "module.lstm.weight_hh_l0  dot:  1573024.5    \n",
      "module.lstm.bias_ih_l0  dot:  185181.40625    \n",
      "module.lstm.bias_hh_l0  dot:  185181.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21545136.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6390.68017578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1786042.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1786042.625    \n",
      "module.adapter.frcn_linear.weight  dot:  540687616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  335904.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1023062.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  898.822509765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  980807.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20296872.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  403113.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20063.01953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  696.389892578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  117926.546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1974224.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  403113.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  372.93109130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  87.34600067138672    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1298.7840576171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17951096.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32303288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452490.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  356/6933] Loss: -620.3556 [iq: 10.7619,ans: 11.2703,interp: 8.9411,fusion: -651.3289]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  166046.234375    \n",
      "module.ans_embedding.weight  dot:  432504.9375    \n",
      "module.lstm.weight_ih_l0  dot:  4405689.0    \n",
      "module.lstm.weight_hh_l0  dot:  1192946.625    \n",
      "module.lstm.bias_ih_l0  dot:  248414.921875    \n",
      "module.lstm.bias_hh_l0  dot:  248414.921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45010152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23568.03125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4877344.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4877344.0    \n",
      "module.adapter.frcn_linear.weight  dot:  648006848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  388980.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1532454.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1377.179931640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2062868.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23679794.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  479567.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11900.08984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  327.18878173828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  61724.8046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1429828.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  479567.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  774.0526123046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  198.06918334960938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1545.0430908203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22284140.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  48428660.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452491.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  357/6933] Loss: -592.3663 [iq: 9.5400,ans: 10.4818,interp: 8.5431,fusion: -620.9311]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  136970.25    \n",
      "module.ans_embedding.weight  dot:  285688.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3203309.25    \n",
      "module.lstm.weight_hh_l0  dot:  1102411.25    \n",
      "module.lstm.bias_ih_l0  dot:  202420.28125    \n",
      "module.lstm.bias_hh_l0  dot:  202420.28125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26182288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1519.009521484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2235151.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2235151.0    \n",
      "module.adapter.frcn_linear.weight  dot:  572589312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  340087.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2241928.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2117.53369140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2872117.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22162596.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  429170.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13384.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  397.27105712890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  70690.203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1389757.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  429170.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18.0482177734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.883843421936035    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  26.721824645996094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17787776.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34292920.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452492.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  358/6933] Loss: -594.3474 [iq: 9.0132,ans: 10.4520,interp: 8.1892,fusion: -622.0018]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  122668.71875    \n",
      "module.ans_embedding.weight  dot:  118712.984375    \n",
      "module.lstm.weight_ih_l0  dot:  1314338.25    \n",
      "module.lstm.weight_hh_l0  dot:  635321.625    \n",
      "module.lstm.bias_ih_l0  dot:  48617.1875    \n",
      "module.lstm.bias_hh_l0  dot:  48617.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17561920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13694.732421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1496927.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1496927.75    \n",
      "module.adapter.frcn_linear.weight  dot:  470244544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  285429.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2133918.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2229.20361328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2604340.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17768050.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  330164.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8226.982421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  252.111083984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45440.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1076085.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  330164.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1107.2974853515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  186.146484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4784.388671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21806028.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49578844.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452493.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  359/6933] Loss: -605.0215 [iq: 12.9267,ans: 12.7689,interp: 12.6376,fusion: -643.3548]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  165569.6875    \n",
      "module.ans_embedding.weight  dot:  185064.6875    \n",
      "module.lstm.weight_ih_l0  dot:  4199825.0    \n",
      "module.lstm.weight_hh_l0  dot:  1825412.75    \n",
      "module.lstm.bias_ih_l0  dot:  221887.59375    \n",
      "module.lstm.bias_hh_l0  dot:  221887.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24419016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12891.5166015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2180717.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2180717.0    \n",
      "module.adapter.frcn_linear.weight  dot:  693561856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  447284.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4998752.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4983.70654296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  6205024.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23624362.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  479979.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17777.814453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  349.029296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  105116.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2026164.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  479979.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  350.4717712402344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  115.3546371459961    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  979.490234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.800116025829084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22361224.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50814220.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452493.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  360/6933] Loss: -605.0428 [iq: 10.8973,ans: 11.9434,interp: 10.6849,fusion: -638.5683]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  218049.984375    \n",
      "module.ans_embedding.weight  dot:  312390.8125    \n",
      "module.lstm.weight_ih_l0  dot:  6199093.0    \n",
      "module.lstm.weight_hh_l0  dot:  1502520.875    \n",
      "module.lstm.bias_ih_l0  dot:  385384.34375    \n",
      "module.lstm.bias_hh_l0  dot:  385384.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33671704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8486.408203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3547697.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3547697.5    \n",
      "module.adapter.frcn_linear.weight  dot:  568638656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  344604.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2401658.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2603.9228515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3577385.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22715068.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  483689.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28970.46875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  646.77734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  164865.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2081140.125    \n",
      "module.attflat_lang.linear_merge.bias  dot:  483689.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  91.68214416503906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.273515701293945    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  191.1409149169922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21526236.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  44291304.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452494.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  361/6933] Loss: -606.4756 [iq: 8.2736,ans: 10.6127,interp: 9.1241,fusion: -634.4860]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  196958.359375    \n",
      "module.ans_embedding.weight  dot:  164611.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3542474.5    \n",
      "module.lstm.weight_hh_l0  dot:  1999477.25    \n",
      "module.lstm.bias_ih_l0  dot:  147609.3125    \n",
      "module.lstm.bias_hh_l0  dot:  147609.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21678644.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4843.16748046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2053142.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2053142.875    \n",
      "module.adapter.frcn_linear.weight  dot:  548250048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  325435.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3658006.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4638.2744140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5644234.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23085464.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  411915.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  39949.3046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  720.7840576171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  231826.65625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2094645.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  411915.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  299.58984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  96.57759094238281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  860.322021484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22206176.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  51078984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452495.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  362/6933] Loss: -626.5690 [iq: 9.7127,ans: 11.0063,interp: 9.0599,fusion: -656.3478]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  168380.4375    \n",
      "module.ans_embedding.weight  dot:  133527.8125    \n",
      "module.lstm.weight_ih_l0  dot:  2055230.5    \n",
      "module.lstm.weight_hh_l0  dot:  1170456.0    \n",
      "module.lstm.bias_ih_l0  dot:  66551.4453125    \n",
      "module.lstm.bias_hh_l0  dot:  66551.4453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19696630.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13066.337890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1582477.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1582477.5    \n",
      "module.adapter.frcn_linear.weight  dot:  446974336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  258445.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2378748.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2749.7890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3302336.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.022684490540996e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19635232.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  302095.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16518.5234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  406.0924987792969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  107259.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1481556.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  302095.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  223.8121337890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.51112365722656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  287.12322998046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21642436.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  44843816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452497.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  363/6933] Loss: -617.2994 [iq: 12.2607,ans: 12.1804,interp: 11.4289,fusion: -653.1694]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  202693.625    \n",
      "module.ans_embedding.weight  dot:  154866.09375    \n",
      "module.lstm.weight_ih_l0  dot:  4635979.0    \n",
      "module.lstm.weight_hh_l0  dot:  2155394.5    \n",
      "module.lstm.bias_ih_l0  dot:  225274.859375    \n",
      "module.lstm.bias_hh_l0  dot:  225274.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22069828.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3495.251953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1702203.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1702203.375    \n",
      "module.adapter.frcn_linear.weight  dot:  704794496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  443033.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4457804.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5658.35595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5723564.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23979724.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  479838.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24230.0703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  384.4129333496094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  139699.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2204035.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  479838.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  35.43229675292969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.450827598571777    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  32.476837158203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20182230.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  46116600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452497.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  364/6933] Loss: -613.1750 [iq: 13.2194,ans: 12.3428,interp: 11.2946,fusion: -650.0319]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  219663.859375    \n",
      "module.ans_embedding.weight  dot:  350046.0625    \n",
      "module.lstm.weight_ih_l0  dot:  6399742.0    \n",
      "module.lstm.weight_hh_l0  dot:  593443.25    \n",
      "module.lstm.bias_ih_l0  dot:  480484.90625    \n",
      "module.lstm.bias_hh_l0  dot:  480484.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44939264.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12564.91796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4358945.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4358945.0    \n",
      "module.adapter.frcn_linear.weight  dot:  623434240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  379385.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2094390.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2077.320068359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2257558.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25378972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  521715.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3407.124755859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  266.8219909667969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  6799.5478515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1386801.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  521715.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.847442626953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.434819221496582    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  17.477672576904297    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21960052.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50424012.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452498.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  365/6933] Loss: -591.9576 [iq: 11.9206,ans: 11.5172,interp: 9.6325,fusion: -625.0280]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  163059.96875    \n",
      "module.ans_embedding.weight  dot:  318651.0625    \n",
      "module.lstm.weight_ih_l0  dot:  3294094.5    \n",
      "module.lstm.weight_hh_l0  dot:  1379505.0    \n",
      "module.lstm.bias_ih_l0  dot:  144885.4375    \n",
      "module.lstm.bias_hh_l0  dot:  144885.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30815728.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3682.864501953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2627776.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2627776.0    \n",
      "module.adapter.frcn_linear.weight  dot:  552321536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  347509.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2561976.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2412.088134765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2691518.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22784054.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  472900.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21575.802734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  784.0496826171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  123772.2578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2424182.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  472900.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  238.75601196289062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  53.403072357177734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  652.3629150390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20708150.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  43108888.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452499.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  366/6933] Loss: -628.6114 [iq: 12.1696,ans: 11.5853,interp: 10.6944,fusion: -663.0607]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  209835.34375    \n",
      "module.ans_embedding.weight  dot:  107641.4375    \n",
      "module.lstm.weight_ih_l0  dot:  4707954.5    \n",
      "module.lstm.weight_hh_l0  dot:  855529.25    \n",
      "module.lstm.bias_ih_l0  dot:  299842.0    \n",
      "module.lstm.bias_hh_l0  dot:  299842.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12810544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6090.93701171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1006980.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1006980.25    \n",
      "module.adapter.frcn_linear.weight  dot:  381897344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  215789.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3693728.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3656.10791015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5464152.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17448788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  295187.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18711.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  559.404296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  107965.203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1399506.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  295187.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  44.00756072998047    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.936924934387207    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  144.78695678710938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15506296.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31906492.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452500.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  367/6933] Loss: -627.4234 [iq: 11.4198,ans: 11.1808,interp: 9.6178,fusion: -659.6418]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  230807.515625    \n",
      "module.ans_embedding.weight  dot:  109295.2265625    \n",
      "module.lstm.weight_ih_l0  dot:  5758881.5    \n",
      "module.lstm.weight_hh_l0  dot:  1643249.25    \n",
      "module.lstm.bias_ih_l0  dot:  296078.8125    \n",
      "module.lstm.bias_hh_l0  dot:  296078.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15570993.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15286.3828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1306074.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1306074.875    \n",
      "module.adapter.frcn_linear.weight  dot:  458880096.0    \n",
      "module.adapter.frcn_linear.bias  dot:  290822.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1703835.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1448.7210693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1576586.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  21458468.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  418327.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24272.70703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  555.8241577148438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  158091.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2434615.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  418327.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  53.76824188232422    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.544382095336914    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  116.73352813720703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.220446049250313e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16738319.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  33105084.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452501.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  368/6933] Loss: -636.1364 [iq: 11.7828,ans: 10.7865,interp: 8.6526,fusion: -667.3582]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  205101.96875    \n",
      "module.ans_embedding.weight  dot:  123888.1875    \n",
      "module.lstm.weight_ih_l0  dot:  9767658.0    \n",
      "module.lstm.weight_hh_l0  dot:  1397995.0    \n",
      "module.lstm.bias_ih_l0  dot:  633519.0    \n",
      "module.lstm.bias_hh_l0  dot:  633519.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13375030.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8890.0322265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1041302.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1041302.375    \n",
      "module.adapter.frcn_linear.weight  dot:  415235776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  259227.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1218686.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1140.7080078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1298267.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22199720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  426769.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24393.1328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  547.3834838867188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  136491.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2402516.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  426769.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  362.33514404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.79339599609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1010.794921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  16197474.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32143504.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452502.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  369/6933] Loss: -635.3823 [iq: 10.7721,ans: 10.6928,interp: 8.0192,fusion: -664.8663]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  265651.28125    \n",
      "module.ans_embedding.weight  dot:  192288.65625    \n",
      "module.lstm.weight_ih_l0  dot:  16147280.0    \n",
      "module.lstm.weight_hh_l0  dot:  1059114.0    \n",
      "module.lstm.bias_ih_l0  dot:  1173064.0    \n",
      "module.lstm.bias_hh_l0  dot:  1173064.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21914666.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15307.6953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2177101.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2177101.25    \n",
      "module.adapter.frcn_linear.weight  dot:  518400576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  310437.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2734509.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2760.507568359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3553645.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  24736468.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  451614.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12835.775390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  643.3076171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  16433.509765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  1948970.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  451614.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  354.5574035644531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  61.88574981689453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1091.848876953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17823422.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  39785120.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452503.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  370/6933] Loss: -619.7704 [iq: 11.4108,ans: 11.2651,interp: 9.4088,fusion: -651.8551]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  229205.71875    \n",
      "module.ans_embedding.weight  dot:  221937.96875    \n",
      "module.lstm.weight_ih_l0  dot:  6186970.0    \n",
      "module.lstm.weight_hh_l0  dot:  1225520.5    \n",
      "module.lstm.bias_ih_l0  dot:  364668.53125    \n",
      "module.lstm.bias_hh_l0  dot:  364668.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19799834.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8458.87109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1692144.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1692144.0    \n",
      "module.adapter.frcn_linear.weight  dot:  351556992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  224251.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1641958.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1306.150390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1882509.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19115016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  349403.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28367.240234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  518.9052734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  185488.765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2122035.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  349403.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  557.590087890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  135.70761108398438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  907.719482421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17596944.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  38841880.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452504.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  371/6933] Loss: -631.4162 [iq: 12.7284,ans: 11.8644,interp: 11.2087,fusion: -667.2177]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  156743.453125    \n",
      "module.ans_embedding.weight  dot:  239222.484375    \n",
      "module.lstm.weight_ih_l0  dot:  6453809.0    \n",
      "module.lstm.weight_hh_l0  dot:  696526.1875    \n",
      "module.lstm.bias_ih_l0  dot:  487047.8125    \n",
      "module.lstm.bias_hh_l0  dot:  487047.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28981700.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12442.6015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2473413.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2473413.0    \n",
      "module.adapter.frcn_linear.weight  dot:  437786528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  282858.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1096140.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  996.3106689453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1151872.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21712818.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  438426.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12170.5263671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  417.8970947265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  60267.8828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2426547.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  438426.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  368.98590087890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  105.41217041015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  691.5548095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22047248.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52937904.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452505.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  372/6933] Loss: -623.1404 [iq: 13.3033,ans: 12.8574,interp: 11.5956,fusion: -660.8967]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  217814.5625    \n",
      "module.ans_embedding.weight  dot:  257108.171875    \n",
      "module.lstm.weight_ih_l0  dot:  4421068.0    \n",
      "module.lstm.weight_hh_l0  dot:  3048496.75    \n",
      "module.lstm.bias_ih_l0  dot:  182232.140625    \n",
      "module.lstm.bias_hh_l0  dot:  182232.140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28857188.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21674.2265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2233659.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2233659.0    \n",
      "module.adapter.frcn_linear.weight  dot:  561110976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  346613.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1805237.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1454.938720703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2053778.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23334450.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  388971.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  59631.08984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  587.4637451171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  397871.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3717807.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  388971.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  59.424739837646484    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.651241779327393    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  205.1166229248047    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.1391778065881226e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20798484.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  39689148.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452506.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  373/6933] Loss: -649.6030 [iq: 11.9513,ans: 12.2618,interp: 11.0216,fusion: -684.8377]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  317457.15625    \n",
      "module.ans_embedding.weight  dot:  213107.6875    \n",
      "module.lstm.weight_ih_l0  dot:  17776464.0    \n",
      "module.lstm.weight_hh_l0  dot:  1295785.875    \n",
      "module.lstm.bias_ih_l0  dot:  1256461.0    \n",
      "module.lstm.bias_hh_l0  dot:  1256461.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21884740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19890.04296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1861979.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1861979.125    \n",
      "module.adapter.frcn_linear.weight  dot:  656688256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  396532.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2141424.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1598.5498046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2217295.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27714192.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  478608.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10683.810546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  384.5358581542969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  32011.22265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2556490.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  478608.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  581.97509765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  111.1745834350586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1739.67041015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20190326.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  48084276.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452507.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  374/6933] Loss: -625.6906 [iq: 9.9921,ans: 11.4315,interp: 8.9794,fusion: -656.0935]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  285209.375    \n",
      "module.ans_embedding.weight  dot:  118325.8359375    \n",
      "module.lstm.weight_ih_l0  dot:  5503416.0    \n",
      "module.lstm.weight_hh_l0  dot:  1141747.0    \n",
      "module.lstm.bias_ih_l0  dot:  336826.9375    \n",
      "module.lstm.bias_hh_l0  dot:  336826.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13833652.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20523.658203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1064811.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1064811.125    \n",
      "module.adapter.frcn_linear.weight  dot:  477311424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  290751.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2336627.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2359.7431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3086719.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23167622.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  397213.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20482.037109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  488.1463623046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  141206.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2353428.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  397213.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1056.137451171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  128.84140014648438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4286.326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18102450.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  38048880.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452508.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  375/6933] Loss: -635.9069 [iq: 10.7014,ans: 11.9029,interp: 9.8479,fusion: -668.3591]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  646517.1875    \n",
      "module.ans_embedding.weight  dot:  405564.375    \n",
      "module.lstm.weight_ih_l0  dot:  21191332.0    \n",
      "module.lstm.weight_hh_l0  dot:  7531428.0    \n",
      "module.lstm.bias_ih_l0  dot:  1421767.875    \n",
      "module.lstm.bias_hh_l0  dot:  1421767.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40533616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6688.71826171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4079054.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4079054.25    \n",
      "module.adapter.frcn_linear.weight  dot:  669426688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  439288.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2601706.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2287.844482421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3113844.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25292788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  479377.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  93715.765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  878.0772705078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  664232.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4358212.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  479377.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  120.6182632446289    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.25157928466797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  299.4875183105469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22144976.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  51162276.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452508.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  376/6933] Loss: -637.7909 [iq: 10.4698,ans: 11.7491,interp: 10.4837,fusion: -670.4935]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  291426.71875    \n",
      "module.ans_embedding.weight  dot:  167459.828125    \n",
      "module.lstm.weight_ih_l0  dot:  3557461.0    \n",
      "module.lstm.weight_hh_l0  dot:  1407884.75    \n",
      "module.lstm.bias_ih_l0  dot:  198472.6875    \n",
      "module.lstm.bias_hh_l0  dot:  198472.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18098962.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6238.841796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1584323.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1584323.0    \n",
      "module.adapter.frcn_linear.weight  dot:  592003584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  370031.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2226942.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1746.649658203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2067891.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26930444.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  450054.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28011.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  522.1256713867188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  188688.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2144367.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  450054.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  83.96818542480469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  29.42125701904297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  168.78692626953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17400184.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  37005984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452509.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  377/6933] Loss: -625.4113 [iq: 11.0162,ans: 11.7631,interp: 10.9011,fusion: -659.0916]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  258430.203125    \n",
      "module.ans_embedding.weight  dot:  231731.578125    \n",
      "module.lstm.weight_ih_l0  dot:  10221607.0    \n",
      "module.lstm.weight_hh_l0  dot:  1150435.5    \n",
      "module.lstm.bias_ih_l0  dot:  793429.75    \n",
      "module.lstm.bias_hh_l0  dot:  793429.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20414032.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8103.5126953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1359813.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1359813.875    \n",
      "module.adapter.frcn_linear.weight  dot:  445533984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  281923.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2882942.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2817.4697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3381526.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23374160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  428399.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21435.74609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  541.2305908203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  133779.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2953981.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  428399.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  73.66822052001953    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.21306037902832    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  147.1009521484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20288224.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  41553104.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452510.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  378/6933] Loss: -653.4945 [iq: 11.1414,ans: 11.9847,interp: 9.7570,fusion: -686.3776]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  397271.3125    \n",
      "module.ans_embedding.weight  dot:  221247.046875    \n",
      "module.lstm.weight_ih_l0  dot:  4338555.5    \n",
      "module.lstm.weight_hh_l0  dot:  1424851.75    \n",
      "module.lstm.bias_ih_l0  dot:  213412.65625    \n",
      "module.lstm.bias_hh_l0  dot:  213412.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17748592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5818.49658203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1103851.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1103851.25    \n",
      "module.adapter.frcn_linear.weight  dot:  538058368.0    \n",
      "module.adapter.frcn_linear.bias  dot:  340202.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3911323.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4304.81591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5030441.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.725290298461914e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24773072.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  431447.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18925.62109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  428.9817810058594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  101460.8203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2956051.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  431447.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  90.9324951171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.907629013061523    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  104.36929321289062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17370050.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34248900.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452511.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  379/6933] Loss: -640.7477 [iq: 11.5655,ans: 11.6700,interp: 11.3744,fusion: -675.3575]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  208891.703125    \n",
      "module.ans_embedding.weight  dot:  208456.8125    \n",
      "module.lstm.weight_ih_l0  dot:  4222327.0    \n",
      "module.lstm.weight_hh_l0  dot:  1094864.0    \n",
      "module.lstm.bias_ih_l0  dot:  228070.625    \n",
      "module.lstm.bias_hh_l0  dot:  228070.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18031958.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8238.9775390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1228733.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1228733.0    \n",
      "module.adapter.frcn_linear.weight  dot:  328328896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  193999.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2525968.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2391.26708984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3104150.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21729706.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  327161.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  45467.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  574.2501831054688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  346944.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  2861101.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  327161.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  41.62860107421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.540892601013184    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  102.36219787597656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17762310.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  36533984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452512.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  380/6933] Loss: -655.9037 [iq: 9.7542,ans: 11.0918,interp: 10.5777,fusion: -687.3275]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  279623.875    \n",
      "module.ans_embedding.weight  dot:  201359.5    \n",
      "module.lstm.weight_ih_l0  dot:  9192213.0    \n",
      "module.lstm.weight_hh_l0  dot:  815115.6875    \n",
      "module.lstm.bias_ih_l0  dot:  585087.8125    \n",
      "module.lstm.bias_hh_l0  dot:  585087.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23923216.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8174.2255859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1863156.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1863156.25    \n",
      "module.adapter.frcn_linear.weight  dot:  484383488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  308128.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2193821.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1954.353759765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2545052.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26400652.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  431278.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5704.2412109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  349.1060485839844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  12580.2548828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2338747.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  431278.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  357.2433776855469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.2846450805664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  766.833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22483558.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  52324600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452513.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  381/6933] Loss: -640.8900 [iq: 13.7611,ans: 12.9176,interp: 11.3246,fusion: -678.8933]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  370085.0625    \n",
      "module.ans_embedding.weight  dot:  189264.8125    \n",
      "module.lstm.weight_ih_l0  dot:  3530898.0    \n",
      "module.lstm.weight_hh_l0  dot:  1187726.125    \n",
      "module.lstm.bias_ih_l0  dot:  96267.2109375    \n",
      "module.lstm.bias_hh_l0  dot:  96267.2109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19680702.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9192.068359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1500028.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1500028.875    \n",
      "module.adapter.frcn_linear.weight  dot:  494961568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  312576.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  920415.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  800.2274169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  931544.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23225082.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  384325.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18043.50390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  498.28240966796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  85840.5390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2673178.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  384325.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  96.29914855957031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.761493682861328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  94.05135345458984    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18452488.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  40064880.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452514.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  382/6933] Loss: -635.9786 [iq: 11.3175,ans: 11.0121,interp: 9.9197,fusion: -668.2279]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  284482.84375    \n",
      "module.ans_embedding.weight  dot:  189167.625    \n",
      "module.lstm.weight_ih_l0  dot:  2832945.0    \n",
      "module.lstm.weight_hh_l0  dot:  942691.6875    \n",
      "module.lstm.bias_ih_l0  dot:  115148.046875    \n",
      "module.lstm.bias_hh_l0  dot:  115148.046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14783420.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11320.0478515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1007176.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1007176.875    \n",
      "module.adapter.frcn_linear.weight  dot:  451482432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  287982.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1664230.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1316.8349609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1346055.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  21527916.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  335434.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13552.6337890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  245.92037963867188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  95810.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2724560.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  335434.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  40.129295349121094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.0051326751709    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  100.91252899169922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15267805.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31904254.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452515.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  383/6933] Loss: -649.7065 [iq: 11.5989,ans: 11.2779,interp: 10.5626,fusion: -683.1458]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  315667.1875    \n",
      "module.ans_embedding.weight  dot:  318039.09375    \n",
      "module.lstm.weight_ih_l0  dot:  4183845.5    \n",
      "module.lstm.weight_hh_l0  dot:  2423507.25    \n",
      "module.lstm.bias_ih_l0  dot:  151154.125    \n",
      "module.lstm.bias_hh_l0  dot:  151154.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20548200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10661.06640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1608354.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1608354.5    \n",
      "module.adapter.frcn_linear.weight  dot:  377408896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  224565.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2818308.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2918.157470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3367284.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24300960.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  362996.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44074.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  676.2166748046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  320712.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4603250.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  362996.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  147.01234436035156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.006919860839844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  176.30307006835938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18496940.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32533688.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452516.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  384/6933] Loss: -670.9360 [iq: 12.3506,ans: 11.4485,interp: 10.5944,fusion: -705.3296]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  409448.4375    \n",
      "module.ans_embedding.weight  dot:  335132.9375    \n",
      "module.lstm.weight_ih_l0  dot:  4186141.5    \n",
      "module.lstm.weight_hh_l0  dot:  2278590.5    \n",
      "module.lstm.bias_ih_l0  dot:  118381.546875    \n",
      "module.lstm.bias_hh_l0  dot:  118381.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20472834.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5874.91015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1576972.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1576972.25    \n",
      "module.adapter.frcn_linear.weight  dot:  392258944.0    \n",
      "module.adapter.frcn_linear.bias  dot:  252789.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2520112.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2548.4580078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3277937.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24677314.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  368729.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  43329.046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  741.9210205078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  323007.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3185735.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  368729.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  55.42490768432617    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.992834091186523    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  176.29820251464844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  18941020.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  36625128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452517.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  385/6933] Loss: -665.0154 [iq: 10.8786,ans: 10.9658,interp: 9.7762,fusion: -696.6360]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  485740.8125    \n",
      "module.ans_embedding.weight  dot:  282887.5625    \n",
      "module.lstm.weight_ih_l0  dot:  6271737.5    \n",
      "module.lstm.weight_hh_l0  dot:  3682888.75    \n",
      "module.lstm.bias_ih_l0  dot:  204160.09375    \n",
      "module.lstm.bias_hh_l0  dot:  204160.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16120725.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6652.025390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1220218.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1220218.75    \n",
      "module.adapter.frcn_linear.weight  dot:  409417696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  263466.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3953263.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4121.82666015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5662944.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23834002.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  383133.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  86886.78125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  989.4186401367188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  688596.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3971725.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  383133.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  61.193504333496094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.980681419372559    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  233.98117065429688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16354334.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  27670124.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452518.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  386/6933] Loss: -667.6873 [iq: 9.4231,ans: 10.0273,interp: 8.4771,fusion: -695.6147]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  354027.4375    \n",
      "module.ans_embedding.weight  dot:  430050.8125    \n",
      "module.lstm.weight_ih_l0  dot:  9709410.0    \n",
      "module.lstm.weight_hh_l0  dot:  1747857.5    \n",
      "module.lstm.bias_ih_l0  dot:  626743.0    \n",
      "module.lstm.bias_hh_l0  dot:  626743.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33081892.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  27820.0546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3146040.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3146040.75    \n",
      "module.adapter.frcn_linear.weight  dot:  568652480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  380066.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1968051.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1699.28271484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2217003.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  29892688.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  510762.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  32368.447265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  767.1493530273438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  227138.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3139922.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  510762.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1233.079345703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  225.7794647216797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3956.81591796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20479184.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  40497636.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452519.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  387/6933] Loss: -658.8704 [iq: 12.9229,ans: 11.8071,interp: 10.7751,fusion: -694.3755]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1184773.75    \n",
      "module.ans_embedding.weight  dot:  568294.6875    \n",
      "module.lstm.weight_ih_l0  dot:  46194644.0    \n",
      "module.lstm.weight_hh_l0  dot:  8836555.0    \n",
      "module.lstm.bias_ih_l0  dot:  3107700.5    \n",
      "module.lstm.bias_hh_l0  dot:  3107700.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38185616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4386.2705078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3206831.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3206831.0    \n",
      "module.adapter.frcn_linear.weight  dot:  558538432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  367455.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2844687.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2927.91064453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3097467.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27989844.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  457267.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  91869.1875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  714.69775390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  733104.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3481714.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  457267.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  74.25555419921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.06828498840332    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  264.97113037109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20874294.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42454584.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452520.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  388/6933] Loss: -647.2225 [iq: 11.4682,ans: 11.1503,interp: 8.7145,fusion: -678.5555]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  796237.5    \n",
      "module.ans_embedding.weight  dot:  339445.8125    \n",
      "module.lstm.weight_ih_l0  dot:  5382817.0    \n",
      "module.lstm.weight_hh_l0  dot:  2932850.0    \n",
      "module.lstm.bias_ih_l0  dot:  184206.4375    \n",
      "module.lstm.bias_hh_l0  dot:  184206.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23854092.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5444.56103515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1741575.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1741575.0    \n",
      "module.adapter.frcn_linear.weight  dot:  347771360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  229716.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  961088.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  667.7698974609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  812375.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19643934.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  302633.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22213.53125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  369.004150390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144463.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2446374.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  302633.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  229.37387084960938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.16078186035156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  309.22869873046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16930476.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34305788.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452521.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  389/6933] Loss: -650.4385 [iq: 11.7152,ans: 11.2061,interp: 10.2303,fusion: -683.5901]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  367994.65625    \n",
      "module.ans_embedding.weight  dot:  482711.90625    \n",
      "module.lstm.weight_ih_l0  dot:  3758711.25    \n",
      "module.lstm.weight_hh_l0  dot:  2808678.25    \n",
      "module.lstm.bias_ih_l0  dot:  136763.84375    \n",
      "module.lstm.bias_hh_l0  dot:  136763.84375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46341592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37547.171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4043184.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4043184.0    \n",
      "module.adapter.frcn_linear.weight  dot:  459957952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  275836.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2969634.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2863.304443359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3689996.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28018128.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  383458.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  55401.875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  517.7396240234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  379330.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3238633.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  383458.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  223.7596435546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.5093994140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1210.4697265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  25643552.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  50056208.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452522.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  390/6933] Loss: -644.7365 [iq: 12.0527,ans: 11.9850,interp: 10.8897,fusion: -679.6639]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  598485.6875    \n",
      "module.ans_embedding.weight  dot:  293755.40625    \n",
      "module.lstm.weight_ih_l0  dot:  5159307.0    \n",
      "module.lstm.weight_hh_l0  dot:  1533292.75    \n",
      "module.lstm.bias_ih_l0  dot:  234118.78125    \n",
      "module.lstm.bias_hh_l0  dot:  234118.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27325870.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16508.474609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2549626.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2549626.5    \n",
      "module.adapter.frcn_linear.weight  dot:  651489536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  411098.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1314962.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1093.165283203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1334905.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  30175576.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  460757.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23082.029296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  729.2312622070312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  147173.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3608640.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  460757.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  214.6436309814453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  52.546512603759766    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  803.256103515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19937436.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  40676604.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452523.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  391/6933] Loss: -655.1114 [iq: 11.4388,ans: 11.4361,interp: 10.7674,fusion: -688.7537]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  625872.625    \n",
      "module.ans_embedding.weight  dot:  301146.90625    \n",
      "module.lstm.weight_ih_l0  dot:  6439323.0    \n",
      "module.lstm.weight_hh_l0  dot:  2918322.75    \n",
      "module.lstm.bias_ih_l0  dot:  174796.0    \n",
      "module.lstm.bias_hh_l0  dot:  174796.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20164678.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14704.138671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1527598.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1527598.25    \n",
      "module.adapter.frcn_linear.weight  dot:  466435840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  303528.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3730013.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3588.769287109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4666409.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26526732.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  394171.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  45597.203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1143.70068359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  321884.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4586463.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  394171.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  309.67047119140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  64.45252990722656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1029.8189697265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16655571.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  27085106.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452524.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  392/6933] Loss: -672.1369 [iq: 9.6510,ans: 10.7872,interp: 8.5298,fusion: -701.1049]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  362173.78125    \n",
      "module.ans_embedding.weight  dot:  429734.8125    \n",
      "module.lstm.weight_ih_l0  dot:  11232472.0    \n",
      "module.lstm.weight_hh_l0  dot:  1266230.75    \n",
      "module.lstm.bias_ih_l0  dot:  760122.0625    \n",
      "module.lstm.bias_hh_l0  dot:  760122.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22821252.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19874.07421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2038654.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2038654.5    \n",
      "module.adapter.frcn_linear.weight  dot:  564924480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  367393.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  5410260.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5608.671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7144735.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  31049112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  478082.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23942.619140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  692.8937377929688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  156980.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.204139258945361e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3277873.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  478082.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1147.951171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  273.79156494140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3503.079833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.46265255252365e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18704562.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  38535640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452525.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  393/6933] Loss: -659.8433 [iq: 11.1013,ans: 10.7158,interp: 8.8232,fusion: -690.4835]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  492807.125    \n",
      "module.ans_embedding.weight  dot:  450913.21875    \n",
      "module.lstm.weight_ih_l0  dot:  10964550.0    \n",
      "module.lstm.weight_hh_l0  dot:  1162435.25    \n",
      "module.lstm.bias_ih_l0  dot:  774518.0    \n",
      "module.lstm.bias_hh_l0  dot:  774518.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31487152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23272.03125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2837950.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2837950.5    \n",
      "module.adapter.frcn_linear.weight  dot:  445126176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  297742.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1716829.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1206.3349609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2340198.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26896416.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  401209.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26194.25    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  825.4736328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  99071.546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3404828.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  401209.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  311.39105224609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.19310760498047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  802.5675048828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18457018.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  29592392.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452526.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  394/6933] Loss: -664.9534 [iq: 8.8251,ans: 9.6778,interp: 7.9410,fusion: -691.3972]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1026921.5    \n",
      "module.ans_embedding.weight  dot:  237409.140625    \n",
      "module.lstm.weight_ih_l0  dot:  22760400.0    \n",
      "module.lstm.weight_hh_l0  dot:  6360850.5    \n",
      "module.lstm.bias_ih_l0  dot:  1425700.0    \n",
      "module.lstm.bias_hh_l0  dot:  1425700.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13527938.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9767.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  835430.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  835430.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  427440640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  262772.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4330804.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3942.862060546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5352322.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23381188.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  308906.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  103751.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  667.8026123046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  868619.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3432862.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  308906.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  245.62106323242188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  66.7519760131836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  828.5924682617188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13740756.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23911538.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452527.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  395/6933] Loss: -669.1568 [iq: 10.4009,ans: 10.1272,interp: 8.5157,fusion: -698.2005]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  863680.0    \n",
      "module.ans_embedding.weight  dot:  383667.21875    \n",
      "module.lstm.weight_ih_l0  dot:  38550504.0    \n",
      "module.lstm.weight_hh_l0  dot:  2588767.5    \n",
      "module.lstm.bias_ih_l0  dot:  3133538.0    \n",
      "module.lstm.bias_hh_l0  dot:  3133538.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17782236.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10461.2421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  989085.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  989085.5    \n",
      "module.adapter.frcn_linear.weight  dot:  447988672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  304055.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4164718.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  4516.5927734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  5347016.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26748612.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  450957.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11112.1162109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  568.876708984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  29645.01953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3945933.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  450957.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  151.66775512695312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.16765594482422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  387.09881591796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.863665026277886e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17701690.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32477174.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452528.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  396/6933] Loss: -672.9588 [iq: 11.6141,ans: 11.0183,interp: 9.4114,fusion: -705.0027]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  717347.0    \n",
      "module.ans_embedding.weight  dot:  251743.109375    \n",
      "module.lstm.weight_ih_l0  dot:  12198484.0    \n",
      "module.lstm.weight_hh_l0  dot:  1676388.75    \n",
      "module.lstm.bias_ih_l0  dot:  591683.25    \n",
      "module.lstm.bias_hh_l0  dot:  591683.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20080236.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23257.06640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1718125.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1718125.25    \n",
      "module.adapter.frcn_linear.weight  dot:  493249728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  333303.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3562549.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3679.70703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3826464.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27264800.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  428696.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20937.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  629.1636962890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  103195.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3145394.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  428696.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  902.2435913085938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  177.00021362304688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2302.368408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17706440.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  33190546.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452528.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  397/6933] Loss: -669.6792 [iq: 10.2879,ans: 10.3868,interp: 9.0870,fusion: -699.4409]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  287610.625    \n",
      "module.ans_embedding.weight  dot:  406507.40625    \n",
      "module.lstm.weight_ih_l0  dot:  2859591.0    \n",
      "module.lstm.weight_hh_l0  dot:  824551.4375    \n",
      "module.lstm.bias_ih_l0  dot:  111441.953125    \n",
      "module.lstm.bias_hh_l0  dot:  111441.953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35662400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  44865.53515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3272184.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3272184.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387547808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  238403.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1508522.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1194.8812255859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1502408.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27999900.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  335380.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11098.1064453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  370.0505065917969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  41702.9453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2122431.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  335380.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7691.4052734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1245.748046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15060.90625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.284395226430206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20873820.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  39267200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452529.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  398/6933] Loss: -660.4463 [iq: 11.7111,ans: 10.9042,interp: 8.5787,fusion: -691.6403]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  763765.75    \n",
      "module.ans_embedding.weight  dot:  408610.78125    \n",
      "module.lstm.weight_ih_l0  dot:  34373912.0    \n",
      "module.lstm.weight_hh_l0  dot:  2494620.5    \n",
      "module.lstm.bias_ih_l0  dot:  2436187.0    \n",
      "module.lstm.bias_hh_l0  dot:  2436187.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28836478.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29519.3046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2008076.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2008076.25    \n",
      "module.adapter.frcn_linear.weight  dot:  384347392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  246107.234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  845373.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  791.862548828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  905107.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26285830.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  376090.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25472.064453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  475.30792236328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  98539.609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3036424.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  376090.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  435.2038269042969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  84.796630859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1540.487548828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22332006.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  49327872.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452530.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  399/6933] Loss: -648.6454 [iq: 14.4363,ans: 11.6931,interp: 11.5338,fusion: -686.3085]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  622291.25    \n",
      "module.ans_embedding.weight  dot:  384060.71875    \n",
      "module.lstm.weight_ih_l0  dot:  12203046.0    \n",
      "module.lstm.weight_hh_l0  dot:  1430064.5    \n",
      "module.lstm.bias_ih_l0  dot:  798101.0    \n",
      "module.lstm.bias_hh_l0  dot:  798101.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16126091.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6095.451171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  933341.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  933341.375    \n",
      "module.adapter.frcn_linear.weight  dot:  384163200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  247515.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2361008.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2163.573486328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2869823.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5288605936802924e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26409048.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  346732.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19437.234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  646.31884765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  83873.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3967512.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  346732.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  69.14834594726562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.081432342529297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  173.5828857421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17306576.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  29887208.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452531.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  400/6933] Loss: -673.3524 [iq: 12.3801,ans: 11.2298,interp: 9.5570,fusion: -706.5194]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  544639.8125    \n",
      "module.ans_embedding.weight  dot:  317568.0625    \n",
      "module.lstm.weight_ih_l0  dot:  19912002.0    \n",
      "module.lstm.weight_hh_l0  dot:  1524150.625    \n",
      "module.lstm.bias_ih_l0  dot:  1352600.25    \n",
      "module.lstm.bias_hh_l0  dot:  1352600.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20317544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  51693.83984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1594789.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1594789.25    \n",
      "module.adapter.frcn_linear.weight  dot:  356087520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  231708.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1383439.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  887.38525390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1507559.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25877176.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  340262.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22615.27734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  853.228515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  70831.4765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3461111.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  340262.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5233.33056640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  963.453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10591.146484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18678652.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  30573318.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452532.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  401/6933] Loss: -697.7004 [iq: 10.4983,ans: 10.3305,interp: 9.1600,fusion: -727.6891]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1016796.0    \n",
      "module.ans_embedding.weight  dot:  429533.4375    \n",
      "module.lstm.weight_ih_l0  dot:  26368032.0    \n",
      "module.lstm.weight_hh_l0  dot:  2628523.0    \n",
      "module.lstm.bias_ih_l0  dot:  1912199.25    \n",
      "module.lstm.bias_hh_l0  dot:  1912199.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18913738.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6354.14453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1200198.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1200198.5    \n",
      "module.adapter.frcn_linear.weight  dot:  460354528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  327540.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1085997.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  863.8763427734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  952396.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28000352.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  396477.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22682.43359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  885.5505981445312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  108444.8046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4911222.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  396477.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  543.5840454101562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  113.58055114746094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  703.7440185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  17412136.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  37543428.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452533.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  402/6933] Loss: -676.6677 [iq: 13.9267,ans: 11.4148,interp: 11.2686,fusion: -713.2778]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1853476.75    \n",
      "module.ans_embedding.weight  dot:  314904.71875    \n",
      "module.lstm.weight_ih_l0  dot:  98337200.0    \n",
      "module.lstm.weight_hh_l0  dot:  7337005.0    \n",
      "module.lstm.bias_ih_l0  dot:  7136620.0    \n",
      "module.lstm.bias_hh_l0  dot:  7136620.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19856190.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15664.560546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1347879.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1347879.375    \n",
      "module.adapter.frcn_linear.weight  dot:  566380096.0    \n",
      "module.adapter.frcn_linear.bias  dot:  382219.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2116166.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1133.617431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1990307.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32544678.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  486454.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  60605.55078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  982.7282104492188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  186980.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  3835273.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  486454.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  463.7371826171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  124.25072479248047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1134.591064453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16641621.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32506678.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452534.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  403/6933] Loss: -671.1752 [iq: 13.1962,ans: 11.2049,interp: 10.4754,fusion: -706.0517]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1802841.75    \n",
      "module.ans_embedding.weight  dot:  397818.4375    \n",
      "module.lstm.weight_ih_l0  dot:  27438924.0    \n",
      "module.lstm.weight_hh_l0  dot:  6600016.0    \n",
      "module.lstm.bias_ih_l0  dot:  1859406.125    \n",
      "module.lstm.bias_hh_l0  dot:  1859406.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29698566.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19407.310546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2131256.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2131256.5    \n",
      "module.adapter.frcn_linear.weight  dot:  354506464.0    \n",
      "module.adapter.frcn_linear.bias  dot:  222750.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1228062.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  896.420166015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1146513.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23547586.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  275294.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  77052.3125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  637.09423828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  528287.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_lang.linear_merge.weight  dot:  2522815.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  275294.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  77.90574645996094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.108299255371094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  219.74465942382812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.72937269744034e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15311478.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22127364.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452534.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  404/6933] Loss: -649.3899 [iq: 10.2752,ans: 10.2754,interp: 9.7046,fusion: -679.6451]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  993462.5    \n",
      "module.ans_embedding.weight  dot:  430369.5    \n",
      "module.lstm.weight_ih_l0  dot:  24320182.0    \n",
      "module.lstm.weight_hh_l0  dot:  2224291.5    \n",
      "module.lstm.bias_ih_l0  dot:  1613157.125    \n",
      "module.lstm.bias_hh_l0  dot:  1613157.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16488402.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12634.8037109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  750148.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  750148.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  424636992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  269408.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1483212.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1336.9166259765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1350264.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26210184.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  315034.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21442.005859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  768.1502685546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  73473.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3405407.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  315034.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  234.68414306640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.88804626464844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  367.6761474609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15004426.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23104520.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452535.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  405/6933] Loss: -672.4432 [iq: 11.6765,ans: 10.4945,interp: 9.8526,fusion: -704.4667]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  852555.5625    \n",
      "module.ans_embedding.weight  dot:  416081.15625    \n",
      "module.lstm.weight_ih_l0  dot:  7980586.0    \n",
      "module.lstm.weight_hh_l0  dot:  1327453.0    \n",
      "module.lstm.bias_ih_l0  dot:  469379.4375    \n",
      "module.lstm.bias_hh_l0  dot:  469379.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22433836.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3047.84033203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1483528.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1483528.5    \n",
      "module.adapter.frcn_linear.weight  dot:  250019440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159363.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1691872.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1400.3590087890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2087267.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21512140.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  248686.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16287.287109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  755.043701171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50660.5859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3203043.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  248686.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  41.71348571777344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.069770812988281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  84.09303283691406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  16122236.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31078540.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452536.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  406/6933] Loss: -671.4876 [iq: 10.2462,ans: 9.9713,interp: 9.6462,fusion: -701.3512]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  591717.125    \n",
      "module.ans_embedding.weight  dot:  861676.3125    \n",
      "module.lstm.weight_ih_l0  dot:  20519996.0    \n",
      "module.lstm.weight_hh_l0  dot:  2497881.0    \n",
      "module.lstm.bias_ih_l0  dot:  1326821.625    \n",
      "module.lstm.bias_hh_l0  dot:  1326821.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45033616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9723.2880859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3675971.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3675971.5    \n",
      "module.adapter.frcn_linear.weight  dot:  405643360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  277507.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1675958.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1299.99658203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1550542.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33432736.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  477420.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31309.763671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1325.221435546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  165898.234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4557840.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  477420.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  159.74166870117188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.98797607421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  315.74365234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22488924.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  35706220.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452537.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  407/6933] Loss: -690.2866 [iq: 9.7699,ans: 9.5251,interp: 8.3715,fusion: -717.9531]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  996908.375    \n",
      "module.ans_embedding.weight  dot:  483222.125    \n",
      "module.lstm.weight_ih_l0  dot:  6182375.5    \n",
      "module.lstm.weight_hh_l0  dot:  3632111.0    \n",
      "module.lstm.bias_ih_l0  dot:  203237.75    \n",
      "module.lstm.bias_hh_l0  dot:  203237.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21363662.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4986.3154296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1004335.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1004335.375    \n",
      "module.adapter.frcn_linear.weight  dot:  408193696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  251833.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2757145.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2311.5166015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2983226.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26739772.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  301227.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  55974.859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  456.9211120605469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  417231.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4378261.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  301227.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16.268875122070312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.307978868484497    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  21.291011810302734    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17154952.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28307324.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452538.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  408/6933] Loss: -682.2966 [iq: 11.1394,ans: 10.5631,interp: 9.6434,fusion: -713.6426]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1233943.25    \n",
      "module.ans_embedding.weight  dot:  510246.46875    \n",
      "module.lstm.weight_ih_l0  dot:  10663954.0    \n",
      "module.lstm.weight_hh_l0  dot:  4344099.5    \n",
      "module.lstm.bias_ih_l0  dot:  409292.8125    \n",
      "module.lstm.bias_hh_l0  dot:  409292.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23438788.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17001.2578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1683089.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1683089.0    \n",
      "module.adapter.frcn_linear.weight  dot:  391104960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  257031.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1880683.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1567.942626953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2093660.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4190391084412113e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  28830336.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  339011.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  104690.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1171.16259765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  946015.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4246756.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  339011.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  240.67324829101562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.510292053222656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  831.05908203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15165938.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25074740.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452538.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  409/6933] Loss: -677.4444 [iq: 9.8853,ans: 10.0030,interp: 8.7988,fusion: -706.1315]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  365354.3125    \n",
      "module.ans_embedding.weight  dot:  466856.15625    \n",
      "module.lstm.weight_ih_l0  dot:  4362193.5    \n",
      "module.lstm.weight_hh_l0  dot:  1099492.25    \n",
      "module.lstm.bias_ih_l0  dot:  155712.171875    \n",
      "module.lstm.bias_hh_l0  dot:  155712.171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20652954.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  45376.9140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1149079.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1149079.25    \n",
      "module.adapter.frcn_linear.weight  dot:  284086720.0    \n",
      "module.adapter.frcn_linear.bias  dot:  183131.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2330271.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2310.175048828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2654231.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22704236.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  246397.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22269.478515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  616.4263916015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  97931.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3546958.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  246397.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  192.01385498046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  37.942813873291016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  457.6983642578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17117972.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28619840.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452539.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  410/6933] Loss: -692.9707 [iq: 12.2678,ans: 11.6458,interp: 10.6702,fusion: -727.5545]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1522063.875    \n",
      "module.ans_embedding.weight  dot:  430709.6875    \n",
      "module.lstm.weight_ih_l0  dot:  16783700.0    \n",
      "module.lstm.weight_hh_l0  dot:  5015337.0    \n",
      "module.lstm.bias_ih_l0  dot:  780175.75    \n",
      "module.lstm.bias_hh_l0  dot:  780175.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33199252.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14965.599609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1841149.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1841149.75    \n",
      "module.adapter.frcn_linear.weight  dot:  399586688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  278319.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2794448.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2405.7587890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3008876.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26213030.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  326620.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  40085.0859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  964.4788818359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  247479.390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5010831.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  326620.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  66.0233383178711    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.699996948242188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  137.092529296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.443246102274315e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21242892.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34438000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452540.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  411/6933] Loss: -684.7642 [iq: 12.0477,ans: 10.7014,interp: 11.2525,fusion: -718.7658]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1287343.5    \n",
      "module.ans_embedding.weight  dot:  598568.375    \n",
      "module.lstm.weight_ih_l0  dot:  10806380.0    \n",
      "module.lstm.weight_hh_l0  dot:  4380701.0    \n",
      "module.lstm.bias_ih_l0  dot:  370152.6875    \n",
      "module.lstm.bias_hh_l0  dot:  370152.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28104624.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12987.96875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1949284.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1949284.75    \n",
      "module.adapter.frcn_linear.weight  dot:  457159168.0    \n",
      "module.adapter.frcn_linear.bias  dot:  288843.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  6282678.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  5544.6220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  7551396.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32251206.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  369045.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  85240.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  865.577392578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  698856.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4735435.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  369045.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2349.373046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  550.9034423828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6394.3291015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17025508.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28740344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452541.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  412/6933] Loss: -717.5668 [iq: 8.2376,ans: 9.2023,interp: 7.5464,fusion: -742.5531]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1920203.25    \n",
      "module.ans_embedding.weight  dot:  613653.375    \n",
      "module.lstm.weight_ih_l0  dot:  43783788.0    \n",
      "module.lstm.weight_hh_l0  dot:  8952230.0    \n",
      "module.lstm.bias_ih_l0  dot:  2753684.25    \n",
      "module.lstm.bias_hh_l0  dot:  2753684.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26072328.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29703.958984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2096114.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2096114.25    \n",
      "module.adapter.frcn_linear.weight  dot:  468249024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  303284.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3294588.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2724.75048828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3889195.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  29159032.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  312234.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  174200.90625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1306.775390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1527151.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5156207.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  312234.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2317.7294921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  557.0803833007812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4333.91943359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17060664.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28687600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452541.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  413/6933] Loss: -708.7772 [iq: 10.2599,ans: 10.8963,interp: 9.6026,fusion: -739.5360]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  211493.8125    \n",
      "module.ans_embedding.weight  dot:  864384.125    \n",
      "module.lstm.weight_ih_l0  dot:  5859522.0    \n",
      "module.lstm.weight_hh_l0  dot:  2695858.0    \n",
      "module.lstm.bias_ih_l0  dot:  403690.125    \n",
      "module.lstm.bias_hh_l0  dot:  403690.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46451600.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20695.65625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2713582.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2713582.0    \n",
      "module.adapter.frcn_linear.weight  dot:  548869504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  385121.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  4011276.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  3874.2333984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4819393.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  35271400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  436778.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  32342.673828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  597.0450439453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  259709.703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7470805.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  436778.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  606.2852783203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  139.13015747070312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2223.079833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  24789404.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  39775640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452542.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  414/6933] Loss: -703.3603 [iq: 12.3509,ans: 11.7195,interp: 12.0821,fusion: -739.5128]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1136200.5    \n",
      "module.ans_embedding.weight  dot:  558517.25    \n",
      "module.lstm.weight_ih_l0  dot:  8342025.5    \n",
      "module.lstm.weight_hh_l0  dot:  2899405.5    \n",
      "module.lstm.bias_ih_l0  dot:  133704.3125    \n",
      "module.lstm.bias_hh_l0  dot:  133704.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19672964.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22016.552734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  943271.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  943271.625    \n",
      "module.adapter.frcn_linear.weight  dot:  415849984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  280305.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1824887.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1295.33935546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1871981.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28175464.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  323955.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37432.4296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  736.67333984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  274041.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4534769.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  323955.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4159.90234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  443.9346618652344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4516.3369140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16453489.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28615552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452543.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  415/6933] Loss: -707.9757 [iq: 8.8545,ans: 10.1305,interp: 9.1190,fusion: -736.0797]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1587418.25    \n",
      "module.ans_embedding.weight  dot:  608361.3125    \n",
      "module.lstm.weight_ih_l0  dot:  20234696.0    \n",
      "module.lstm.weight_hh_l0  dot:  2865467.25    \n",
      "module.lstm.bias_ih_l0  dot:  858325.0    \n",
      "module.lstm.bias_hh_l0  dot:  858325.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29986798.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13208.021484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1812450.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1812450.0    \n",
      "module.adapter.frcn_linear.weight  dot:  377642688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  251920.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1782806.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1550.239501953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1918621.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28973886.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  320329.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  74482.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1252.40576171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  506629.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5860424.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  320329.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  170.3250274658203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.446243286132812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  433.28564453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18519036.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  27630602.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452544.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  416/6933] Loss: -698.2322 [iq: 12.5476,ans: 11.2177,interp: 11.6781,fusion: -733.6756]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1206963.375    \n",
      "module.ans_embedding.weight  dot:  334196.0    \n",
      "module.lstm.weight_ih_l0  dot:  68025152.0    \n",
      "module.lstm.weight_hh_l0  dot:  5173991.0    \n",
      "module.lstm.bias_ih_l0  dot:  5069738.0    \n",
      "module.lstm.bias_hh_l0  dot:  5069738.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16673129.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3064.5009765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1025466.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1025466.0    \n",
      "module.adapter.frcn_linear.weight  dot:  490204064.0    \n",
      "module.adapter.frcn_linear.bias  dot:  325508.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1910961.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1687.141845703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2177726.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  38715832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  419566.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17545.97265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  702.7597045898438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50174.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5224823.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  419566.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.4606289267539978    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.13784493505954742    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  0.4270249009132385    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14590135.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22233920.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452545.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  417/6933] Loss: -709.9490 [iq: 9.7208,ans: 9.4322,interp: 8.2761,fusion: -737.3780]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1218424.625    \n",
      "module.ans_embedding.weight  dot:  594167.75    \n",
      "module.lstm.weight_ih_l0  dot:  10819905.0    \n",
      "module.lstm.weight_hh_l0  dot:  3165218.75    \n",
      "module.lstm.bias_ih_l0  dot:  391460.34375    \n",
      "module.lstm.bias_hh_l0  dot:  391460.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22467954.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16155.7939453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1107889.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1107889.25    \n",
      "module.adapter.frcn_linear.weight  dot:  273588352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  172121.203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1904352.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1409.9556884765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2247479.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21563196.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  217354.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  74682.015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  602.4841918945312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  536232.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3307857.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  217354.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  737.4764404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  181.0114288330078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2923.514404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14925650.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20428212.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452546.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  418/6933] Loss: -703.0875 [iq: 11.6034,ans: 9.6391,interp: 8.3961,fusion: -732.7262]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1825387.5    \n",
      "module.ans_embedding.weight  dot:  440223.28125    \n",
      "module.lstm.weight_ih_l0  dot:  31938558.0    \n",
      "module.lstm.weight_hh_l0  dot:  3633027.5    \n",
      "module.lstm.bias_ih_l0  dot:  1946487.5    \n",
      "module.lstm.bias_hh_l0  dot:  1946487.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25873188.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  67619.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2147726.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2147726.5    \n",
      "module.adapter.frcn_linear.weight  dot:  253216800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  170500.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1008399.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  892.7510375976562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  993712.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23048168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241323.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25002.115234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  729.69970703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  81409.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0090737962163985e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2812693.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241323.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2023.4215087890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  319.3408203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6775.89306640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14445450.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25422666.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452547.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  419/6933] Loss: -686.6218 [iq: 12.3844,ans: 10.7207,interp: 9.1499,fusion: -718.8768]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1692275.75    \n",
      "module.ans_embedding.weight  dot:  790488.5    \n",
      "module.lstm.weight_ih_l0  dot:  61105076.0    \n",
      "module.lstm.weight_hh_l0  dot:  4436964.0    \n",
      "module.lstm.bias_ih_l0  dot:  4467013.0    \n",
      "module.lstm.bias_hh_l0  dot:  4467013.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  57322768.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  63139.69140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6364900.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6364900.0    \n",
      "module.adapter.frcn_linear.weight  dot:  404659968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  267706.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1308092.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  996.927978515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1207556.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  34148840.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  390673.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28692.580078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  513.8023071289062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  111004.609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3731834.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  390673.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1165.5338134765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  339.6180114746094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2738.93359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20215198.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42639496.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452548.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  420/6933] Loss: -694.1014 [iq: 12.1778,ans: 10.0390,interp: 9.2372,fusion: -725.5555]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4679194.0    \n",
      "module.ans_embedding.weight  dot:  665592.1875    \n",
      "module.lstm.weight_ih_l0  dot:  82282112.0    \n",
      "module.lstm.weight_hh_l0  dot:  18092788.0    \n",
      "module.lstm.bias_ih_l0  dot:  5473577.0    \n",
      "module.lstm.bias_hh_l0  dot:  5473577.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32769206.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16750.46484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1850100.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1850100.75    \n",
      "module.adapter.frcn_linear.weight  dot:  329912736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  210159.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1657559.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1300.0355224609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1965068.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25964536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  255931.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  165476.65625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  880.57958984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1262000.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5499759.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  255931.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1679.1807861328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  383.37939453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6536.2197265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20769516.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  33523338.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452548.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  421/6933] Loss: -698.9391 [iq: 15.9612,ans: 12.0336,interp: 14.1388,fusion: -741.0728]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1579483.75    \n",
      "module.ans_embedding.weight  dot:  915142.5625    \n",
      "module.lstm.weight_ih_l0  dot:  116088608.0    \n",
      "module.lstm.weight_hh_l0  dot:  11542439.0    \n",
      "module.lstm.bias_ih_l0  dot:  9173320.0    \n",
      "module.lstm.bias_hh_l0  dot:  9173320.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40370168.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  121764.28125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2768411.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2768411.0    \n",
      "module.adapter.frcn_linear.weight  dot:  406813152.0    \n",
      "module.adapter.frcn_linear.bias  dot:  286642.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2096376.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1925.9117431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2248634.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  37465056.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  441248.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  131291.984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  578.49169921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  973433.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5067947717616335e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5801444.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  441248.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2227.65966796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  390.4088134765625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6138.6337890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.112745616817847e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20932642.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  42284260.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452549.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  422/6933] Loss: -698.5793 [iq: 14.3962,ans: 11.5025,interp: 10.7599,fusion: -735.2379]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6484947.0    \n",
      "module.ans_embedding.weight  dot:  592802.5    \n",
      "module.lstm.weight_ih_l0  dot:  175639904.0    \n",
      "module.lstm.weight_hh_l0  dot:  32638898.0    \n",
      "module.lstm.bias_ih_l0  dot:  11402536.0    \n",
      "module.lstm.bias_hh_l0  dot:  11402536.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17561432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14368.04296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  633720.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  633720.625    \n",
      "module.adapter.frcn_linear.weight  dot:  390642432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  270663.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1302414.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  979.2474365234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1184058.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  29899854.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  313801.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  409746.71875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1417.63427734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3335048.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.1125182431424037e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6277776.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  313801.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  299.0299072265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.29542541503906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1224.64306640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15583081.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20613326.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452550.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  423/6933] Loss: -711.9048 [iq: 10.7190,ans: 9.8991,interp: 8.9299,fusion: -741.4528]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2416489.25    \n",
      "module.ans_embedding.weight  dot:  562395.875    \n",
      "module.lstm.weight_ih_l0  dot:  151751616.0    \n",
      "module.lstm.weight_hh_l0  dot:  15411660.0    \n",
      "module.lstm.bias_ih_l0  dot:  11994598.0    \n",
      "module.lstm.bias_hh_l0  dot:  11994598.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19761560.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25258.634765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1004633.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1004633.375    \n",
      "module.adapter.frcn_linear.weight  dot:  326026816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  221705.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1180492.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1092.4072265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1158895.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29532724.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  307603.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  102796.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  792.9442749023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  628714.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4531110.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  307603.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  62.35211181640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.496251106262207    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  111.21672821044922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.188399461644508e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16044014.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  27145852.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452550.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  424/6933] Loss: -704.1060 [iq: 13.2635,ans: 10.8160,interp: 10.7493,fusion: -738.9348]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1864003.625    \n",
      "module.ans_embedding.weight  dot:  514478.0625    \n",
      "module.lstm.weight_ih_l0  dot:  28909836.0    \n",
      "module.lstm.weight_hh_l0  dot:  8602355.0    \n",
      "module.lstm.bias_ih_l0  dot:  1727357.0    \n",
      "module.lstm.bias_hh_l0  dot:  1727357.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19675104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25678.279296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  780762.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  780762.0    \n",
      "module.adapter.frcn_linear.weight  dot:  387184416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  260973.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1221961.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  682.91064453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1088806.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30894400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  303515.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  121870.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  558.349365234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1049973.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4741363.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  303515.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  884.595703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  138.76583862304688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1768.240478515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15901000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22026006.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452551.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  425/6933] Loss: -710.3467 [iq: 9.5372,ans: 9.3162,interp: 8.5012,fusion: -737.7013]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1640652.75    \n",
      "module.ans_embedding.weight  dot:  950932.5625    \n",
      "module.lstm.weight_ih_l0  dot:  30049540.0    \n",
      "module.lstm.weight_hh_l0  dot:  7682960.0    \n",
      "module.lstm.bias_ih_l0  dot:  2069821.75    \n",
      "module.lstm.bias_hh_l0  dot:  2069821.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38167344.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12058.837890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2347397.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2347397.5    \n",
      "module.adapter.frcn_linear.weight  dot:  404198464.0    \n",
      "module.adapter.frcn_linear.bias  dot:  270475.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1423773.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  877.520263671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1309787.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31693058.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  319713.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  89271.59375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  872.7430419921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  808869.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5467865.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  319713.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  332.92034912109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.60453796386719    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1034.6318359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  21349070.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32248700.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452552.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  426/6933] Loss: -737.3248 [iq: 10.2477,ans: 9.8831,interp: 9.1772,fusion: -766.6328]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  913150.375    \n",
      "module.ans_embedding.weight  dot:  668359.875    \n",
      "module.lstm.weight_ih_l0  dot:  5965280.0    \n",
      "module.lstm.weight_hh_l0  dot:  2509195.0    \n",
      "module.lstm.bias_ih_l0  dot:  109693.5625    \n",
      "module.lstm.bias_hh_l0  dot:  109693.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21814744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13405.86328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  974424.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  974424.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  293249536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  199924.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1143892.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  925.85693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1133694.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26403636.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  255345.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25073.455078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  398.2069091796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  245590.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4418986.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  255345.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1681.8905029296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  378.4021301269531    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6850.7919921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16253132.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24247388.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452553.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  427/6933] Loss: -723.7239 [iq: 11.3149,ans: 10.6304,interp: 10.5382,fusion: -756.2075]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  281874.09375    \n",
      "module.ans_embedding.weight  dot:  1058312.125    \n",
      "module.lstm.weight_ih_l0  dot:  4121389.25    \n",
      "module.lstm.weight_hh_l0  dot:  1107248.625    \n",
      "module.lstm.bias_ih_l0  dot:  202452.09375    \n",
      "module.lstm.bias_hh_l0  dot:  202452.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50381328.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  49363.69140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3050642.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3050642.5    \n",
      "module.adapter.frcn_linear.weight  dot:  325921792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  235338.3125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  974840.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  755.871337890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  910029.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28735668.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  299105.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17634.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  434.927001953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  85057.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4406552.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  299105.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  102.38191223144531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.85325050354004    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  345.92401123046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  22795474.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  40638288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452554.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  428/6933] Loss: -701.6646 [iq: 11.5969,ans: 11.1189,interp: 10.4134,fusion: -734.7939]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2578211.0    \n",
      "module.ans_embedding.weight  dot:  931746.0    \n",
      "module.lstm.weight_ih_l0  dot:  37935160.0    \n",
      "module.lstm.weight_hh_l0  dot:  8737517.0    \n",
      "module.lstm.bias_ih_l0  dot:  2272195.0    \n",
      "module.lstm.bias_hh_l0  dot:  2272195.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25370436.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16736.23046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1169645.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1169645.875    \n",
      "module.adapter.frcn_linear.weight  dot:  351220160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  236449.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2016583.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1276.0181884765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1883324.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.980392986908555e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29267568.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  277932.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  103343.4296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  864.46142578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  877291.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5169080.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  277932.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  438.95782470703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.69898986816406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  904.7632446289062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16991268.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26755820.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452554.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  429/6933] Loss: -714.0211 [iq: 12.1905,ans: 11.2360,interp: 11.8943,fusion: -749.3419]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3059379.25    \n",
      "module.ans_embedding.weight  dot:  785064.375    \n",
      "module.lstm.weight_ih_l0  dot:  14698924.0    \n",
      "module.lstm.weight_hh_l0  dot:  3080265.5    \n",
      "module.lstm.bias_ih_l0  dot:  223406.84375    \n",
      "module.lstm.bias_hh_l0  dot:  223406.84375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47204924.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  52608.40234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2646946.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2646946.0    \n",
      "module.adapter.frcn_linear.weight  dot:  358689472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  233289.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3084376.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1949.9971923828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3676618.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  31483414.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  281740.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  57643.69140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  974.4197998046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  346801.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6488372.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  281740.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  807.8990478515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  148.0065155029297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1621.500244140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.118661133314163e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21532672.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26926892.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452555.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  430/6933] Loss: -711.5820 [iq: 11.3238,ans: 11.0792,interp: 10.6837,fusion: -744.6687]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3297313.75    \n",
      "module.ans_embedding.weight  dot:  721006.0    \n",
      "module.lstm.weight_ih_l0  dot:  39581112.0    \n",
      "module.lstm.weight_hh_l0  dot:  7114795.5    \n",
      "module.lstm.bias_ih_l0  dot:  2237337.5    \n",
      "module.lstm.bias_hh_l0  dot:  2237337.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26117060.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3008.1884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1461155.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1461155.75    \n",
      "module.adapter.frcn_linear.weight  dot:  288568192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  184490.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1010885.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  693.9075927734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  965760.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26444324.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  231021.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34361.9375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  549.650146484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  231405.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4199117.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  231021.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  222.31396484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.270381927490234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  447.9178466796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  15721006.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26727300.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452556.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  431/6933] Loss: -714.5981 [iq: 10.7899,ans: 9.5686,interp: 10.0125,fusion: -744.9691]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1931072.0    \n",
      "module.ans_embedding.weight  dot:  847293.0    \n",
      "module.lstm.weight_ih_l0  dot:  35967568.0    \n",
      "module.lstm.weight_hh_l0  dot:  3438323.5    \n",
      "module.lstm.bias_ih_l0  dot:  1803691.5    \n",
      "module.lstm.bias_hh_l0  dot:  1803691.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38934388.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21598.40234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3068703.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3068703.0    \n",
      "module.adapter.frcn_linear.weight  dot:  265969504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  184081.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1408185.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1060.929931640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1164528.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23575912.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  225198.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46620.078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  748.5936279296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  311915.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3670535.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  225198.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  225.4277801513672    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  68.97923278808594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  473.5911865234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16499094.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31009212.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452557.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  432/6933] Loss: -710.0148 [iq: 11.5805,ans: 10.4167,interp: 10.1014,fusion: -742.1134]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  955647.6875    \n",
      "module.ans_embedding.weight  dot:  477748.15625    \n",
      "module.lstm.weight_ih_l0  dot:  46338780.0    \n",
      "module.lstm.weight_hh_l0  dot:  3912906.75    \n",
      "module.lstm.bias_ih_l0  dot:  3222450.5    \n",
      "module.lstm.bias_hh_l0  dot:  3222450.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28906256.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10048.2353515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2570064.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2570064.0    \n",
      "module.adapter.frcn_linear.weight  dot:  426647104.0    \n",
      "module.adapter.frcn_linear.bias  dot:  280459.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2353120.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1621.730712890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2724244.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  33584812.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  323687.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26385.60546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1038.294189453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  90203.2890625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4698896.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  323687.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  155.46295166015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.852874755859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  198.44561767578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15904479.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26557722.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452558.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  433/6933] Loss: -745.2863 [iq: 9.7054,ans: 9.2680,interp: 9.1426,fusion: -773.4022]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4074881.5    \n",
      "module.ans_embedding.weight  dot:  682180.375    \n",
      "module.lstm.weight_ih_l0  dot:  210472992.0    \n",
      "module.lstm.weight_hh_l0  dot:  22011992.0    \n",
      "module.lstm.bias_ih_l0  dot:  15443821.0    \n",
      "module.lstm.bias_hh_l0  dot:  15443821.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20891572.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5165.966796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  848818.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  848818.875    \n",
      "module.adapter.frcn_linear.weight  dot:  534204032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  371760.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1894159.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1469.5516357421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1839916.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  45637544.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  488336.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  206117.421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  914.5499267578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1490070.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7165324.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  488336.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  143.77638244628906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.756874084472656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  231.30712890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.51754214434186e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17108782.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24403056.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452558.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  434/6933] Loss: -745.1240 [iq: 10.3994,ans: 9.8570,interp: 8.8245,fusion: -774.2048]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  544509.4375    \n",
      "module.ans_embedding.weight  dot:  1154469.25    \n",
      "module.lstm.weight_ih_l0  dot:  31303304.0    \n",
      "module.lstm.weight_hh_l0  dot:  3478977.75    \n",
      "module.lstm.bias_ih_l0  dot:  2111747.0    \n",
      "module.lstm.bias_hh_l0  dot:  2111747.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  55743940.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17608.916015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2851475.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2851475.25    \n",
      "module.adapter.frcn_linear.weight  dot:  343712320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  232881.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2242855.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1909.113525390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2414843.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  34505036.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  339974.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17005.044921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  652.1995239257812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  60466.9765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9471102.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  339974.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  181.3511962890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  25.311206817626953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  723.337890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.412217565111007e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  28288082.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31194152.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452559.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  435/6933] Loss: -746.3359 [iq: 10.6107,ans: 10.2811,interp: 10.9300,fusion: -778.1577]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2000368.5    \n",
      "module.ans_embedding.weight  dot:  1137483.5    \n",
      "module.lstm.weight_ih_l0  dot:  30371172.0    \n",
      "module.lstm.weight_hh_l0  dot:  5142699.5    \n",
      "module.lstm.bias_ih_l0  dot:  1909715.625    \n",
      "module.lstm.bias_hh_l0  dot:  1909715.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27284484.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  68746.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1116560.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1116560.5    \n",
      "module.adapter.frcn_linear.weight  dot:  380156608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  265059.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2408865.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1781.48193359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2153216.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4190391084412113e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  30778724.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  299538.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  63106.6640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  822.7263793945312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  500616.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4803179.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  299538.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12506.1435546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1830.268310546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10552.9716796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18226412.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  29879068.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452560.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  436/6933] Loss: -719.5654 [iq: 11.8378,ans: 10.4697,interp: 10.0804,fusion: -751.9533]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1126263.25    \n",
      "module.ans_embedding.weight  dot:  1476537.0    \n",
      "module.lstm.weight_ih_l0  dot:  27530104.0    \n",
      "module.lstm.weight_hh_l0  dot:  2661945.5    \n",
      "module.lstm.bias_ih_l0  dot:  1633191.75    \n",
      "module.lstm.bias_hh_l0  dot:  1633191.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46803436.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  59423.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3331992.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3331992.75    \n",
      "module.adapter.frcn_linear.weight  dot:  260467504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  172096.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  850566.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  610.7288818359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  687910.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.606537787476555e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31476854.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  275103.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18756.20703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  600.9666748046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  57585.55859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3687027.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  275103.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1098.87158203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  278.7231140136719    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3576.736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20559242.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31805176.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452560.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  437/6933] Loss: -710.4040 [iq: 9.7608,ans: 9.5026,interp: 8.5226,fusion: -738.1901]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1141056.125    \n",
      "module.ans_embedding.weight  dot:  784200.125    \n",
      "module.lstm.weight_ih_l0  dot:  65023432.0    \n",
      "module.lstm.weight_hh_l0  dot:  6679681.5    \n",
      "module.lstm.bias_ih_l0  dot:  5129043.0    \n",
      "module.lstm.bias_hh_l0  dot:  5129043.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29465204.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16788.740234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1727558.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1727558.5    \n",
      "module.adapter.frcn_linear.weight  dot:  251087328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  176136.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1539196.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1200.825927734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1526721.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26568500.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241452.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  58334.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  491.51080322265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  406505.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4017582.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241452.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  794.95458984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  168.46969604492188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1659.486083984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.672262990534364e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14124549.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22144366.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452561.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  438/6933] Loss: -727.7452 [iq: 11.3762,ans: 9.8522,interp: 9.5321,fusion: -758.5057]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2689099.25    \n",
      "module.ans_embedding.weight  dot:  873677.875    \n",
      "module.lstm.weight_ih_l0  dot:  60851808.0    \n",
      "module.lstm.weight_hh_l0  dot:  4984290.5    \n",
      "module.lstm.bias_ih_l0  dot:  3643419.0    \n",
      "module.lstm.bias_hh_l0  dot:  3643419.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39599604.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33270.98828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3200683.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3200683.5    \n",
      "module.adapter.frcn_linear.weight  dot:  478721984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  352680.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3044453.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  2491.4189453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3099165.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  46158496.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  446905.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25176.36328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  841.4204711914062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  99748.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6118123.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  446905.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1204.7933349609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  299.1414794921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3953.9599609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.635345149910485e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18176440.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34933436.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452562.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  439/6933] Loss: -712.1191 [iq: 10.5055,ans: 10.3530,interp: 9.7977,fusion: -742.7754]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1543772.375    \n",
      "module.ans_embedding.weight  dot:  735318.3125    \n",
      "module.lstm.weight_ih_l0  dot:  9627780.0    \n",
      "module.lstm.weight_hh_l0  dot:  1707160.875    \n",
      "module.lstm.bias_ih_l0  dot:  136112.984375    \n",
      "module.lstm.bias_hh_l0  dot:  136112.984375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27106714.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  48032.109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1330140.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1330140.375    \n",
      "module.adapter.frcn_linear.weight  dot:  331521792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  232130.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1562322.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1063.7486572265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1250486.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30932278.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  274384.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23773.37109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  581.9979248046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  113433.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5714073.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  274384.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  398.5165100097656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  82.7874984741211    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  533.895751953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.143885234952904e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15404254.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23635610.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452562.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  440/6933] Loss: -730.9041 [iq: 13.5594,ans: 11.2986,interp: 11.4128,fusion: -767.1749]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2884622.0    \n",
      "module.ans_embedding.weight  dot:  818969.25    \n",
      "module.lstm.weight_ih_l0  dot:  81611056.0    \n",
      "module.lstm.weight_hh_l0  dot:  8158786.5    \n",
      "module.lstm.bias_ih_l0  dot:  5011632.0    \n",
      "module.lstm.bias_hh_l0  dot:  5011632.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20654400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1470.7606201171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  977006.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  977006.5    \n",
      "module.adapter.frcn_linear.weight  dot:  227834000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  159100.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  994396.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  562.8037719726562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  808889.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25498502.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  233124.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  40559.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  814.3712158203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  192134.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5052014.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  233124.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8.392970085144043    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.6450283527374268    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37.00532150268555    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14320151.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21870698.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452563.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  441/6933] Loss: -762.1341 [iq: 10.4556,ans: 10.1032,interp: 8.9579,fusion: -791.6508]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3697773.75    \n",
      "module.ans_embedding.weight  dot:  649373.25    \n",
      "module.lstm.weight_ih_l0  dot:  15211385.0    \n",
      "module.lstm.weight_hh_l0  dot:  5134058.5    \n",
      "module.lstm.bias_ih_l0  dot:  147747.71875    \n",
      "module.lstm.bias_hh_l0  dot:  147747.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22342044.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34560.0    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1210194.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1210194.875    \n",
      "module.adapter.frcn_linear.weight  dot:  346857600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  236340.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2080809.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1474.58935546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2137142.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32915916.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  282991.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  62118.29296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  957.6095581054688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  502155.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4744514.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  282991.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  216.52297973632812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.75761413574219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  424.549560546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12017486.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  18622724.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452563.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  442/6933] Loss: -715.6427 [iq: 8.8372,ans: 8.8585,interp: 8.2805,fusion: -741.6188]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8813217.0    \n",
      "module.ans_embedding.weight  dot:  710823.375    \n",
      "module.lstm.weight_ih_l0  dot:  252600176.0    \n",
      "module.lstm.weight_hh_l0  dot:  36530136.0    \n",
      "module.lstm.bias_ih_l0  dot:  16811118.0    \n",
      "module.lstm.bias_hh_l0  dot:  16811118.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20470430.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14343.859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  981380.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  981380.75    \n",
      "module.adapter.frcn_linear.weight  dot:  445917248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  307009.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1683217.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  917.0953369140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1672370.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  43667992.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  380307.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  447839.6875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1179.21875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3830131.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6856795.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  380307.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  368.5013732910156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  107.24569702148438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  497.6998291015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14124586.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22151500.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452564.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  443/6933] Loss: -729.4369 [iq: 10.4101,ans: 9.6370,interp: 8.9967,fusion: -758.4807]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  364488.625    \n",
      "module.ans_embedding.weight  dot:  949301.5    \n",
      "module.lstm.weight_ih_l0  dot:  10892278.0    \n",
      "module.lstm.weight_hh_l0  dot:  2412013.0    \n",
      "module.lstm.bias_ih_l0  dot:  728475.625    \n",
      "module.lstm.bias_hh_l0  dot:  728475.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28769916.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6017.85595703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1102683.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1102683.25    \n",
      "module.adapter.frcn_linear.weight  dot:  255605040.0    \n",
      "module.adapter.frcn_linear.bias  dot:  172647.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1616943.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1234.199951171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1739450.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  29864758.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  250524.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18651.484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  629.4462280273438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  78875.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6259981.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  250524.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  236.25631713867188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  48.99323654174805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1009.351318359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17547256.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25971358.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452565.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  444/6933] Loss: -759.5822 [iq: 8.5012,ans: 8.3853,interp: 7.9124,fusion: -784.3811]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3673503.0    \n",
      "module.ans_embedding.weight  dot:  497254.375    \n",
      "module.lstm.weight_ih_l0  dot:  42256464.0    \n",
      "module.lstm.weight_hh_l0  dot:  11062502.0    \n",
      "module.lstm.bias_ih_l0  dot:  2344575.0    \n",
      "module.lstm.bias_hh_l0  dot:  2344575.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14853955.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20114.83203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  676231.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  676231.0    \n",
      "module.adapter.frcn_linear.weight  dot:  225520640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  142619.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  3191695.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1893.06396484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  4079112.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24070532.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  186663.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  235517.53125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  976.5302124023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2060633.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4996342.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  186663.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  148.1172637939453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.848575592041016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  462.7281494140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.400002915465166e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10625633.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13172269.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452565.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  445/6933] Loss: -741.5770 [iq: 8.5455,ans: 8.1467,interp: 8.3619,fusion: -766.6311]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2280567.0    \n",
      "module.ans_embedding.weight  dot:  786078.5625    \n",
      "module.lstm.weight_ih_l0  dot:  38245164.0    \n",
      "module.lstm.weight_hh_l0  dot:  6170921.5    \n",
      "module.lstm.bias_ih_l0  dot:  2262827.75    \n",
      "module.lstm.bias_hh_l0  dot:  2262827.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28110646.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33909.2578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1425644.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1425644.25    \n",
      "module.adapter.frcn_linear.weight  dot:  327010304.0    \n",
      "module.adapter.frcn_linear.bias  dot:  213410.234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1512726.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1225.5784912109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1558916.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  32036500.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  254151.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  82375.25    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  803.804443359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  741885.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4875720.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  254151.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  92.22630310058594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.588134765625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  136.62501525878906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14994696.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21049450.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452566.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  446/6933] Loss: -744.0724 [iq: 10.8971,ans: 9.8034,interp: 8.8429,fusion: -773.6158]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  283454.5625    \n",
      "module.ans_embedding.weight  dot:  939773.875    \n",
      "module.lstm.weight_ih_l0  dot:  8443694.0    \n",
      "module.lstm.weight_hh_l0  dot:  1104782.625    \n",
      "module.lstm.bias_ih_l0  dot:  559465.75    \n",
      "module.lstm.bias_hh_l0  dot:  559465.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34299400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  79547.8046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2103665.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2103665.25    \n",
      "module.adapter.frcn_linear.weight  dot:  240204880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  165262.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1474241.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1037.798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1430607.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26260504.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  221208.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22436.076171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  675.2850341796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  92968.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4201541.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  221208.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18339.7265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3077.41357421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  18012.0234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15200676.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22248828.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452567.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  447/6933] Loss: -749.9393 [iq: 9.6558,ans: 10.1252,interp: 10.3079,fusion: -780.0283]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1036846.0    \n",
      "module.ans_embedding.weight  dot:  1196491.875    \n",
      "module.lstm.weight_ih_l0  dot:  8978246.0    \n",
      "module.lstm.weight_hh_l0  dot:  2434349.25    \n",
      "module.lstm.bias_ih_l0  dot:  281897.875    \n",
      "module.lstm.bias_hh_l0  dot:  281897.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32791112.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15703.9267578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1481822.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1481822.875    \n",
      "module.adapter.frcn_linear.weight  dot:  298505792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  208482.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1157626.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  986.5232543945312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1008291.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31820036.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  275317.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  48584.140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  970.7635498046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  312608.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6021671.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  275317.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  218.72244262695312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.43342971801758    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  646.5354614257812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17014840.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23409574.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452567.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  448/6933] Loss: -763.8935 [iq: 8.7396,ans: 8.6022,interp: 8.1343,fusion: -789.3696]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1627370.25    \n",
      "module.ans_embedding.weight  dot:  1075910.0    \n",
      "module.lstm.weight_ih_l0  dot:  31425762.0    \n",
      "module.lstm.weight_hh_l0  dot:  4600274.0    \n",
      "module.lstm.bias_ih_l0  dot:  2173380.5    \n",
      "module.lstm.bias_hh_l0  dot:  2173380.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28997136.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8261.3671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1697786.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1697786.75    \n",
      "module.adapter.frcn_linear.weight  dot:  318600384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  222879.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1277474.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  874.138671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1165959.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  31200088.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  265809.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  57741.109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  854.7244873046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  284494.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6579776.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  265809.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  396.85723876953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.49435424804688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1048.939453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17147180.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32104460.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452568.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  449/6933] Loss: -744.7769 [iq: 11.4263,ans: 9.9127,interp: 10.4240,fusion: -776.5398]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2352959.75    \n",
      "module.ans_embedding.weight  dot:  1052868.25    \n",
      "module.lstm.weight_ih_l0  dot:  25203436.0    \n",
      "module.lstm.weight_hh_l0  dot:  7429800.5    \n",
      "module.lstm.bias_ih_l0  dot:  1189744.5    \n",
      "module.lstm.bias_hh_l0  dot:  1189744.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25912468.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14896.576171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1356739.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1356739.0    \n",
      "module.adapter.frcn_linear.weight  dot:  248703952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  163703.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1610553.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1128.7445068359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1611180.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  25355856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  195862.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  176403.75    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1201.843017578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1697214.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5364537.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  195862.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  109.33930969238281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.599956512451172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  242.29800415039062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14173308.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19652452.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452569.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  450/6933] Loss: -764.8802 [iq: 11.7609,ans: 9.6330,interp: 8.2790,fusion: -794.5530]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1275783.375    \n",
      "module.ans_embedding.weight  dot:  1518237.5    \n",
      "module.lstm.weight_ih_l0  dot:  73311488.0    \n",
      "module.lstm.weight_hh_l0  dot:  7473115.5    \n",
      "module.lstm.bias_ih_l0  dot:  5535262.5    \n",
      "module.lstm.bias_hh_l0  dot:  5535262.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  61553764.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14166.7705078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5282097.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5282097.5    \n",
      "module.adapter.frcn_linear.weight  dot:  337923552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  236978.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1135639.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  754.8451538085938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1046367.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  37632792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  318298.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  67999.875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  856.30224609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  347942.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5482946.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  318298.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  241.55874633789062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.73795700073242    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  321.5718078613281    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  21558992.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  36574112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452570.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  451/6933] Loss: -775.9199 [iq: 10.5271,ans: 9.1028,interp: 9.0870,fusion: -804.6368]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5776471.0    \n",
      "module.ans_embedding.weight  dot:  644706.0    \n",
      "module.lstm.weight_ih_l0  dot:  115952944.0    \n",
      "module.lstm.weight_hh_l0  dot:  10560588.0    \n",
      "module.lstm.bias_ih_l0  dot:  6044659.0    \n",
      "module.lstm.bias_hh_l0  dot:  6044659.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40395548.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39760.41015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3354280.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3354280.5    \n",
      "module.adapter.frcn_linear.weight  dot:  345264288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  252096.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1294112.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  831.8960571289062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1212959.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  38282404.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  336520.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  101174.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1181.85546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  840438.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5208382.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  336520.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  133.47996520996094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  29.263978958129883    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  144.90679931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14197103.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23815882.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452571.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  452/6933] Loss: -725.1449 [iq: 11.4268,ans: 9.3934,interp: 9.8241,fusion: -755.7892]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4697500.5    \n",
      "module.ans_embedding.weight  dot:  889770.5625    \n",
      "module.lstm.weight_ih_l0  dot:  63199012.0    \n",
      "module.lstm.weight_hh_l0  dot:  12442514.0    \n",
      "module.lstm.bias_ih_l0  dot:  3477066.5    \n",
      "module.lstm.bias_hh_l0  dot:  3477066.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33543746.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35892.0390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2815900.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2815900.0    \n",
      "module.adapter.frcn_linear.weight  dot:  212227552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  135193.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1190811.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  788.0908203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1126615.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25288188.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  189967.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  93656.21875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1111.793701171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  734021.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5078721.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  189967.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2366.93994140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  558.4437255859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7447.56689453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.352074256530614e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15694164.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  31712986.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452571.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  453/6933] Loss: -729.6428 [iq: 11.9144,ans: 10.3656,interp: 10.9261,fusion: -762.8488]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  12114380.0    \n",
      "module.ans_embedding.weight  dot:  719234.125    \n",
      "module.lstm.weight_ih_l0  dot:  155764000.0    \n",
      "module.lstm.weight_hh_l0  dot:  30678706.0    \n",
      "module.lstm.bias_ih_l0  dot:  9001200.0    \n",
      "module.lstm.bias_hh_l0  dot:  9001200.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17926582.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8027.1201171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  905830.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  905830.125    \n",
      "module.adapter.frcn_linear.weight  dot:  265970880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  173117.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1594914.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1269.55029296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1754061.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  29458320.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  216555.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  290344.5625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1831.9029541015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2496051.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6319922.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  216555.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  309.63165283203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  82.07917022705078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  733.4696655273438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12554292.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19891232.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452572.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  454/6933] Loss: -746.2119 [iq: 10.1708,ans: 9.9639,interp: 9.2489,fusion: -775.5955]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1240024.0    \n",
      "module.ans_embedding.weight  dot:  910008.5    \n",
      "module.lstm.weight_ih_l0  dot:  9303459.0    \n",
      "module.lstm.weight_hh_l0  dot:  2920802.75    \n",
      "module.lstm.bias_ih_l0  dot:  214773.796875    \n",
      "module.lstm.bias_hh_l0  dot:  214773.796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36870920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  59250.71484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2734651.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2734651.5    \n",
      "module.adapter.frcn_linear.weight  dot:  294346144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  188660.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1534670.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1178.5474853515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1563277.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  33907168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  242947.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  91537.5625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  828.3646240234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  790962.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5436242.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  242947.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4867.20263671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  969.2877197265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13421.0400390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16076072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23362762.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452572.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  455/6933] Loss: -746.0906 [iq: 8.5583,ans: 8.9609,interp: 9.1695,fusion: -772.7794]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2006713.375    \n",
      "module.ans_embedding.weight  dot:  915355.6875    \n",
      "module.lstm.weight_ih_l0  dot:  55790536.0    \n",
      "module.lstm.weight_hh_l0  dot:  9840107.0    \n",
      "module.lstm.bias_ih_l0  dot:  3502118.5    \n",
      "module.lstm.bias_hh_l0  dot:  3502118.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26736424.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  42979.65234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1057704.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1057704.75    \n",
      "module.adapter.frcn_linear.weight  dot:  242722640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  169282.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1469186.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  830.573974609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1494841.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27414584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211430.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  154639.15625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  681.7117919921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1475906.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5859540.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211430.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  439.8595886230469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  130.99880981445312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  751.1495971679688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14384250.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17102104.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452573.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  456/6933] Loss: -772.2767 [iq: 8.8450,ans: 8.8535,interp: 8.9408,fusion: -798.9160]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  650886.0    \n",
      "module.ans_embedding.weight  dot:  1015404.25    \n",
      "module.lstm.weight_ih_l0  dot:  6495458.0    \n",
      "module.lstm.weight_hh_l0  dot:  2573443.75    \n",
      "module.lstm.bias_ih_l0  dot:  259436.609375    \n",
      "module.lstm.bias_hh_l0  dot:  259436.609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21539656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71960.21875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  946182.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  946182.0    \n",
      "module.adapter.frcn_linear.weight  dot:  266762656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  170136.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2250725.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1417.5040283203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2273753.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29060742.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211477.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38974.87890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  465.85174560546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  376454.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.02446117834188e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4852320.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211477.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11598.8134765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2275.7587890625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  30193.083984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14063561.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21185804.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452574.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  457/6933] Loss: -760.4559 [iq: 9.1743,ans: 8.5161,interp: 8.5995,fusion: -786.7457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  561619.0    \n",
      "module.ans_embedding.weight  dot:  647218.25    \n",
      "module.lstm.weight_ih_l0  dot:  11353817.0    \n",
      "module.lstm.weight_hh_l0  dot:  1810832.75    \n",
      "module.lstm.bias_ih_l0  dot:  683624.125    \n",
      "module.lstm.bias_hh_l0  dot:  683624.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24726020.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9709.029296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1566684.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1566684.25    \n",
      "module.adapter.frcn_linear.weight  dot:  226137824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  145266.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1493437.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  955.842529296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1683363.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28742850.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  207188.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  43612.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1088.878662109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  186803.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4845567.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  207188.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12.512065887451172    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.200700283050537    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  52.12208557128906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13149667.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  18893594.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452575.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  458/6933] Loss: -766.7545 [iq: 8.5500,ans: 9.1996,interp: 8.7194,fusion: -793.2234]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2452597.25    \n",
      "module.ans_embedding.weight  dot:  666041.875    \n",
      "module.lstm.weight_ih_l0  dot:  12144911.0    \n",
      "module.lstm.weight_hh_l0  dot:  5284254.0    \n",
      "module.lstm.bias_ih_l0  dot:  561223.75    \n",
      "module.lstm.bias_hh_l0  dot:  561223.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30494262.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  164723.71875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2069016.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2069016.875    \n",
      "module.adapter.frcn_linear.weight  dot:  253041760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  173589.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1349468.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  871.5562133789062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1324821.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  28486776.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  205225.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  99365.0    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  850.2986450195312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  962224.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1205884220544249e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5830639.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  205225.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6236.31982421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1231.788818359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  19095.39453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13674029.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16916478.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452576.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  459/6933] Loss: -735.1170 [iq: 8.6235,ans: 8.5030,interp: 8.1611,fusion: -760.4045]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4787028.5    \n",
      "module.ans_embedding.weight  dot:  883461.625    \n",
      "module.lstm.weight_ih_l0  dot:  235792880.0    \n",
      "module.lstm.weight_hh_l0  dot:  21369744.0    \n",
      "module.lstm.bias_ih_l0  dot:  17106914.0    \n",
      "module.lstm.bias_hh_l0  dot:  17106914.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30805132.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11314.36328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1538234.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1538234.5    \n",
      "module.adapter.frcn_linear.weight  dot:  259141376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  170840.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1078001.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  806.6337890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  898050.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.2532413974404335e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  32336244.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  237372.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  105794.6015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  915.0856323242188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  659970.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6713780.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  237372.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  657.5750122070312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  193.08609008789062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1992.88916015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16780216.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22043068.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452576.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  460/6933] Loss: -760.1092 [iq: 11.1946,ans: 10.2668,interp: 10.1587,fusion: -791.7293]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2279884.5    \n",
      "module.ans_embedding.weight  dot:  1023235.125    \n",
      "module.lstm.weight_ih_l0  dot:  10605332.0    \n",
      "module.lstm.weight_hh_l0  dot:  3156294.25    \n",
      "module.lstm.bias_ih_l0  dot:  457899.46875    \n",
      "module.lstm.bias_hh_l0  dot:  457899.46875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35306496.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20785.82421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1851468.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1851468.25    \n",
      "module.adapter.frcn_linear.weight  dot:  208291488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  130456.8203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2583380.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1759.7293701171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2932096.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25843344.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  184057.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  50915.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  904.44921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  429960.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5288605936802924e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5315441.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  184057.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  318.21673583984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.023040771484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  361.54345703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13542605.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22781000.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452577.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  461/6933] Loss: -749.9094 [iq: 11.2024,ans: 9.6131,interp: 9.2423,fusion: -779.9672]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6053277.5    \n",
      "module.ans_embedding.weight  dot:  1062842.75    \n",
      "module.lstm.weight_ih_l0  dot:  62335024.0    \n",
      "module.lstm.weight_hh_l0  dot:  11926478.0    \n",
      "module.lstm.bias_ih_l0  dot:  2989309.0    \n",
      "module.lstm.bias_hh_l0  dot:  2989309.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28113844.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25885.162109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1293115.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1293115.25    \n",
      "module.adapter.frcn_linear.weight  dot:  267223856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  182209.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1410785.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1067.0537109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1442496.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32609496.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  241350.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  95188.5390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1250.85205078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  736562.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5221067.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  241350.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  536.455322265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  151.1181640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1330.0216064453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.1614710021822248e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14632506.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20776680.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452578.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  462/6933] Loss: -766.9371 [iq: 11.3970,ans: 9.7086,interp: 9.3945,fusion: -797.4372]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3324048.75    \n",
      "module.ans_embedding.weight  dot:  904411.25    \n",
      "module.lstm.weight_ih_l0  dot:  56950012.0    \n",
      "module.lstm.weight_hh_l0  dot:  10365609.0    \n",
      "module.lstm.bias_ih_l0  dot:  4004602.75    \n",
      "module.lstm.bias_hh_l0  dot:  4004602.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22810380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12937.853515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1009365.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1009365.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  200737728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  131064.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1585216.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1081.6351318359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1958305.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.5710992203094065e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24948040.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  177189.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  150571.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1483.526611328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1454656.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6174411.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  177189.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  570.331298828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  107.51201629638672    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1181.9317626953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12131727.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  18450658.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452579.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  463/6933] Loss: -774.6205 [iq: 10.3872,ans: 8.9007,interp: 9.2876,fusion: -803.1960]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1185289.25    \n",
      "module.ans_embedding.weight  dot:  1341052.125    \n",
      "module.lstm.weight_ih_l0  dot:  34870852.0    \n",
      "module.lstm.weight_hh_l0  dot:  6690337.0    \n",
      "module.lstm.bias_ih_l0  dot:  2579126.5    \n",
      "module.lstm.bias_hh_l0  dot:  2579126.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39905272.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  49279.17578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1926368.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1926368.125    \n",
      "module.adapter.frcn_linear.weight  dot:  177134720.0    \n",
      "module.adapter.frcn_linear.bias  dot:  116687.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1139384.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  734.0723266601562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  776927.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24891610.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  180363.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23980.8984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  386.2806396484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  134919.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8417267710901797e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7051602.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  180363.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1593.02392578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  317.9053039550781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5575.45654296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  19320730.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28014864.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452579.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  464/6933] Loss: -756.9210 [iq: 13.5858,ans: 10.9148,interp: 9.9166,fusion: -791.3383]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1207369.5    \n",
      "module.ans_embedding.weight  dot:  1263497.0    \n",
      "module.lstm.weight_ih_l0  dot:  15181838.0    \n",
      "module.lstm.weight_hh_l0  dot:  3624205.5    \n",
      "module.lstm.bias_ih_l0  dot:  804870.375    \n",
      "module.lstm.bias_hh_l0  dot:  804870.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26462480.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28361.19921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  848195.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  848195.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  162845808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107436.5078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1010389.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  623.1103515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  837661.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19852468.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  140190.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  61779.796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1073.19677734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  548903.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4665323.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  140190.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3838.27685546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1038.1322021484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9518.8046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13329410.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19785988.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452580.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  465/6933] Loss: -776.4067 [iq: 12.5956,ans: 9.5147,interp: 9.6211,fusion: -808.1381]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3511561.25    \n",
      "module.ans_embedding.weight  dot:  822718.375    \n",
      "module.lstm.weight_ih_l0  dot:  90836576.0    \n",
      "module.lstm.weight_hh_l0  dot:  8122164.0    \n",
      "module.lstm.bias_ih_l0  dot:  6013706.0    \n",
      "module.lstm.bias_hh_l0  dot:  6013706.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20242740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28753.63671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  796711.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  796711.375    \n",
      "module.adapter.frcn_linear.weight  dot:  248128448.0    \n",
      "module.adapter.frcn_linear.bias  dot:  167244.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1540317.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1042.127685546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1395898.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30222020.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  224651.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  51601.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1219.152099609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  221558.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5680276.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  224651.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  139.00180053710938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.02107238769531    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  361.085205078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12837195.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19479430.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452581.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  466/6933] Loss: -771.7310 [iq: 9.6478,ans: 8.8951,interp: 7.9856,fusion: -798.2595]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7818002.5    \n",
      "module.ans_embedding.weight  dot:  781607.4375    \n",
      "module.lstm.weight_ih_l0  dot:  291939360.0    \n",
      "module.lstm.weight_hh_l0  dot:  24294964.0    \n",
      "module.lstm.bias_ih_l0  dot:  22123018.0    \n",
      "module.lstm.bias_hh_l0  dot:  22123018.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21028080.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15197.0859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  589220.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  589220.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  319668320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  223532.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1111260.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  759.8198852539062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  818479.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.412026217120001e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  38958492.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  287063.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  214221.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2868.414794921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1341158.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5945623.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  287063.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  872.1612548828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  149.41744995117188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1957.0130615234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  12860837.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17718008.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452581.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  467/6933] Loss: -775.9149 [iq: 12.0724,ans: 10.2358,interp: 10.7733,fusion: -808.9963]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  525805.0    \n",
      "module.ans_embedding.weight  dot:  1241082.0    \n",
      "module.lstm.weight_ih_l0  dot:  10089164.0    \n",
      "module.lstm.weight_hh_l0  dot:  2591103.0    \n",
      "module.lstm.bias_ih_l0  dot:  584482.375    \n",
      "module.lstm.bias_hh_l0  dot:  584482.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49199704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24977.421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3785617.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3785617.75    \n",
      "module.adapter.frcn_linear.weight  dot:  199938592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  136966.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  817157.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  532.580322265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  727681.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25679370.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  187530.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22833.28515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  639.1771850585938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  89018.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5509360.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  187530.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1279.903076171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  252.88980102539062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3506.8408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  15469800.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25756564.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452582.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  468/6933] Loss: -778.2480 [iq: 9.6343,ans: 9.7486,interp: 11.2380,fusion: -808.8690]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2178962.0    \n",
      "module.ans_embedding.weight  dot:  965444.25    \n",
      "module.lstm.weight_ih_l0  dot:  17737344.0    \n",
      "module.lstm.weight_hh_l0  dot:  5746532.0    \n",
      "module.lstm.bias_ih_l0  dot:  757168.5    \n",
      "module.lstm.bias_hh_l0  dot:  757168.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24448632.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11872.4521484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1232921.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1232921.625    \n",
      "module.adapter.frcn_linear.weight  dot:  296913632.0    \n",
      "module.adapter.frcn_linear.bias  dot:  220834.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  978418.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  702.4635009765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  798027.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.980392986908555e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  38192048.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  293287.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  50721.65625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1203.2637939453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  395129.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6781703.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  293287.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  260.9559326171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.593143463134766    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  382.567138671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14539928.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23240680.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452582.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  469/6933] Loss: -781.5250 [iq: 8.3220,ans: 8.7843,interp: 8.3898,fusion: -807.0211]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1418753.5    \n",
      "module.ans_embedding.weight  dot:  697928.25    \n",
      "module.lstm.weight_ih_l0  dot:  14315314.0    \n",
      "module.lstm.weight_hh_l0  dot:  3041851.5    \n",
      "module.lstm.bias_ih_l0  dot:  356978.625    \n",
      "module.lstm.bias_hh_l0  dot:  356978.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19885760.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4963.7783203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  975409.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  975409.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  214752576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  153361.578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  810004.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  519.3289794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  757710.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27113848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  199077.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25604.064453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  544.418212890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  148658.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5089170.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  199077.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  186.4561767578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.747161865234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  204.05780029296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12599080.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20110682.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452583.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  470/6933] Loss: -792.1226 [iq: 9.1699,ans: 8.0256,interp: 8.7579,fusion: -818.0760]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3802992.5    \n",
      "module.ans_embedding.weight  dot:  1397297.0    \n",
      "module.lstm.weight_ih_l0  dot:  189301936.0    \n",
      "module.lstm.weight_hh_l0  dot:  19292956.0    \n",
      "module.lstm.bias_ih_l0  dot:  12971019.0    \n",
      "module.lstm.bias_hh_l0  dot:  12971019.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37770620.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  45030.0625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2323178.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2323178.25    \n",
      "module.adapter.frcn_linear.weight  dot:  310938048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  229806.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  650524.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  369.62353515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  523706.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  44249328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  338426.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  223520.21875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2623.05224609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1621868.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8001017.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  338426.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4600.9267578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1022.3192138671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  17442.169921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18233740.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34173064.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452584.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  471/6933] Loss: -721.2209 [iq: 13.0731,ans: 11.3821,interp: 12.3014,fusion: -757.9776]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8403991.0    \n",
      "module.ans_embedding.weight  dot:  1129143.0    \n",
      "module.lstm.weight_ih_l0  dot:  232373376.0    \n",
      "module.lstm.weight_hh_l0  dot:  27410528.0    \n",
      "module.lstm.bias_ih_l0  dot:  16026890.0    \n",
      "module.lstm.bias_hh_l0  dot:  16026890.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29669758.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5108.671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1462895.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1462895.25    \n",
      "module.adapter.frcn_linear.weight  dot:  347702208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  235098.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1513959.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  797.24609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1149330.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  44349256.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  327476.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  179159.125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  826.9125366210938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1219220.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.783725333050825e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7495121.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  327476.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  441.65545654296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.0400161743164    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1473.491455078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14681241.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26778316.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452584.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  472/6933] Loss: -742.1281 [iq: 11.5459,ans: 10.4672,interp: 10.3385,fusion: -774.4796]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  244699.53125    \n",
      "module.ans_embedding.weight  dot:  1662499.0    \n",
      "module.lstm.weight_ih_l0  dot:  10145810.0    \n",
      "module.lstm.weight_hh_l0  dot:  1304203.0    \n",
      "module.lstm.bias_ih_l0  dot:  722344.5    \n",
      "module.lstm.bias_hh_l0  dot:  722344.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  78642968.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34274.7890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  7588915.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  7588915.0    \n",
      "module.adapter.frcn_linear.weight  dot:  179381952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  115903.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  977165.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  668.4287109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  861415.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24606220.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  162179.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26296.62890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  807.6472778320312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  149314.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.341224434436299e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4629794.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  162179.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1667.53857421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  445.99005126953125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2174.9560546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  18239764.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  33734792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452585.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  473/6933] Loss: -782.1883 [iq: 9.6419,ans: 8.3049,interp: 8.5628,fusion: -808.6980]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  20174714.0    \n",
      "module.ans_embedding.weight  dot:  981935.75    \n",
      "module.lstm.weight_ih_l0  dot:  316342208.0    \n",
      "module.lstm.weight_hh_l0  dot:  46759760.0    \n",
      "module.lstm.bias_ih_l0  dot:  21488672.0    \n",
      "module.lstm.bias_hh_l0  dot:  21488672.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25527688.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11549.6337890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1282897.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1282897.625    \n",
      "module.adapter.frcn_linear.weight  dot:  473568960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  332474.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2754718.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1664.39697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3046048.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  54416112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  393813.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  497467.9375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2451.448486328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  4324118.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8549854.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  393813.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  163.23788452148438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.318809509277344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  120.20044708251953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12496490.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19895972.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452586.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  474/6933] Loss: -762.0822 [iq: 12.0766,ans: 9.3288,interp: 9.2192,fusion: -792.7068]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  386498.1875    \n",
      "module.ans_embedding.weight  dot:  1404329.375    \n",
      "module.lstm.weight_ih_l0  dot:  10309276.0    \n",
      "module.lstm.weight_hh_l0  dot:  4073341.75    \n",
      "module.lstm.bias_ih_l0  dot:  678030.0    \n",
      "module.lstm.bias_hh_l0  dot:  678030.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45990216.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  86190.2734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2000580.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2000580.75    \n",
      "module.adapter.frcn_linear.weight  dot:  171607456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  113169.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  753692.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  466.5838928222656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  621299.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  26382056.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  170426.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20631.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  562.7371826171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  93821.8515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.609784471336752e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7068386.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  170426.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4132.97900390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  489.1382751464844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15519.7578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  16054014.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19789754.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452586.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  475/6933] Loss: -774.2132 [iq: 12.4189,ans: 10.0544,interp: 10.3806,fusion: -807.0670]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  738874.625    \n",
      "module.ans_embedding.weight  dot:  1015152.8125    \n",
      "module.lstm.weight_ih_l0  dot:  26620520.0    \n",
      "module.lstm.weight_hh_l0  dot:  3962770.5    \n",
      "module.lstm.bias_ih_l0  dot:  1817356.25    \n",
      "module.lstm.bias_hh_l0  dot:  1817356.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28606064.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14799.748046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2259959.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2259959.5    \n",
      "module.adapter.frcn_linear.weight  dot:  193609888.0    \n",
      "module.adapter.frcn_linear.bias  dot:  125097.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1757725.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1081.4234619140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1862035.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25726966.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  165351.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  30949.5    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  916.2725830078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  150691.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7209913494298235e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5450810.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  165351.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  101.5119857788086    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  61.12297439575195    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  138.8318328857422    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11158022.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19925742.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452587.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  476/6933] Loss: -765.5314 [iq: 10.4646,ans: 9.1849,interp: 8.3176,fusion: -793.4986]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3097962.75    \n",
      "module.ans_embedding.weight  dot:  1218284.375    \n",
      "module.lstm.weight_ih_l0  dot:  13116196.0    \n",
      "module.lstm.weight_hh_l0  dot:  3154936.75    \n",
      "module.lstm.bias_ih_l0  dot:  303164.25    \n",
      "module.lstm.bias_hh_l0  dot:  303164.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38688852.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25691.896484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1783833.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1783833.0    \n",
      "module.adapter.frcn_linear.weight  dot:  199216720.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127885.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1008051.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  665.773681640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  907436.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27506274.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  167883.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  42775.59765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1199.2664794921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  250262.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5686294.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  167883.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  323.20751953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  68.96435546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  537.743408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13579206.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23919636.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452587.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  477/6933] Loss: -744.5490 [iq: 11.4935,ans: 9.4423,interp: 9.7615,fusion: -775.2462]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1734668.0    \n",
      "module.ans_embedding.weight  dot:  1237807.5    \n",
      "module.lstm.weight_ih_l0  dot:  23491436.0    \n",
      "module.lstm.weight_hh_l0  dot:  4526763.5    \n",
      "module.lstm.bias_ih_l0  dot:  1451880.75    \n",
      "module.lstm.bias_hh_l0  dot:  1451880.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26507042.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24525.654296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1230214.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1230214.75    \n",
      "module.adapter.frcn_linear.weight  dot:  180292608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  118899.7109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  764089.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  508.7602844238281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  638309.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23742248.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  155211.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  57683.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  455.715576171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  494879.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4818463.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  155211.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3088.048828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  797.73779296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8554.4169921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12061623.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19905020.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452588.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  478/6933] Loss: -764.5989 [iq: 9.8831,ans: 8.7920,interp: 8.5956,fusion: -791.8696]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2563223.5    \n",
      "module.ans_embedding.weight  dot:  1028753.125    \n",
      "module.lstm.weight_ih_l0  dot:  43474896.0    \n",
      "module.lstm.weight_hh_l0  dot:  7787703.0    \n",
      "module.lstm.bias_ih_l0  dot:  2650141.25    \n",
      "module.lstm.bias_hh_l0  dot:  2650141.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40101980.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28931.326171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2292151.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2292151.5    \n",
      "module.adapter.frcn_linear.weight  dot:  190346032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  125122.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1670097.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1210.679931640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1992980.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  25783860.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  161112.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  122032.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  773.4876708984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1171311.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5528544.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  161112.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  385.7259826660156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  110.83100891113281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1231.253662109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12771984.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17980314.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452589.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  479/6933] Loss: -773.4323 [iq: 10.1054,ans: 9.2455,interp: 9.2397,fusion: -802.0228]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  10204022.0    \n",
      "module.ans_embedding.weight  dot:  1138106.875    \n",
      "module.lstm.weight_ih_l0  dot:  208298864.0    \n",
      "module.lstm.weight_hh_l0  dot:  31173770.0    \n",
      "module.lstm.bias_ih_l0  dot:  13408098.0    \n",
      "module.lstm.bias_hh_l0  dot:  13408098.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30318744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15174.638671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1061692.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1061692.875    \n",
      "module.adapter.frcn_linear.weight  dot:  419811648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  298986.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1858662.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1098.39599609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1925737.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  52931560.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  346071.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  375905.09375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1717.3984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  3487732.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.462382877245545e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8673728.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  346071.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  822.3490600585938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  172.80905151367188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2303.12841796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  14634386.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25493796.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452589.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  480/6933] Loss: -756.0935 [iq: 12.1529,ans: 10.6515,interp: 11.7116,fusion: -790.6096]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1751398.25    \n",
      "module.ans_embedding.weight  dot:  2196462.5    \n",
      "module.lstm.weight_ih_l0  dot:  96784848.0    \n",
      "module.lstm.weight_hh_l0  dot:  11666788.0    \n",
      "module.lstm.bias_ih_l0  dot:  6998912.0    \n",
      "module.lstm.bias_hh_l0  dot:  6998912.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  146135728.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  85202.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12867732.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12867732.0    \n",
      "module.adapter.frcn_linear.weight  dot:  210895552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  158034.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  968168.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  603.8798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  778921.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  30799978.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  208297.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44054.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1024.72998046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  291967.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4759613.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  208297.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5860.49365234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1314.81787109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15083.248046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1256418019911507e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  20915000.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  40819144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452590.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  481/6933] Loss: -749.2629 [iq: 8.1872,ans: 8.5332,interp: 8.3204,fusion: -774.3036]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5684254.5    \n",
      "module.ans_embedding.weight  dot:  1073575.5    \n",
      "module.lstm.weight_ih_l0  dot:  102619616.0    \n",
      "module.lstm.weight_hh_l0  dot:  17218824.0    \n",
      "module.lstm.bias_ih_l0  dot:  6438484.0    \n",
      "module.lstm.bias_hh_l0  dot:  6438484.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23259640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30183.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  724636.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  724636.625    \n",
      "module.adapter.frcn_linear.weight  dot:  228582848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  149921.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1491816.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  816.9904174804688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1343892.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30832936.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  188879.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  118819.421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  940.5975952148438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1174014.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6209814.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  188879.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  31951.1015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5248.92626953125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  36158.3046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12910506.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16823376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452591.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  482/6933] Loss: -775.0532 [iq: 9.7383,ans: 9.4561,interp: 12.5060,fusion: -806.7535]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8971926.0    \n",
      "module.ans_embedding.weight  dot:  1080572.25    \n",
      "module.lstm.weight_ih_l0  dot:  90328752.0    \n",
      "module.lstm.weight_hh_l0  dot:  10128402.0    \n",
      "module.lstm.bias_ih_l0  dot:  4646593.0    \n",
      "module.lstm.bias_hh_l0  dot:  4646593.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25167224.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20707.515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1316015.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1316015.625    \n",
      "module.adapter.frcn_linear.weight  dot:  237305664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  161264.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1698013.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1028.471923828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1742095.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33298394.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  211398.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  76583.96875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1821.90625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  474499.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.036295184865594e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6187898.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  211398.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  52.95292282104492    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.48874568939209    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  88.99823760986328    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11445360.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19059716.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452591.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  483/6933] Loss: -750.9541 [iq: 10.8134,ans: 9.6780,interp: 10.1463,fusion: -781.5918]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  273206.25    \n",
      "module.ans_embedding.weight  dot:  989155.625    \n",
      "module.lstm.weight_ih_l0  dot:  9883274.0    \n",
      "module.lstm.weight_hh_l0  dot:  2858850.75    \n",
      "module.lstm.bias_ih_l0  dot:  531815.1875    \n",
      "module.lstm.bias_hh_l0  dot:  531815.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23644936.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  52391.42578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  816584.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  816584.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  191342832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127495.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1528055.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1054.577880859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1442437.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26579388.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  164050.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18346.8359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  451.22821044921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  116704.234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5259307.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  164050.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  921.06494140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  226.7017059326172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1416.2730712890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10649400.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12790864.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452592.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  484/6933] Loss: -778.8770 [iq: 10.7567,ans: 9.3961,interp: 8.5466,fusion: -807.5762]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2964572.0    \n",
      "module.ans_embedding.weight  dot:  1448500.5    \n",
      "module.lstm.weight_ih_l0  dot:  36820872.0    \n",
      "module.lstm.weight_hh_l0  dot:  5585377.0    \n",
      "module.lstm.bias_ih_l0  dot:  2094068.625    \n",
      "module.lstm.bias_hh_l0  dot:  2094068.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47210136.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33009.33984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1895929.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1895929.0    \n",
      "module.adapter.frcn_linear.weight  dot:  143508960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96397.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  938191.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  737.30615234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1027596.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25603400.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  162584.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  69303.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  606.5804443359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  602071.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4757806.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  162584.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  317.3714599609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.77383422851562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  765.2020263671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15418349.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  22664062.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452593.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  485/6933] Loss: -782.1863 [iq: 13.4401,ans: 9.0596,interp: 9.7144,fusion: -814.4005]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3384526.75    \n",
      "module.ans_embedding.weight  dot:  1718024.75    \n",
      "module.lstm.weight_ih_l0  dot:  27334436.0    \n",
      "module.lstm.weight_hh_l0  dot:  5618078.0    \n",
      "module.lstm.bias_ih_l0  dot:  1426559.0    \n",
      "module.lstm.bias_hh_l0  dot:  1426559.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46020440.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11809.0234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3699234.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3699234.0    \n",
      "module.adapter.frcn_linear.weight  dot:  150262256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  91692.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  990172.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  474.6594543457031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  880225.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22973108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  128137.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  118534.59375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1245.5343017578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  949618.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4545279.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  128137.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  22.203876495361328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.671072959899902    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  36.04554748535156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13540599.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26483368.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452593.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  486/6933] Loss: -763.1227 [iq: 12.6344,ans: 9.2501,interp: 9.2071,fusion: -794.2143]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  434064.65625    \n",
      "module.ans_embedding.weight  dot:  1448722.75    \n",
      "module.lstm.weight_ih_l0  dot:  17578426.0    \n",
      "module.lstm.weight_hh_l0  dot:  3385136.0    \n",
      "module.lstm.bias_ih_l0  dot:  1411533.5    \n",
      "module.lstm.bias_hh_l0  dot:  1411533.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51956704.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34252.41015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4027669.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4027669.25    \n",
      "module.adapter.frcn_linear.weight  dot:  223078336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  154396.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1067085.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  830.7572021484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  821829.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30019502.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  193878.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  55268.03125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  847.5615234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  317400.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5272946.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  193878.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  35352.9609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5436.8828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35376.1171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1574829983374002e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15151263.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28575910.0    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452594.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  487/6933] Loss: -765.9350 [iq: 14.3134,ans: 10.4861,interp: 10.2861,fusion: -801.0206]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1470102.25    \n",
      "module.ans_embedding.weight  dot:  1298203.75    \n",
      "module.lstm.weight_ih_l0  dot:  21504968.0    \n",
      "module.lstm.weight_hh_l0  dot:  6347543.5    \n",
      "module.lstm.bias_ih_l0  dot:  1473697.625    \n",
      "module.lstm.bias_hh_l0  dot:  1473697.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23666466.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17069.49609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  959541.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  959541.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  159604432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99139.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1369504.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  721.8372192382812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1463365.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23630688.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  139986.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  80448.90625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  867.534423828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  657473.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4691941.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  139986.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  385.3469543457031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  83.7435531616211    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  665.1328735351562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  11181629.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15494587.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452595.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  488/6933] Loss: -779.6475 [iq: 12.9847,ans: 9.5419,interp: 10.7453,fusion: -812.9195]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1359779.75    \n",
      "module.ans_embedding.weight  dot:  1217335.375    \n",
      "module.lstm.weight_ih_l0  dot:  46907088.0    \n",
      "module.lstm.weight_hh_l0  dot:  5873252.0    \n",
      "module.lstm.bias_ih_l0  dot:  3331746.5    \n",
      "module.lstm.bias_hh_l0  dot:  3331746.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30643832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15074.458984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1814459.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1814459.125    \n",
      "module.adapter.frcn_linear.weight  dot:  169719616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  113255.5546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  875408.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  602.7255249023438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  764625.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24326424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  155811.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  71911.875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1049.4996337890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  643388.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4877914.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  155811.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  507.4298095703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.88898468017578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1141.4443359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11246574.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  18444654.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452596.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  489/6933] Loss: -765.2430 [iq: 13.0606,ans: 9.6319,interp: 10.1329,fusion: -798.0684]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7446404.0    \n",
      "module.ans_embedding.weight  dot:  1013819.5625    \n",
      "module.lstm.weight_ih_l0  dot:  61589328.0    \n",
      "module.lstm.weight_hh_l0  dot:  13177730.0    \n",
      "module.lstm.bias_ih_l0  dot:  2931364.0    \n",
      "module.lstm.bias_hh_l0  dot:  2931364.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22893526.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8996.91796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  882934.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  882934.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  226244208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  144122.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2659493.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1719.21044921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3080140.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2116743164369836e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33108388.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  202524.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  170155.703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1455.730712890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1755363.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6358935.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  202524.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  881.628662109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  157.30657958984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1805.554931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10967812.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16811984.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452596.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  490/6933] Loss: -790.1338 [iq: 8.6537,ans: 7.9444,interp: 8.1732,fusion: -814.9050]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2566479.5    \n",
      "module.ans_embedding.weight  dot:  1884130.875    \n",
      "module.lstm.weight_ih_l0  dot:  24921490.0    \n",
      "module.lstm.weight_hh_l0  dot:  3970014.0    \n",
      "module.lstm.bias_ih_l0  dot:  710182.0    \n",
      "module.lstm.bias_hh_l0  dot:  710182.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  62186512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26133.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6705424.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6705424.0    \n",
      "module.adapter.frcn_linear.weight  dot:  242376208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  182384.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  617924.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  506.9757385253906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  505916.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31742080.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  222656.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  27725.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  568.9886474609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  191193.65625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5661449.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  222656.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  401.4284362792969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  116.72023010253906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  751.9940185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15687331.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  32845724.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452597.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  491/6933] Loss: -755.3730 [iq: 9.2591,ans: 8.6787,interp: 8.7751,fusion: -782.0860]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4739780.5    \n",
      "module.ans_embedding.weight  dot:  2118172.0    \n",
      "module.lstm.weight_ih_l0  dot:  28936002.0    \n",
      "module.lstm.weight_hh_l0  dot:  7376527.5    \n",
      "module.lstm.bias_ih_l0  dot:  1388867.5    \n",
      "module.lstm.bias_hh_l0  dot:  1388867.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  106253560.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5298.9013671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  12010762.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  12010762.0    \n",
      "module.adapter.frcn_linear.weight  dot:  226893536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  160592.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1257705.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  842.8941650390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1100431.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30365128.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  201519.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  113538.5234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1392.7132568359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  943983.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5392278.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  201519.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  171.30197143554688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.517675399780273    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  174.31228637695312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17392498.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  43392376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452597.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  492/6933] Loss: -757.2495 [iq: 8.1704,ans: 8.7516,interp: 9.7631,fusion: -783.9346]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  23053132.0    \n",
      "module.ans_embedding.weight  dot:  1057394.25    \n",
      "module.lstm.weight_ih_l0  dot:  1050877824.0    \n",
      "module.lstm.weight_hh_l0  dot:  101417752.0    \n",
      "module.lstm.bias_ih_l0  dot:  64141344.0    \n",
      "module.lstm.bias_hh_l0  dot:  64141344.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20120544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31528.458984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  887391.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  887391.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  202768384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  138720.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  957928.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  612.0900268554688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  645437.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25193440.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  170786.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  154216.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  862.3255615234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1147090.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5480000.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  170786.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  329.46246337890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  76.54066467285156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  471.5575866699219    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9343655.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16247628.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452598.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  493/6933] Loss: -760.8780 [iq: 10.9189,ans: 10.1795,interp: 10.2318,fusion: -792.2082]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1737619.0    \n",
      "module.ans_embedding.weight  dot:  1079715.5    \n",
      "module.lstm.weight_ih_l0  dot:  8831258.0    \n",
      "module.lstm.weight_hh_l0  dot:  3385960.5    \n",
      "module.lstm.bias_ih_l0  dot:  290745.0    \n",
      "module.lstm.bias_hh_l0  dot:  290745.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25474456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23874.884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  933321.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  933321.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  214667024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  155785.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1099305.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  623.382080078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  839757.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  30632264.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  210008.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  29684.06640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  964.5900268554688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  142535.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6444066.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  210008.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  344.6614990234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  110.745361328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  958.3199462890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12299398.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16953672.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452598.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  494/6933] Loss: -817.5969 [iq: 8.9042,ans: 9.1307,interp: 8.5573,fusion: -844.1891]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  282091.125    \n",
      "module.ans_embedding.weight  dot:  1660388.0    \n",
      "module.lstm.weight_ih_l0  dot:  3435296.0    \n",
      "module.lstm.weight_hh_l0  dot:  1037576.875    \n",
      "module.lstm.bias_ih_l0  dot:  168344.4375    \n",
      "module.lstm.bias_hh_l0  dot:  168344.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42704512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  137478.375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2525690.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2525690.0    \n",
      "module.adapter.frcn_linear.weight  dot:  179651824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  125520.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  782140.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  559.0223388671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  614428.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  25431414.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  168464.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20899.36328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  589.5037841796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  103810.9609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.115907697472721e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4960798.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  168464.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12941.884765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2621.059326171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  21318.53515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13537629.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21871422.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452599.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  495/6933] Loss: -758.5987 [iq: 12.4508,ans: 10.0904,interp: 11.5069,fusion: -792.6468]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  641262.5    \n",
      "module.ans_embedding.weight  dot:  1329324.875    \n",
      "module.lstm.weight_ih_l0  dot:  10599054.0    \n",
      "module.lstm.weight_hh_l0  dot:  4189565.75    \n",
      "module.lstm.bias_ih_l0  dot:  655119.0625    \n",
      "module.lstm.bias_hh_l0  dot:  655119.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50817656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33618.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3585169.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3585169.75    \n",
      "module.adapter.frcn_linear.weight  dot:  197846816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127183.5546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1000604.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  617.2969970703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  823444.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.676156433764845e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26532258.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  161285.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38040.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  594.1220703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  229313.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5727924.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  161285.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  881.1428833007812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  237.70440673828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1226.98095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  14796762.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  27406088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452600.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  496/6933] Loss: -761.1550 [iq: 11.4811,ans: 9.4932,interp: 10.1530,fusion: -792.2823]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4100436.0    \n",
      "module.ans_embedding.weight  dot:  1207035.5    \n",
      "module.lstm.weight_ih_l0  dot:  208706688.0    \n",
      "module.lstm.weight_hh_l0  dot:  18767284.0    \n",
      "module.lstm.bias_ih_l0  dot:  14642823.0    \n",
      "module.lstm.bias_hh_l0  dot:  14642823.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  43893076.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  43333.66796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3339593.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3339593.5    \n",
      "module.adapter.frcn_linear.weight  dot:  233961088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  160477.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1174681.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  675.110595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  897244.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  31523728.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  206105.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  63468.71484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1645.65087890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  407646.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6048348.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  206105.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1695.5654296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  414.0975341796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5388.4970703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13539295.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23983184.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452601.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  497/6933] Loss: -767.0698 [iq: 13.1824,ans: 10.3193,interp: 9.8201,fusion: -800.3915]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4163782.25    \n",
      "module.ans_embedding.weight  dot:  1379024.125    \n",
      "module.lstm.weight_ih_l0  dot:  141849024.0    \n",
      "module.lstm.weight_hh_l0  dot:  11879865.0    \n",
      "module.lstm.bias_ih_l0  dot:  9560624.0    \n",
      "module.lstm.bias_hh_l0  dot:  9560624.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51018764.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25725.427734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6347401.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6347401.0    \n",
      "module.adapter.frcn_linear.weight  dot:  218173952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  149550.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  903724.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  532.5235595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  804438.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  30819884.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  200404.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  75589.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  836.3403930664062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  363870.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4918581.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  200404.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  539.1820068359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  152.15020751953125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  634.6364135742188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12356066.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  28928608.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452601.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  498/6933] Loss: -776.5530 [iq: 10.5949,ans: 9.3797,interp: 8.4016,fusion: -804.9292]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5779824.5    \n",
      "module.ans_embedding.weight  dot:  902330.5625    \n",
      "module.lstm.weight_ih_l0  dot:  59102072.0    \n",
      "module.lstm.weight_hh_l0  dot:  14647580.0    \n",
      "module.lstm.bias_ih_l0  dot:  3218982.75    \n",
      "module.lstm.bias_hh_l0  dot:  3218982.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27779814.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  55348.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1459107.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1459107.875    \n",
      "module.adapter.frcn_linear.weight  dot:  219399456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  151940.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  842805.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  597.0814208984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  851849.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.456524038687348e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  31046494.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  200760.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  178015.015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2237.981201171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1765644.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6952442.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  200760.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1264.426513671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  294.05548095703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2101.6005859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12872744.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21647342.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452601.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  499/6933] Loss: -798.5118 [iq: 9.9506,ans: 9.7328,interp: 9.1326,fusion: -827.3278]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  457870.84375    \n",
      "module.ans_embedding.weight  dot:  1531491.75    \n",
      "module.lstm.weight_ih_l0  dot:  1945851.25    \n",
      "module.lstm.weight_hh_l0  dot:  854161.5625    \n",
      "module.lstm.bias_ih_l0  dot:  90917.015625    \n",
      "module.lstm.bias_hh_l0  dot:  90917.015625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37828892.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  122541.5546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2120517.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2120517.0    \n",
      "module.adapter.frcn_linear.weight  dot:  183452560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  133141.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  621410.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  425.4525451660156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  525212.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26133720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  165283.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19017.421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  674.2816772460938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  130214.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5692917.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  165283.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  453.72845458984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  104.75091552734375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  649.6278076171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14710424.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21183316.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452602.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  500/6933] Loss: -766.4001 [iq: 12.4029,ans: 10.5977,interp: 10.6065,fusion: -800.0072]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4196193.5    \n",
      "module.ans_embedding.weight  dot:  916030.625    \n",
      "module.lstm.weight_ih_l0  dot:  61942328.0    \n",
      "module.lstm.weight_hh_l0  dot:  8268141.0    \n",
      "module.lstm.bias_ih_l0  dot:  3650382.5    \n",
      "module.lstm.bias_hh_l0  dot:  3650382.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23485188.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21904.517578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  713686.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  713686.625    \n",
      "module.adapter.frcn_linear.weight  dot:  215553344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  143656.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1054118.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  563.166259765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  849610.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30600902.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  183050.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  67093.984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  836.1154174804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  528119.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5268598.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  183050.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  595.5782470703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.120262145996094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2205.59375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10425282.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13746211.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452602.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  501/6933] Loss: -761.1597 [iq: 12.1079,ans: 10.3413,interp: 11.8781,fusion: -795.4869]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5987869.0    \n",
      "module.ans_embedding.weight  dot:  1037380.0    \n",
      "module.lstm.weight_ih_l0  dot:  158548304.0    \n",
      "module.lstm.weight_hh_l0  dot:  14437515.0    \n",
      "module.lstm.bias_ih_l0  dot:  9621140.0    \n",
      "module.lstm.bias_hh_l0  dot:  9621140.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17014208.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17446.5546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  684599.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  684599.125    \n",
      "module.adapter.frcn_linear.weight  dot:  219316144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  153262.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  813783.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  538.665771484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  633945.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  31117186.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  201058.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44850.69921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1195.80908203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  211181.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6362922.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  201058.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  80.10785675048828    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.965740203857422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  102.12625122070312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10945125.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16642976.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452603.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  502/6933] Loss: -762.1797 [iq: 11.0928,ans: 10.7092,interp: 10.2829,fusion: -794.2646]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1085644.5    \n",
      "module.ans_embedding.weight  dot:  1201481.5    \n",
      "module.lstm.weight_ih_l0  dot:  7634604.5    \n",
      "module.lstm.weight_hh_l0  dot:  3564541.25    \n",
      "module.lstm.bias_ih_l0  dot:  330062.78125    \n",
      "module.lstm.bias_hh_l0  dot:  330062.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  57703868.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30970.18359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4299736.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4299736.0    \n",
      "module.adapter.frcn_linear.weight  dot:  191309696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  132025.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  845752.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  483.62274169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  669465.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27110306.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  163528.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  46297.93359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  667.9390258789062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  419630.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2116743164369836e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5338016.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  163528.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7174.82470703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1284.138671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  17710.8046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  15018922.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26720300.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452603.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  503/6933] Loss: -793.1208 [iq: 9.7510,ans: 9.4805,interp: 10.1386,fusion: -822.4908]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  13335653.0    \n",
      "module.ans_embedding.weight  dot:  1108879.5    \n",
      "module.lstm.weight_ih_l0  dot:  63921460.0    \n",
      "module.lstm.weight_hh_l0  dot:  9141542.0    \n",
      "module.lstm.bias_ih_l0  dot:  657373.375    \n",
      "module.lstm.bias_hh_l0  dot:  657373.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30609314.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3330.519775390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  999659.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  999659.5    \n",
      "module.adapter.frcn_linear.weight  dot:  167312704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  110320.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1271020.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  820.3180541992188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1117651.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.036295184865594e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25069476.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  149229.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  74819.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1637.5166015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  471643.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6340009.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  149229.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  428.0556335449219    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  85.22100830078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1097.259765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  11293034.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16952448.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452604.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  504/6933] Loss: -783.9056 [iq: 9.8755,ans: 9.5491,interp: 9.8489,fusion: -813.1791]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  264585.15625    \n",
      "module.ans_embedding.weight  dot:  1343646.0    \n",
      "module.lstm.weight_ih_l0  dot:  2738629.5    \n",
      "module.lstm.weight_hh_l0  dot:  3488032.5    \n",
      "module.lstm.bias_ih_l0  dot:  142458.21875    \n",
      "module.lstm.bias_hh_l0  dot:  142458.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41609612.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  48172.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3083422.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3083422.75    \n",
      "module.adapter.frcn_linear.weight  dot:  190167424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  127302.859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2072350.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1389.1337890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1644426.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  28725566.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  177242.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37236.48046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1192.957763671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  200923.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6626133.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  177242.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1682.50537109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  423.1224365234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3636.79248046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.6867397195928788e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13376008.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24555144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452605.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  505/6933] Loss: -787.4468 [iq: 10.4789,ans: 9.9393,interp: 10.5094,fusion: -818.3745]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1824316.25    \n",
      "module.ans_embedding.weight  dot:  1167302.875    \n",
      "module.lstm.weight_ih_l0  dot:  10013972.0    \n",
      "module.lstm.weight_hh_l0  dot:  4304322.0    \n",
      "module.lstm.bias_ih_l0  dot:  252799.046875    \n",
      "module.lstm.bias_hh_l0  dot:  252799.046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25353860.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  124082.4296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2378625.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2378625.0    \n",
      "module.adapter.frcn_linear.weight  dot:  146242672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  100525.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  763626.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  619.2127075195312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  816594.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23221244.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  146460.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  76420.09375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1146.4610595703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  582449.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5489030.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  146460.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2298.04833984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  294.38751220703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2752.143798828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.4141578453272814e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10270390.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15781065.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452605.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  506/6933] Loss: -807.7077 [iq: 11.1780,ans: 10.0436,interp: 9.4219,fusion: -838.3513]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1902705.25    \n",
      "module.ans_embedding.weight  dot:  1282999.75    \n",
      "module.lstm.weight_ih_l0  dot:  24515016.0    \n",
      "module.lstm.weight_hh_l0  dot:  5454676.0    \n",
      "module.lstm.bias_ih_l0  dot:  1473431.125    \n",
      "module.lstm.bias_hh_l0  dot:  1473431.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24345656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  50843.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  800453.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  800453.0    \n",
      "module.adapter.frcn_linear.weight  dot:  107900656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  64394.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  996694.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  610.2188720703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1278715.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18133778.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99847.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  69973.8125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  584.6757202148438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  620416.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3933352.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99847.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3266.6669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  650.6264038085938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5063.67578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.2878590395266656e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10522935.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14655632.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452606.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  507/6933] Loss: -787.1838 [iq: 11.2590,ans: 9.3942,interp: 9.2625,fusion: -817.0995]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2060812.625    \n",
      "module.ans_embedding.weight  dot:  944221.0    \n",
      "module.lstm.weight_ih_l0  dot:  8415854.0    \n",
      "module.lstm.weight_hh_l0  dot:  3243903.0    \n",
      "module.lstm.bias_ih_l0  dot:  236645.9375    \n",
      "module.lstm.bias_hh_l0  dot:  236645.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23090152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8853.404296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1437067.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1437067.0    \n",
      "module.adapter.frcn_linear.weight  dot:  200526112.0    \n",
      "module.adapter.frcn_linear.bias  dot:  141978.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1160174.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  774.0787353515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  981804.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31135808.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  183517.140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  81174.515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1416.8228759765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  690426.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.71482053399086e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5951496.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  183517.140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  450.9536437988281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  155.7806396484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  561.3785400390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11213578.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15911776.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452607.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  508/6933] Loss: -814.8959 [iq: 9.6984,ans: 8.3552,interp: 8.2103,fusion: -841.1599]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2921228.25    \n",
      "module.ans_embedding.weight  dot:  1876847.5    \n",
      "module.lstm.weight_ih_l0  dot:  17066044.0    \n",
      "module.lstm.weight_hh_l0  dot:  4183354.0    \n",
      "module.lstm.bias_ih_l0  dot:  217968.75    \n",
      "module.lstm.bias_hh_l0  dot:  217968.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28685490.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33940.3515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  841034.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  841034.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  129830896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  79163.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1156734.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  840.387451171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1475623.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21759648.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  118439.5703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  43599.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  862.5126953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  465444.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4443415.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  118439.5703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1160.9774169921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  420.6790466308594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1282.903076171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.491895578771164e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11168486.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17548060.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452608.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  509/6933] Loss: -765.4323 [iq: 13.6387,ans: 10.1893,interp: 11.0852,fusion: -800.3455]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3246351.0    \n",
      "module.ans_embedding.weight  dot:  958126.9375    \n",
      "module.lstm.weight_ih_l0  dot:  118989080.0    \n",
      "module.lstm.weight_hh_l0  dot:  14292825.0    \n",
      "module.lstm.bias_ih_l0  dot:  8000624.5    \n",
      "module.lstm.bias_hh_l0  dot:  8000624.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20225248.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22989.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  626437.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  626437.875    \n",
      "module.adapter.frcn_linear.weight  dot:  150357072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101455.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1153908.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  736.6224365234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1152524.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26441140.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  154774.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  68746.25    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  950.5457763671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  444717.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.859825715655461e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5897604.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  154774.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1343.8060302734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  289.45306396484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1848.519775390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9785199.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13009307.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452608.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  510/6933] Loss: -807.6108 [iq: 11.4395,ans: 9.7020,interp: 9.1264,fusion: -837.8786]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4985383.0    \n",
      "module.ans_embedding.weight  dot:  1578943.375    \n",
      "module.lstm.weight_ih_l0  dot:  113052704.0    \n",
      "module.lstm.weight_hh_l0  dot:  19345592.0    \n",
      "module.lstm.bias_ih_l0  dot:  6937213.5    \n",
      "module.lstm.bias_hh_l0  dot:  6937213.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44561152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9359.1884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4250382.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4250382.5    \n",
      "module.adapter.frcn_linear.weight  dot:  192038960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119309.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1079575.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  666.2180786132812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  948710.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29798864.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  162736.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  155687.96875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2097.9404296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1567119.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4602986.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  162736.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  394.9576110839844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  109.66024780273438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  603.21337890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.535394613318203e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12328320.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21271656.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452609.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  511/6933] Loss: -767.5863 [iq: 10.7148,ans: 8.6582,interp: 8.6031,fusion: -795.5624]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  747614.0625    \n",
      "module.ans_embedding.weight  dot:  932961.1875    \n",
      "module.lstm.weight_ih_l0  dot:  3759124.5    \n",
      "module.lstm.weight_hh_l0  dot:  1841650.75    \n",
      "module.lstm.bias_ih_l0  dot:  158798.21875    \n",
      "module.lstm.bias_hh_l0  dot:  158798.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16876824.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23942.419921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  563432.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  563432.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  139151424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  97931.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  756724.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  501.66802978515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  699707.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.494019544334151e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25594064.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  156369.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44178.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1237.314208984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  429704.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5483255.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  156369.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  478.71282958984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  91.07672119140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1666.7476806640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9388013.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12713107.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452609.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  512/6933] Loss: -822.0184 [iq: 10.7974,ans: 9.4639,interp: 9.5493,fusion: -851.8289]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7295977.0    \n",
      "module.ans_embedding.weight  dot:  1504756.5    \n",
      "module.lstm.weight_ih_l0  dot:  138845248.0    \n",
      "module.lstm.weight_hh_l0  dot:  20100888.0    \n",
      "module.lstm.bias_ih_l0  dot:  9219811.0    \n",
      "module.lstm.bias_hh_l0  dot:  9219811.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48832232.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  124368.6328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3891078.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3891078.5    \n",
      "module.adapter.frcn_linear.weight  dot:  197280832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  139208.359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  949262.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  643.3812255859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1089925.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33010252.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  196643.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  213784.8125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  893.840576171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2252063.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.954948286060244e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6919718.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  196643.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1492.37744140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  382.03875732421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2733.9560546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12831028.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20987322.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452610.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  513/6933] Loss: -794.3669 [iq: 10.3964,ans: 9.0639,interp: 9.0027,fusion: -822.8298]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2185024.25    \n",
      "module.ans_embedding.weight  dot:  1087880.625    \n",
      "module.lstm.weight_ih_l0  dot:  9844454.0    \n",
      "module.lstm.weight_hh_l0  dot:  1535747.0    \n",
      "module.lstm.bias_ih_l0  dot:  371158.5625    \n",
      "module.lstm.bias_hh_l0  dot:  371158.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25604352.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13946.115234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  950615.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  950615.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  173677024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  126614.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  708115.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  552.2222900390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  646369.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.822151484200731e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27685348.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  174053.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24220.44140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  440.75872802734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172070.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7320417100563645e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5432528.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  174053.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  414.6306457519531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  102.7493896484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  322.85626220703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.668487818548783e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11637320.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14930460.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452611.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  514/6933] Loss: -811.8947 [iq: 9.8993,ans: 8.6109,interp: 9.7612,fusion: -840.1660]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  446646.4375    \n",
      "module.ans_embedding.weight  dot:  959668.375    \n",
      "module.lstm.weight_ih_l0  dot:  3162089.25    \n",
      "module.lstm.weight_hh_l0  dot:  1237143.25    \n",
      "module.lstm.bias_ih_l0  dot:  100409.3125    \n",
      "module.lstm.bias_hh_l0  dot:  100409.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24079680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6671.400390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1527910.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1527910.25    \n",
      "module.adapter.frcn_linear.weight  dot:  157278592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102519.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  860043.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  486.68157958984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  743270.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26034160.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  153282.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53029.3046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1620.2633056640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  362748.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6070418.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  153282.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1306.08935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  264.98785400390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3685.1630859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  10830557.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19327218.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452612.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  515/6933] Loss: -810.7844 [iq: 9.0718,ans: 8.7088,interp: 9.0854,fusion: -837.6504]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1507975.875    \n",
      "module.ans_embedding.weight  dot:  1367006.125    \n",
      "module.lstm.weight_ih_l0  dot:  18820242.0    \n",
      "module.lstm.weight_hh_l0  dot:  5468317.0    \n",
      "module.lstm.bias_ih_l0  dot:  1153219.5    \n",
      "module.lstm.bias_hh_l0  dot:  1153219.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32684736.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40068.9921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2524352.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2524352.5    \n",
      "module.adapter.frcn_linear.weight  dot:  135666544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92074.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  842090.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  488.13995361328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  741698.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22625062.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136189.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  52871.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  870.436279296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  482761.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5553164.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136189.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1255.8443603515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  300.0451965332031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4231.60107421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10649419.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  18235032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452612.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  516/6933] Loss: -796.8281 [iq: 9.4878,ans: 8.3958,interp: 8.6193,fusion: -823.3311]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1921907.5    \n",
      "module.ans_embedding.weight  dot:  1092516.625    \n",
      "module.lstm.weight_ih_l0  dot:  22158026.0    \n",
      "module.lstm.weight_hh_l0  dot:  5842325.0    \n",
      "module.lstm.bias_ih_l0  dot:  1149701.75    \n",
      "module.lstm.bias_hh_l0  dot:  1149701.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20550592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  46380.5390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  559534.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  559534.375    \n",
      "module.adapter.frcn_linear.weight  dot:  178040960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  134104.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  888534.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  714.7515869140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  792047.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28191788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  175486.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  59085.73046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1285.9937744140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  533859.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6576528.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  175486.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  498.9903869628906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  94.10799407958984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  598.5496215820312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10669573.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14899459.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452613.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  517/6933] Loss: -821.3694 [iq: 10.7272,ans: 9.1021,interp: 8.7655,fusion: -849.9642]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3261378.25    \n",
      "module.ans_embedding.weight  dot:  1173451.25    \n",
      "module.lstm.weight_ih_l0  dot:  70028400.0    \n",
      "module.lstm.weight_hh_l0  dot:  8962903.0    \n",
      "module.lstm.bias_ih_l0  dot:  3851256.5    \n",
      "module.lstm.bias_hh_l0  dot:  3851256.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32458324.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21135.9453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2196540.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2196540.0    \n",
      "module.adapter.frcn_linear.weight  dot:  203381808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  138708.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1152493.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  702.180908203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1030918.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33193460.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  186741.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  29906.65234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  667.6466674804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  215904.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7027833.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  186741.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1094.8021240234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  205.62063598632812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3016.07470703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11989454.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17147280.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452614.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  518/6933] Loss: -771.8230 [iq: 12.9788,ans: 9.5583,interp: 12.4647,fusion: -806.8248]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5893762.5    \n",
      "module.ans_embedding.weight  dot:  1328375.5    \n",
      "module.lstm.weight_ih_l0  dot:  69793392.0    \n",
      "module.lstm.weight_hh_l0  dot:  12371840.0    \n",
      "module.lstm.bias_ih_l0  dot:  3627491.25    \n",
      "module.lstm.bias_hh_l0  dot:  3627491.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25823592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18233.8203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  948946.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  948946.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  181284016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  123322.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1263160.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  850.824951171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1281151.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32755664.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  188027.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  162306.1875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1065.683349609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1522407.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6621074.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  188027.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  273.42938232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.84934997558594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  411.8144226074219    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11190406.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15610748.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452614.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  519/6933] Loss: -806.9862 [iq: 9.5350,ans: 8.8412,interp: 9.4114,fusion: -834.7737]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8286755.0    \n",
      "module.ans_embedding.weight  dot:  1259751.375    \n",
      "module.lstm.weight_ih_l0  dot:  32277350.0    \n",
      "module.lstm.weight_hh_l0  dot:  7188491.5    \n",
      "module.lstm.bias_ih_l0  dot:  275500.71875    \n",
      "module.lstm.bias_hh_l0  dot:  275500.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26149424.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  93967.84375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1018127.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1018127.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  161957360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107179.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1072416.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  739.4411010742188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1190316.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6621584109088872e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27958928.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160770.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  110831.09375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1431.3377685546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  927873.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0206804290646687e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5888271.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160770.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2312.618408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  506.7449951171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10900.50390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10499414.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17497900.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452615.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  520/6933] Loss: -771.3069 [iq: 10.8576,ans: 9.0282,interp: 9.4508,fusion: -800.6436]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2085253.75    \n",
      "module.ans_embedding.weight  dot:  864685.875    \n",
      "module.lstm.weight_ih_l0  dot:  23767776.0    \n",
      "module.lstm.weight_hh_l0  dot:  5459363.0    \n",
      "module.lstm.bias_ih_l0  dot:  1137603.0    \n",
      "module.lstm.bias_hh_l0  dot:  1137603.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19699192.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41251.015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  537008.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  537008.875    \n",
      "module.adapter.frcn_linear.weight  dot:  146633648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  97557.9453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  945475.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  653.3641357421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  922599.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21705582.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  123109.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  72166.671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2229.8154296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  726588.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4835039.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  123109.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1831.433349609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  435.8138427734375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4043.41748046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8987287.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12245483.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452616.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  521/6933] Loss: -791.6849 [iq: 9.5310,ans: 8.7378,interp: 8.7835,fusion: -818.7371]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  417425.9375    \n",
      "module.ans_embedding.weight  dot:  1379459.25    \n",
      "module.lstm.weight_ih_l0  dot:  7757907.0    \n",
      "module.lstm.weight_hh_l0  dot:  3282512.5    \n",
      "module.lstm.bias_ih_l0  dot:  487985.9375    \n",
      "module.lstm.bias_hh_l0  dot:  487985.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33213736.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39993.48046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1179568.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1179568.0    \n",
      "module.adapter.frcn_linear.weight  dot:  147394064.0    \n",
      "module.adapter.frcn_linear.bias  dot:  94865.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1088491.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  659.9374389648438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  859070.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24151346.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133059.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24019.8046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  840.5244750976562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  159956.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5999796.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133059.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2170.421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  333.149658203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8457.267578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11691856.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17909304.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452616.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  522/6933] Loss: -762.8308 [iq: 13.0906,ans: 10.5611,interp: 11.2970,fusion: -797.7795]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  614581.8125    \n",
      "module.ans_embedding.weight  dot:  1274339.5    \n",
      "module.lstm.weight_ih_l0  dot:  7597009.0    \n",
      "module.lstm.weight_hh_l0  dot:  1695982.125    \n",
      "module.lstm.bias_ih_l0  dot:  392664.46875    \n",
      "module.lstm.bias_hh_l0  dot:  392664.46875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23951680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11432.1357421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  647480.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  647480.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  178736944.0    \n",
      "module.adapter.frcn_linear.bias  dot:  116403.1015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1070249.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  590.4990844726562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1020605.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25294120.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  143256.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38471.51953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1148.026611328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  399107.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5246357.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  143256.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1535.135986328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  254.9421844482422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5770.8095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3888446776254568e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9975096.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15288052.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452617.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  523/6933] Loss: -814.5839 [iq: 9.4703,ans: 8.3247,interp: 9.4040,fusion: -841.7829]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1200628.125    \n",
      "module.ans_embedding.weight  dot:  1063612.875    \n",
      "module.lstm.weight_ih_l0  dot:  6159940.0    \n",
      "module.lstm.weight_hh_l0  dot:  2861660.0    \n",
      "module.lstm.bias_ih_l0  dot:  249335.5    \n",
      "module.lstm.bias_hh_l0  dot:  249335.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28458744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  27893.865234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1840288.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1840288.25    \n",
      "module.adapter.frcn_linear.weight  dot:  180662032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119935.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  859655.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  407.3001403808594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  886829.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28457348.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  162602.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35426.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  716.6314086914062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  340909.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4441937.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  162602.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  862.4896850585938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  147.8194580078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4162.18408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10612806.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14727998.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452617.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  524/6933] Loss: -800.3502 [iq: 9.9185,ans: 8.5696,interp: 9.2785,fusion: -828.1168]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  952733.0    \n",
      "module.ans_embedding.weight  dot:  1572994.75    \n",
      "module.lstm.weight_ih_l0  dot:  15405487.0    \n",
      "module.lstm.weight_hh_l0  dot:  3116059.5    \n",
      "module.lstm.bias_ih_l0  dot:  906647.25    \n",
      "module.lstm.bias_hh_l0  dot:  906647.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47120600.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  195047.4375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3616815.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3616815.0    \n",
      "module.adapter.frcn_linear.weight  dot:  157472176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  107616.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  796491.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  489.3234558105469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  580752.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25027756.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  145110.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  39995.82421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  703.8977661132812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  417200.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4522009.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  145110.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11604.935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2572.70849609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  44197.0859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11809106.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19220304.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452618.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  525/6933] Loss: -792.3929 [iq: 9.7053,ans: 8.8340,interp: 9.0831,fusion: -820.0153]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11847938.0    \n",
      "module.ans_embedding.weight  dot:  1642237.5    \n",
      "module.lstm.weight_ih_l0  dot:  93649680.0    \n",
      "module.lstm.weight_hh_l0  dot:  15131568.0    \n",
      "module.lstm.bias_ih_l0  dot:  4326374.5    \n",
      "module.lstm.bias_hh_l0  dot:  4326374.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31646630.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20902.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1539517.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1539517.0    \n",
      "module.adapter.frcn_linear.weight  dot:  203653088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  136978.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1190661.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  631.2162475585938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1039604.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  33321304.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  191215.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  212589.9375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  2279.7548828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2128883.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6347128.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  191215.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1319.634521484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  323.5724792480469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3291.98095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11513896.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  21264424.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452619.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  526/6933] Loss: -777.5429 [iq: 10.6521,ans: 8.1534,interp: 8.1917,fusion: -804.5401]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  10850821.0    \n",
      "module.ans_embedding.weight  dot:  1351090.375    \n",
      "module.lstm.weight_ih_l0  dot:  178623424.0    \n",
      "module.lstm.weight_hh_l0  dot:  18835772.0    \n",
      "module.lstm.bias_ih_l0  dot:  10340348.0    \n",
      "module.lstm.bias_hh_l0  dot:  10340348.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21696536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20782.66796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  615674.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  615674.375    \n",
      "module.adapter.frcn_linear.weight  dot:  130943352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  89683.265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  636799.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  410.9731750488281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  531939.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.112745616817847e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23114990.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137790.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33733.671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  748.6956787109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  195376.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.584762791637331e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4362872.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137790.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  369.8486328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.19316101074219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  676.55322265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8992087.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12829358.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452620.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  527/6933] Loss: -792.1409 [iq: 11.7430,ans: 8.9320,interp: 8.9606,fusion: -821.7766]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  315134.15625    \n",
      "module.ans_embedding.weight  dot:  1079614.0    \n",
      "module.lstm.weight_ih_l0  dot:  8294782.0    \n",
      "module.lstm.weight_hh_l0  dot:  2042627.25    \n",
      "module.lstm.bias_ih_l0  dot:  613307.9375    \n",
      "module.lstm.bias_hh_l0  dot:  613307.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32955254.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21583.62109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3040309.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3040309.5    \n",
      "module.adapter.frcn_linear.weight  dot:  151781264.0    \n",
      "module.adapter.frcn_linear.bias  dot:  103919.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1048339.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  690.0477294921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1187316.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  23043104.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  131405.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  39120.91015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1474.170654296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  346655.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5066794.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  131405.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  352.20391845703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.12054443359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  569.02392578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8860959.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16335955.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452620.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  528/6933] Loss: -830.9077 [iq: 9.7455,ans: 8.3089,interp: 7.7613,fusion: -856.7234]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  474275.875    \n",
      "module.ans_embedding.weight  dot:  985247.875    \n",
      "module.lstm.weight_ih_l0  dot:  18278024.0    \n",
      "module.lstm.weight_hh_l0  dot:  3216150.5    \n",
      "module.lstm.bias_ih_l0  dot:  1316704.875    \n",
      "module.lstm.bias_hh_l0  dot:  1316704.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35581696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28426.67578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2557381.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2557381.0    \n",
      "module.adapter.frcn_linear.weight  dot:  120755360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80977.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1170448.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  624.0436401367188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1072676.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20925424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124645.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35763.47265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  924.9063720703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  267769.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5520072.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124645.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  453.23138427734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  92.22187805175781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  842.4547119140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11294094.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17003592.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452621.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  529/6933] Loss: -840.9318 [iq: 8.5644,ans: 7.6716,interp: 7.8967,fusion: -865.0645]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1516299.625    \n",
      "module.ans_embedding.weight  dot:  1237190.0    \n",
      "module.lstm.weight_ih_l0  dot:  15744033.0    \n",
      "module.lstm.weight_hh_l0  dot:  4500441.0    \n",
      "module.lstm.bias_ih_l0  dot:  757850.9375    \n",
      "module.lstm.bias_hh_l0  dot:  757850.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24833882.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23069.640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1928736.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1928736.75    \n",
      "module.adapter.frcn_linear.weight  dot:  148041920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96564.1953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  705920.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  427.7443542480469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  617506.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.954948286060244e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22873800.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  126465.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  58070.0625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1598.990478515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  620929.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5210034.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  126465.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  577.9031372070312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  170.5677032470703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1021.40185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8696170.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14807772.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452621.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  530/6933] Loss: -805.3969 [iq: 8.7513,ans: 7.9044,interp: 8.4436,fusion: -830.4962]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1353948.625    \n",
      "module.ans_embedding.weight  dot:  1259271.0    \n",
      "module.lstm.weight_ih_l0  dot:  18120392.0    \n",
      "module.lstm.weight_hh_l0  dot:  4503700.0    \n",
      "module.lstm.bias_ih_l0  dot:  949920.875    \n",
      "module.lstm.bias_hh_l0  dot:  949920.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29539020.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  70660.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2156240.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2156240.25    \n",
      "module.adapter.frcn_linear.weight  dot:  144156832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  98592.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  625681.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  448.0556945800781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  579758.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23901448.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  137757.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  63747.23046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  829.0186767578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  700117.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5217920.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  137757.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3051.07666015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  581.565185546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8547.205078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11148010.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17505540.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452622.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  531/6933] Loss: -827.5744 [iq: 9.3603,ans: 8.8071,interp: 8.7854,fusion: -854.5272]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  452963.25    \n",
      "module.ans_embedding.weight  dot:  1536395.25    \n",
      "module.lstm.weight_ih_l0  dot:  13015166.0    \n",
      "module.lstm.weight_hh_l0  dot:  4549964.0    \n",
      "module.lstm.bias_ih_l0  dot:  819752.875    \n",
      "module.lstm.bias_hh_l0  dot:  819752.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36057920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17545.79296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1500891.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1500891.25    \n",
      "module.adapter.frcn_linear.weight  dot:  191576864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  129346.6640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  954500.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  562.9532470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  717796.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  30778454.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  182890.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  30345.908203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  602.1893310546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  198293.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6303327.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  182890.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1476.306884765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  217.35841369628906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4554.404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13703652.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24044164.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452622.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  532/6933] Loss: -760.3278 [iq: 10.4177,ans: 9.4591,interp: 9.9453,fusion: -790.1498]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1251386.875    \n",
      "module.ans_embedding.weight  dot:  1024421.0625    \n",
      "module.lstm.weight_ih_l0  dot:  52633904.0    \n",
      "module.lstm.weight_hh_l0  dot:  6977409.0    \n",
      "module.lstm.bias_ih_l0  dot:  3519192.5    \n",
      "module.lstm.bias_hh_l0  dot:  3519192.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20561696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41710.80078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  839170.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  839170.5    \n",
      "module.adapter.frcn_linear.weight  dot:  177442352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  126210.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  832283.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  479.8752136230469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  598964.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26660600.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  160785.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  50128.23046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1269.299072265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  383003.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7111901.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  160785.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1115.115478515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  231.6607666015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4402.07666015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9960640.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12594003.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452623.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  533/6933] Loss: -805.9995 [iq: 10.8935,ans: 9.6940,interp: 11.6113,fusion: -838.1983]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  439398.875    \n",
      "module.ans_embedding.weight  dot:  2156175.0    \n",
      "module.lstm.weight_ih_l0  dot:  11198055.0    \n",
      "module.lstm.weight_hh_l0  dot:  1713760.375    \n",
      "module.lstm.bias_ih_l0  dot:  773721.25    \n",
      "module.lstm.bias_hh_l0  dot:  773721.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  56480584.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23225.630859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6945353.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6945353.0    \n",
      "module.adapter.frcn_linear.weight  dot:  129382664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  85508.9609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  602578.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  442.58642578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  492567.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23640320.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133430.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20766.07421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  596.163330078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  167035.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5477834.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133430.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  817.630615234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  218.60279846191406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1066.1893310546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13970118.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  36476096.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452624.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  534/6933] Loss: -792.0414 [iq: 9.8976,ans: 8.3585,interp: 8.7615,fusion: -819.0590]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  784132.8125    \n",
      "module.ans_embedding.weight  dot:  1890075.5    \n",
      "module.lstm.weight_ih_l0  dot:  7839532.0    \n",
      "module.lstm.weight_hh_l0  dot:  3945149.0    \n",
      "module.lstm.bias_ih_l0  dot:  362579.5    \n",
      "module.lstm.bias_hh_l0  dot:  362579.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  69014784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  95523.8984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3354915.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3354915.75    \n",
      "module.adapter.frcn_linear.weight  dot:  169452608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  119005.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  811274.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  631.96484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  632394.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26588716.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  154949.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  43525.19921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1131.5938720703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  401945.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5360787.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  154949.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  792.7083740234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  168.26898193359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1075.353515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14401410.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20861824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452624.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  535/6933] Loss: -789.8673 [iq: 12.3687,ans: 9.1661,interp: 9.5041,fusion: -820.9062]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4876040.5    \n",
      "module.ans_embedding.weight  dot:  1066130.75    \n",
      "module.lstm.weight_ih_l0  dot:  147068512.0    \n",
      "module.lstm.weight_hh_l0  dot:  12938473.0    \n",
      "module.lstm.bias_ih_l0  dot:  8706757.0    \n",
      "module.lstm.bias_hh_l0  dot:  8706757.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37403736.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  67475.8984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1859962.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1859962.75    \n",
      "module.adapter.frcn_linear.weight  dot:  133572576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  92609.6796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  716918.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  607.372802734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  569484.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24551268.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  140846.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44740.4296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1123.207275390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  337404.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4787802.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  140846.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  31286.818359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2923.540771484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35872.1328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5475620784854982e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10566699.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14339090.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452625.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  536/6933] Loss: -803.0502 [iq: 11.7469,ans: 9.1380,interp: 8.2971,fusion: -832.2323]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  594100.25    \n",
      "module.ans_embedding.weight  dot:  939655.0    \n",
      "module.lstm.weight_ih_l0  dot:  6780300.5    \n",
      "module.lstm.weight_hh_l0  dot:  1404476.875    \n",
      "module.lstm.bias_ih_l0  dot:  464434.9375    \n",
      "module.lstm.bias_hh_l0  dot:  464434.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24089260.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  179905.421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1447286.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1447286.25    \n",
      "module.adapter.frcn_linear.weight  dot:  109576680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  71840.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  564101.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  416.7503662109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  481694.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19819080.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  110679.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21953.12890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  724.5555419921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  235641.296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4574547.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  110679.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1321.76220703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  211.35498046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4265.49462890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9206249.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11499552.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452626.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  537/6933] Loss: -843.0356 [iq: 10.7739,ans: 8.4634,interp: 8.9489,fusion: -871.2219]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  361199.09375    \n",
      "module.ans_embedding.weight  dot:  1781028.375    \n",
      "module.lstm.weight_ih_l0  dot:  13968492.0    \n",
      "module.lstm.weight_hh_l0  dot:  7300433.5    \n",
      "module.lstm.bias_ih_l0  dot:  1061279.25    \n",
      "module.lstm.bias_hh_l0  dot:  1061279.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  106803696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  66513.515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10516848.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10516848.0    \n",
      "module.adapter.frcn_linear.weight  dot:  156461232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111913.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  575679.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  405.64508056640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  449454.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.412207322777249e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  28205378.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  164211.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31943.611328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  832.8922119140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  250818.234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4100214.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  164211.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2347.474609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  521.3163452148438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6176.3125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  14948448.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  29928600.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452626.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  538/6933] Loss: -792.8283 [iq: 8.8375,ans: 8.0307,interp: 7.6944,fusion: -817.3910]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  557352.625    \n",
      "module.ans_embedding.weight  dot:  1483190.75    \n",
      "module.lstm.weight_ih_l0  dot:  8369650.5    \n",
      "module.lstm.weight_hh_l0  dot:  1844287.5    \n",
      "module.lstm.bias_ih_l0  dot:  449753.65625    \n",
      "module.lstm.bias_hh_l0  dot:  449753.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23297280.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39112.359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  804835.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  804835.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  105376648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  70067.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  789364.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  404.489501953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  647203.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18147432.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  100019.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25466.037109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  486.7756042480469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  289400.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3962786.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  100019.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  751.650634765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  127.79328918457031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1375.708984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.714384355646416e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8586444.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13333136.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452627.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  539/6933] Loss: -795.7589 [iq: 11.5330,ans: 9.7035,interp: 10.1523,fusion: -827.1477]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  425285.6875    \n",
      "module.ans_embedding.weight  dot:  1127562.125    \n",
      "module.lstm.weight_ih_l0  dot:  10764786.0    \n",
      "module.lstm.weight_hh_l0  dot:  5441485.0    \n",
      "module.lstm.bias_ih_l0  dot:  744552.5    \n",
      "module.lstm.bias_hh_l0  dot:  744552.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19180368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13448.3857421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1036794.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1036794.625    \n",
      "module.adapter.frcn_linear.weight  dot:  117741192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  81143.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  672058.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  415.0443420410156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  627911.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.460574463242665e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23465788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  129179.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16079.935546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  434.883056640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  132325.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2960867934452835e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6676126.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  129179.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  483.963134765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  116.03350830078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  536.36572265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8925074.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12146436.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452627.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  540/6933] Loss: -781.4190 [iq: 9.7705,ans: 9.0636,interp: 9.8210,fusion: -810.0741]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  824400.5    \n",
      "module.ans_embedding.weight  dot:  2089940.375    \n",
      "module.lstm.weight_ih_l0  dot:  2357221.5    \n",
      "module.lstm.weight_hh_l0  dot:  1502406.0    \n",
      "module.lstm.bias_ih_l0  dot:  105275.046875    \n",
      "module.lstm.bias_hh_l0  dot:  105275.046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44440000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  107402.265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5638494.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5638494.0    \n",
      "module.adapter.frcn_linear.weight  dot:  137026736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  90280.359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  657686.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  380.0323181152344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  472519.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.022684490540996e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23543152.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  128035.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21924.33203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  578.434814453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  200688.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4390573.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  128035.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16276.5283203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2668.6591796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  43324.15234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11631953.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  26095336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452628.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  541/6933] Loss: -796.6021 [iq: 8.4083,ans: 7.4926,interp: 7.9815,fusion: -820.4844]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  340860.3125    \n",
      "module.ans_embedding.weight  dot:  1220183.125    \n",
      "module.lstm.weight_ih_l0  dot:  13534506.0    \n",
      "module.lstm.weight_hh_l0  dot:  3236299.5    \n",
      "module.lstm.bias_ih_l0  dot:  1029686.625    \n",
      "module.lstm.bias_hh_l0  dot:  1029686.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36866768.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22618.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2432299.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2432299.5    \n",
      "module.adapter.frcn_linear.weight  dot:  108722808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  75593.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  619141.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  452.58245849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  464031.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21438280.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  120970.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20630.849609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  589.7323608398438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  162282.453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4725725.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  120970.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2995.456298828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  497.0284423828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7046.1181640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  9434110.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14183234.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452628.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  542/6933] Loss: -834.3464 [iq: 8.6435,ans: 8.2921,interp: 8.8731,fusion: -860.1550]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  515588.40625    \n",
      "module.ans_embedding.weight  dot:  1000748.6875    \n",
      "module.lstm.weight_ih_l0  dot:  22533764.0    \n",
      "module.lstm.weight_hh_l0  dot:  5776543.5    \n",
      "module.lstm.bias_ih_l0  dot:  1640429.875    \n",
      "module.lstm.bias_hh_l0  dot:  1640429.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30566540.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5228.732421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2464434.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2464434.0    \n",
      "module.adapter.frcn_linear.weight  dot:  121453016.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83880.578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  518146.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  344.91143798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  424241.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.649564289138652e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23626856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  126681.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  48706.99609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1984.1513671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  488307.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3194388631964102e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5609480.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  126681.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.242932319641113    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.337181568145752    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.436325073242188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.007016857736744e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10888364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17341284.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452629.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  543/6933] Loss: -836.1661 [iq: 10.4740,ans: 8.6824,interp: 8.1848,fusion: -863.5073]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  826478.5625    \n",
      "module.ans_embedding.weight  dot:  1091279.75    \n",
      "module.lstm.weight_ih_l0  dot:  7313216.0    \n",
      "module.lstm.weight_hh_l0  dot:  2131269.25    \n",
      "module.lstm.bias_ih_l0  dot:  308949.9375    \n",
      "module.lstm.bias_hh_l0  dot:  308949.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32374256.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  50205.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3757475.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3757475.5    \n",
      "module.adapter.frcn_linear.weight  dot:  120592432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  77084.3828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1131984.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  752.911865234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1461804.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22536578.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  111670.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  41406.48828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1162.419189453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  517465.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4865636.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  111670.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9301.3076171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1699.05615234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  34040.09375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  10123632.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19004676.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452629.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  544/6933] Loss: -846.9442 [iq: 9.4280,ans: 8.2260,interp: 7.8275,fusion: -872.4257]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  9179699.0    \n",
      "module.ans_embedding.weight  dot:  1172073.625    \n",
      "module.lstm.weight_ih_l0  dot:  149568912.0    \n",
      "module.lstm.weight_hh_l0  dot:  26501982.0    \n",
      "module.lstm.bias_ih_l0  dot:  9544454.0    \n",
      "module.lstm.bias_hh_l0  dot:  9544454.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25474992.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35173.0078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1397634.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1397634.25    \n",
      "module.adapter.frcn_linear.weight  dot:  145962432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102724.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  783639.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  486.4576110839844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  704532.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27193752.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  145954.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  120571.5234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  581.646728515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1025090.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5547706172801554e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8193175.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  145954.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  914.0743408203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  198.6156005859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2606.56005859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9334150.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13082258.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452630.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  545/6933] Loss: -763.0457 [iq: 13.0993,ans: 10.0255,interp: 9.5879,fusion: -795.7584]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  15538806.0    \n",
      "module.ans_embedding.weight  dot:  1272304.625    \n",
      "module.lstm.weight_ih_l0  dot:  557037824.0    \n",
      "module.lstm.weight_hh_l0  dot:  49825388.0    \n",
      "module.lstm.bias_ih_l0  dot:  32792690.0    \n",
      "module.lstm.bias_hh_l0  dot:  32792690.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24963764.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19287.693359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  994970.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  994970.75    \n",
      "module.adapter.frcn_linear.weight  dot:  144981072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99584.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  761616.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  402.02886962890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  644805.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  26493728.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141461.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  125673.515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1148.8807373046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1073856.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.005503236228833e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6890683.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141461.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  164.24154663085938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.353515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  396.8832092285156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  10502064.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14595683.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452630.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  546/6933] Loss: -822.1923 [iq: 10.5489,ans: 9.2638,interp: 8.2860,fusion: -850.2910]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  624405.625    \n",
      "module.ans_embedding.weight  dot:  1218823.75    \n",
      "module.lstm.weight_ih_l0  dot:  15024972.0    \n",
      "module.lstm.weight_hh_l0  dot:  4785288.0    \n",
      "module.lstm.bias_ih_l0  dot:  1108392.375    \n",
      "module.lstm.bias_hh_l0  dot:  1108392.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22033640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12502.185546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1254648.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1254648.875    \n",
      "module.adapter.frcn_linear.weight  dot:  107416512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  68731.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  672826.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.98956298828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  523654.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20864806.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  104794.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34642.859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  800.16162109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  257273.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.082721716258675e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5822133.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  104794.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  196.00173950195312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  37.01441955566406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  482.752685546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8145758.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15489464.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452631.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  547/6933] Loss: -813.3806 [iq: 11.4564,ans: 9.4714,interp: 9.1427,fusion: -843.4511]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  373502.875    \n",
      "module.ans_embedding.weight  dot:  1361551.0    \n",
      "module.lstm.weight_ih_l0  dot:  2660146.75    \n",
      "module.lstm.weight_hh_l0  dot:  1143905.375    \n",
      "module.lstm.bias_ih_l0  dot:  110768.484375    \n",
      "module.lstm.bias_hh_l0  dot:  110768.484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  45068932.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2098.722412109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2661806.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2661806.5    \n",
      "module.adapter.frcn_linear.weight  dot:  109864504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  73762.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  617803.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.802734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  478760.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20484036.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  101874.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33583.890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  884.7529907226562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  394124.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4086187.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  101874.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10.860199928283691    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.982477605342865    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  52.00877380371094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.733102798581967e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8951599.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13893312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452631.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  548/6933] Loss: -820.9753 [iq: 9.1014,ans: 8.2374,interp: 8.3539,fusion: -846.6682]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1334391.75    \n",
      "module.ans_embedding.weight  dot:  1407259.25    \n",
      "module.lstm.weight_ih_l0  dot:  17705478.0    \n",
      "module.lstm.weight_hh_l0  dot:  3318929.5    \n",
      "module.lstm.bias_ih_l0  dot:  931054.875    \n",
      "module.lstm.bias_hh_l0  dot:  931054.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26120298.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17664.00390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1862445.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1862445.0    \n",
      "module.adapter.frcn_linear.weight  dot:  143813136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  101901.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  610344.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.44696044921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  501904.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0206804290646687e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26146388.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136068.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  36275.11328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  635.06201171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  396724.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.496097633615136e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5475639.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136068.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  151.53338623046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.440814971923828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  376.65606689453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10284566.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16864932.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452632.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  549/6933] Loss: -809.3731 [iq: 8.0132,ans: 8.1390,interp: 8.4564,fusion: -833.9816]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  9038713.0    \n",
      "module.ans_embedding.weight  dot:  1287262.125    \n",
      "module.lstm.weight_ih_l0  dot:  139082432.0    \n",
      "module.lstm.weight_hh_l0  dot:  16955212.0    \n",
      "module.lstm.bias_ih_l0  dot:  8448103.0    \n",
      "module.lstm.bias_hh_l0  dot:  8448103.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18022170.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14408.302734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  529254.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  529254.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  183617056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  126044.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1290096.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  696.8724365234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1217668.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31476492.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  157927.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  162878.6875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  924.7205810546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1710172.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5452315.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  157927.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  55.86518859863281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.3404340744018555    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  100.71818542480469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7633242.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10323112.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452632.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  550/6933] Loss: -799.1180 [iq: 9.7313,ans: 8.7663,interp: 10.7601,fusion: -828.3757]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  409670.9375    \n",
      "module.ans_embedding.weight  dot:  1843364.25    \n",
      "module.lstm.weight_ih_l0  dot:  6375911.0    \n",
      "module.lstm.weight_hh_l0  dot:  3606706.5    \n",
      "module.lstm.bias_ih_l0  dot:  396944.0625    \n",
      "module.lstm.bias_hh_l0  dot:  396944.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54731024.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  50741.078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4436312.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4436312.0    \n",
      "module.adapter.frcn_linear.weight  dot:  151242368.0    \n",
      "module.adapter.frcn_linear.bias  dot:  104184.28125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  666988.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  412.45343017578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  587019.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.7853275241795927e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  33368052.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  168180.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25795.369140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  745.2996215820312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  281046.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7854779.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  168180.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1710.6263427734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  511.1650390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2776.87158203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13522217.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  20332816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452633.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  551/6933] Loss: -786.5781 [iq: 10.5303,ans: 9.7626,interp: 11.8007,fusion: -818.6717]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1592789.125    \n",
      "module.ans_embedding.weight  dot:  1220780.0    \n",
      "module.lstm.weight_ih_l0  dot:  18779132.0    \n",
      "module.lstm.weight_hh_l0  dot:  3497743.75    \n",
      "module.lstm.bias_ih_l0  dot:  961219.125    \n",
      "module.lstm.bias_hh_l0  dot:  961219.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27470418.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23708.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  935555.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  935555.125    \n",
      "module.adapter.frcn_linear.weight  dot:  137061088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96861.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  957523.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  531.1143188476562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1200437.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29713902.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  144925.34375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  66484.8046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1411.8856201171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  859330.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7092023.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  144925.34375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1363.6973876953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  329.2620849609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5575.732421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9949508.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14927388.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452633.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  552/6933] Loss: -808.6486 [iq: 12.0008,ans: 9.4808,interp: 10.1602,fusion: -840.2905]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7144825.5    \n",
      "module.ans_embedding.weight  dot:  990837.3125    \n",
      "module.lstm.weight_ih_l0  dot:  91207104.0    \n",
      "module.lstm.weight_hh_l0  dot:  13566507.0    \n",
      "module.lstm.bias_ih_l0  dot:  5301604.5    \n",
      "module.lstm.bias_hh_l0  dot:  5301604.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36118036.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33940.359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2155846.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2155846.0    \n",
      "module.adapter.frcn_linear.weight  dot:  174052704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  116910.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1457564.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  835.485595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1116628.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  29835970.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  147859.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  202644.9375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1990.0390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2119930.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.992157194763422e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7091891.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  147859.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1962.3922119140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  409.9640197753906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4503.2060546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.447464453325665e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10324574.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12361299.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452634.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  553/6933] Loss: -810.8406 [iq: 13.0070,ans: 9.4715,interp: 9.8493,fusion: -843.1685]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4557395.5    \n",
      "module.ans_embedding.weight  dot:  1279070.75    \n",
      "module.lstm.weight_ih_l0  dot:  103598688.0    \n",
      "module.lstm.weight_hh_l0  dot:  13338550.0    \n",
      "module.lstm.bias_ih_l0  dot:  6441283.0    \n",
      "module.lstm.bias_hh_l0  dot:  6441283.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26817180.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36125.3203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  891745.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  891745.0    \n",
      "module.adapter.frcn_linear.weight  dot:  141892480.0    \n",
      "module.adapter.frcn_linear.bias  dot:  102986.265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  898373.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  601.8170166015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  780860.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  28246564.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  144415.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  122665.0625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1150.65869140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1299864.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5492433.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  144415.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1899.254150390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  530.3584594726562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2777.4326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5475620784854982e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7819461.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9533326.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452634.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  554/6933] Loss: -834.4385 [iq: 12.3813,ans: 8.4637,interp: 9.5438,fusion: -864.8274]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5942471.5    \n",
      "module.ans_embedding.weight  dot:  1528970.25    \n",
      "module.lstm.weight_ih_l0  dot:  68064256.0    \n",
      "module.lstm.weight_hh_l0  dot:  11289053.0    \n",
      "module.lstm.bias_ih_l0  dot:  3854758.0    \n",
      "module.lstm.bias_hh_l0  dot:  3854758.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24416224.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12374.6162109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  566659.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  566659.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  152272176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  99315.953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1027326.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  539.481689453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  974213.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.494019544334151e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  28030906.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  132357.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  105565.6953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1398.06640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1105317.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5338161.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  132357.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  139.08456420898438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.293697357177734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  250.90049743652344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7504706.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10387692.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452635.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  555/6933] Loss: -807.2782 [iq: 10.3607,ans: 7.5830,interp: 9.1976,fusion: -834.4194]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  880516.75    \n",
      "module.ans_embedding.weight  dot:  1083147.375    \n",
      "module.lstm.weight_ih_l0  dot:  3813255.75    \n",
      "module.lstm.weight_hh_l0  dot:  1608957.625    \n",
      "module.lstm.bias_ih_l0  dot:  118673.7421875    \n",
      "module.lstm.bias_hh_l0  dot:  118673.7421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18980888.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  106955.078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  692236.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  692236.625    \n",
      "module.adapter.frcn_linear.weight  dot:  147440336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  95890.8203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1211987.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  669.744140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1165031.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  28264042.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  136146.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28032.46875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  737.3148193359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  327487.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5248271.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  136146.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2063.786376953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  417.1429443359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4018.44189453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9386031.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12617514.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452636.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  556/6933] Loss: -799.5399 [iq: 10.2300,ans: 8.6039,interp: 8.4599,fusion: -826.8337]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3655786.75    \n",
      "module.ans_embedding.weight  dot:  2279620.5    \n",
      "module.lstm.weight_ih_l0  dot:  92156088.0    \n",
      "module.lstm.weight_hh_l0  dot:  9659798.0    \n",
      "module.lstm.bias_ih_l0  dot:  5688866.5    \n",
      "module.lstm.bias_hh_l0  dot:  5688866.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  127797928.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71037.8828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10543660.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10543660.0    \n",
      "module.adapter.frcn_linear.weight  dot:  120737648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80269.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  725851.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  502.517578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  797232.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.8466972657479346e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26351244.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  126769.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37355.44140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1373.5438232421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  343908.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.204139258945361e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5409871.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  126769.1953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6447.3994140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1854.67578125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  19841.93359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  17819640.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  34050584.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452636.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  557/6933] Loss: -782.7960 [iq: 9.8412,ans: 8.9228,interp: 9.1213,fusion: -810.6813]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  516378.78125    \n",
      "module.ans_embedding.weight  dot:  982375.0    \n",
      "module.lstm.weight_ih_l0  dot:  5336974.5    \n",
      "module.lstm.weight_hh_l0  dot:  3003764.5    \n",
      "module.lstm.bias_ih_l0  dot:  261605.875    \n",
      "module.lstm.bias_hh_l0  dot:  261605.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19911286.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19982.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  638733.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  638733.5    \n",
      "module.adapter.frcn_linear.weight  dot:  126613360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88331.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  665766.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  401.60711669921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  549540.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.2532413974404335e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26689708.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  132436.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38611.58984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  987.935791015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  480515.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6264030.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  132436.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  794.0233764648438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  162.40379333496094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4984.0927734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8999999.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14274875.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452637.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  558/6933] Loss: -819.2584 [iq: 10.4139,ans: 9.8548,interp: 11.2612,fusion: -850.7883]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  297490.0625    \n",
      "module.ans_embedding.weight  dot:  1518213.75    \n",
      "module.lstm.weight_ih_l0  dot:  5486915.5    \n",
      "module.lstm.weight_hh_l0  dot:  1981153.0    \n",
      "module.lstm.bias_ih_l0  dot:  354544.625    \n",
      "module.lstm.bias_hh_l0  dot:  354544.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29843298.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11148.615234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  955805.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  955805.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  112901200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83082.375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  621866.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  364.2247009277344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  521913.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  23173316.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  124101.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15716.8623046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  441.4540100097656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  140942.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.142974153684918e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5883656.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  124101.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1668.98486328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  392.9317321777344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4174.40185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3165024626005106e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9965626.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16094200.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452637.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  559/6933] Loss: -788.3686 [iq: 10.8472,ans: 8.9431,interp: 11.5356,fusion: -819.6945]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  919523.5625    \n",
      "module.ans_embedding.weight  dot:  1669327.875    \n",
      "module.lstm.weight_ih_l0  dot:  4676632.0    \n",
      "module.lstm.weight_hh_l0  dot:  2798791.5    \n",
      "module.lstm.bias_ih_l0  dot:  174957.71875    \n",
      "module.lstm.bias_hh_l0  dot:  174957.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  53106508.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36631.265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  6807039.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  6807039.0    \n",
      "module.adapter.frcn_linear.weight  dot:  118571800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83514.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  714730.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  444.191650390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  883533.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24628264.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  125128.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22548.58984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  410.83184814453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  234056.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.007016857736744e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3992329.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  125128.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3130.37841796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  667.0640869140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7865.7041015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11138880.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24358868.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452638.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  560/6933] Loss: -793.7800 [iq: 8.0776,ans: 7.4274,interp: 7.2722,fusion: -816.5571]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  448795.0    \n",
      "module.ans_embedding.weight  dot:  1226374.125    \n",
      "module.lstm.weight_ih_l0  dot:  5023891.0    \n",
      "module.lstm.weight_hh_l0  dot:  1929092.75    \n",
      "module.lstm.bias_ih_l0  dot:  309484.53125    \n",
      "module.lstm.bias_hh_l0  dot:  309484.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22537806.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14636.49609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1296133.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1296133.375    \n",
      "module.adapter.frcn_linear.weight  dot:  160343712.0    \n",
      "module.adapter.frcn_linear.bias  dot:  109139.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  773674.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  452.691162109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  601497.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  29949468.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  144379.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33922.54296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  771.166259765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  372108.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7072889.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  144379.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  753.4320068359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  169.39535522460938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2933.697265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3743007560028673e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8896324.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15377084.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452639.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  561/6933] Loss: -833.8613 [iq: 9.0685,ans: 8.3224,interp: 8.2181,fusion: -859.4703]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2029461.75    \n",
      "module.ans_embedding.weight  dot:  1096076.5    \n",
      "module.lstm.weight_ih_l0  dot:  56517324.0    \n",
      "module.lstm.weight_hh_l0  dot:  5395262.0    \n",
      "module.lstm.bias_ih_l0  dot:  4358467.5    \n",
      "module.lstm.bias_hh_l0  dot:  4358467.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37691952.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12669.4833984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2672806.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2672806.0    \n",
      "module.adapter.frcn_linear.weight  dot:  112203920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83725.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  560565.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  359.58746337890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  475021.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22508648.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  120227.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28887.9609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  601.1088256835938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  305406.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4824616.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  120227.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  38.204254150390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.607155799865723    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  41.074676513671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9569778.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16837344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452639.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  562/6933] Loss: -841.1440 [iq: 12.0764,ans: 8.8973,interp: 8.3519,fusion: -870.4695]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1255877.875    \n",
      "module.ans_embedding.weight  dot:  1682492.25    \n",
      "module.lstm.weight_ih_l0  dot:  20838644.0    \n",
      "module.lstm.weight_hh_l0  dot:  1755449.25    \n",
      "module.lstm.bias_ih_l0  dot:  1348103.0    \n",
      "module.lstm.bias_hh_l0  dot:  1348103.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  55899056.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  171343.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5666353.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5666353.0    \n",
      "module.adapter.frcn_linear.weight  dot:  112213488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80017.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  719469.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  456.3751220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  594447.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.381903171539307e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21154702.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  113452.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33948.328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  984.4007568359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  257926.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4308939.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  113452.4609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6959.677734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1116.7242431640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7696.5771484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.093426903522101e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10701454.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17824956.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452640.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  563/6933] Loss: -821.4415 [iq: 11.4990,ans: 8.6810,interp: 9.0873,fusion: -850.7088]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1479321.75    \n",
      "module.ans_embedding.weight  dot:  900389.875    \n",
      "module.lstm.weight_ih_l0  dot:  18754566.0    \n",
      "module.lstm.weight_hh_l0  dot:  3220816.75    \n",
      "module.lstm.bias_ih_l0  dot:  1148404.25    \n",
      "module.lstm.bias_hh_l0  dot:  1148404.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23368966.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8431.33984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1000652.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1000652.875    \n",
      "module.adapter.frcn_linear.weight  dot:  116787696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  76892.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  923381.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  604.039794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  956632.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23841500.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  119002.3984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53400.07421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1586.446533203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  656833.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.080570655176416e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5873708.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  119002.3984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  79.25041198730469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.603391647338867    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  129.71560668945312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8517601.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10662109.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452640.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  564/6933] Loss: -848.7798 [iq: 11.1079,ans: 8.4891,interp: 8.6627,fusion: -877.0396]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2165858.75    \n",
      "module.ans_embedding.weight  dot:  1416815.0    \n",
      "module.lstm.weight_ih_l0  dot:  33305336.0    \n",
      "module.lstm.weight_hh_l0  dot:  5758152.0    \n",
      "module.lstm.bias_ih_l0  dot:  1639163.75    \n",
      "module.lstm.bias_hh_l0  dot:  1639163.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26316044.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34742.0546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  869170.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  869170.25    \n",
      "module.adapter.frcn_linear.weight  dot:  106797872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  73820.6796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  588765.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  428.60101318359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  525490.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22667792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  113393.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25299.759765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  624.31298828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  195903.578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5051615.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  113393.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1417.015380859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  271.94085693359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6853.5078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.3083757955409965e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9462173.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13053426.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452641.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  565/6933] Loss: -818.3336 [iq: 10.4596,ans: 8.2511,interp: 7.8521,fusion: -844.8963]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1356240.125    \n",
      "module.ans_embedding.weight  dot:  1622246.25    \n",
      "module.lstm.weight_ih_l0  dot:  12807628.0    \n",
      "module.lstm.weight_hh_l0  dot:  2626317.5    \n",
      "module.lstm.bias_ih_l0  dot:  771304.25    \n",
      "module.lstm.bias_hh_l0  dot:  771304.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  73980880.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  199779.078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4968113.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4968113.5    \n",
      "module.adapter.frcn_linear.weight  dot:  104127680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66348.1015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  832150.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  455.61444091796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  671046.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.370246304257307e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22310012.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  109393.3515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35135.796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  835.3673095703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  312305.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4960775.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  109393.3515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  68789.0625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10562.4609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  238983.578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  13561328.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19237960.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452641.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  566/6933] Loss: -827.5777 [iq: 10.2925,ans: 9.7391,interp: 9.1698,fusion: -856.7791]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5457214.0    \n",
      "module.ans_embedding.weight  dot:  1104994.75    \n",
      "module.lstm.weight_ih_l0  dot:  83227504.0    \n",
      "module.lstm.weight_hh_l0  dot:  11432710.0    \n",
      "module.lstm.bias_ih_l0  dot:  4440670.0    \n",
      "module.lstm.bias_hh_l0  dot:  4440670.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30687396.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  42463.2421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1708042.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1708042.125    \n",
      "module.adapter.frcn_linear.weight  dot:  125104704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  86959.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  698341.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  412.0557861328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  617631.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25848192.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  130807.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  104272.0703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1330.595947265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1337641.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6213176.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  130807.8515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2591.220703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  757.323974609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6742.9765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9033276.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13870571.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452641.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  567/6933] Loss: -822.8224 [iq: 8.9160,ans: 8.4498,interp: 8.0112,fusion: -848.1993]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6692206.0    \n",
      "module.ans_embedding.weight  dot:  1034165.375    \n",
      "module.lstm.weight_ih_l0  dot:  294132992.0    \n",
      "module.lstm.weight_hh_l0  dot:  26206544.0    \n",
      "module.lstm.bias_ih_l0  dot:  16918556.0    \n",
      "module.lstm.bias_hh_l0  dot:  16918556.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26330332.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31806.74609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1839250.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1839250.5    \n",
      "module.adapter.frcn_linear.weight  dot:  156251680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  112158.015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1146501.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  583.0038452148438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1038500.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29395734.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  152294.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  54000.078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  707.4796752929688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  559264.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5669761.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  152294.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  327.47796630859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.640045166015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1108.1845703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8673951274195133e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8497070.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12969658.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452642.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  568/6933] Loss: -835.2045 [iq: 8.3222,ans: 8.6662,interp: 9.6256,fusion: -861.8185]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2387240.25    \n",
      "module.ans_embedding.weight  dot:  1274843.5    \n",
      "module.lstm.weight_ih_l0  dot:  35643520.0    \n",
      "module.lstm.weight_hh_l0  dot:  7534582.0    \n",
      "module.lstm.bias_ih_l0  dot:  1769365.5    \n",
      "module.lstm.bias_hh_l0  dot:  1769365.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20046040.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36303.9375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1630881.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1630881.25    \n",
      "module.adapter.frcn_linear.weight  dot:  137436928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  95617.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1064272.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  563.14453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1337649.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.8466972657479346e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27428206.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  140809.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  96717.578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1405.15234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1083929.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5905431.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  140809.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  626.5068359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  116.55783081054688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2505.15185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8347499.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13495073.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452642.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  569/6933] Loss: -818.4954 [iq: 8.3195,ans: 8.0775,interp: 9.7580,fusion: -844.6503]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2508361.0    \n",
      "module.ans_embedding.weight  dot:  1255954.875    \n",
      "module.lstm.weight_ih_l0  dot:  45949476.0    \n",
      "module.lstm.weight_hh_l0  dot:  6900728.0    \n",
      "module.lstm.bias_ih_l0  dot:  2575244.5    \n",
      "module.lstm.bias_hh_l0  dot:  2575244.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37893204.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  93851.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3237777.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3237777.0    \n",
      "module.adapter.frcn_linear.weight  dot:  107110504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  73658.1171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  648763.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  405.1855163574219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  476242.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23046630.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  115003.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  60782.5390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  754.1202392578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  620757.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4239170.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  115003.0234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  40198.015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5768.1748046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37269.7734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  10666350.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17025376.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452643.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  570/6933] Loss: -826.5142 [iq: 9.9485,ans: 8.0236,interp: 9.7686,fusion: -854.2549]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  31890988.0    \n",
      "module.ans_embedding.weight  dot:  1265145.5    \n",
      "module.lstm.weight_ih_l0  dot:  577925760.0    \n",
      "module.lstm.weight_hh_l0  dot:  66380612.0    \n",
      "module.lstm.bias_ih_l0  dot:  37024744.0    \n",
      "module.lstm.bias_hh_l0  dot:  37024744.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28339830.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18771.111328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1554805.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1554805.75    \n",
      "module.adapter.frcn_linear.weight  dot:  112429440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  73769.0078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  617291.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  328.4427490234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  504998.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.007016857736744e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22461948.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107405.7265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  125432.0703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  927.9807739257812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1356678.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4650832.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107405.7265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  494.7328796386719    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  145.8214569091797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1689.2568359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.556480193151913e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8901844.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13455555.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452644.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  571/6933] Loss: -799.2667 [iq: 11.0252,ans: 8.7014,interp: 8.5919,fusion: -827.5853]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2311626.0    \n",
      "module.ans_embedding.weight  dot:  1563979.5    \n",
      "module.lstm.weight_ih_l0  dot:  22082988.0    \n",
      "module.lstm.weight_hh_l0  dot:  4537286.0    \n",
      "module.lstm.bias_ih_l0  dot:  1153322.75    \n",
      "module.lstm.bias_hh_l0  dot:  1153322.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44323576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15565.8134765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4463537.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4463537.0    \n",
      "module.adapter.frcn_linear.weight  dot:  151624288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  96570.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1378341.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  755.0344848632812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1091594.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  27799278.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  133944.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  57907.34375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  797.4437255859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  667270.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5010545.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  133944.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  185.710205078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  85.99974822998047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  187.42218017578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10667048.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17817444.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452644.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  572/6933] Loss: -816.5333 [iq: 7.9476,ans: 7.2529,interp: 7.2008,fusion: -838.9346]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  682461.875    \n",
      "module.ans_embedding.weight  dot:  896408.375    \n",
      "module.lstm.weight_ih_l0  dot:  7861197.0    \n",
      "module.lstm.weight_hh_l0  dot:  3884385.0    \n",
      "module.lstm.bias_ih_l0  dot:  463033.75    \n",
      "module.lstm.bias_hh_l0  dot:  463033.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18033456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  59390.21484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  856396.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  856396.875    \n",
      "module.adapter.frcn_linear.weight  dot:  121757800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  84941.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  478875.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  335.67498779296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  344253.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27930076.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  141056.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24238.294921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  677.40283203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  300948.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7215057.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  141056.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  497.264404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  94.51089477539062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1409.080322265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8823086.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12819253.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452645.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  573/6933] Loss: -820.0402 [iq: 9.3763,ans: 9.1724,interp: 9.4886,fusion: -848.0775]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1456803.375    \n",
      "module.ans_embedding.weight  dot:  1496272.5    \n",
      "module.lstm.weight_ih_l0  dot:  19988090.0    \n",
      "module.lstm.weight_hh_l0  dot:  5914175.0    \n",
      "module.lstm.bias_ih_l0  dot:  1497321.375    \n",
      "module.lstm.bias_hh_l0  dot:  1497321.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  40653696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41359.47265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3874364.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3874364.5    \n",
      "module.adapter.frcn_linear.weight  dot:  88976256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58523.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  763300.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  426.01025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  599386.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16423674.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  82848.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  27913.109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  486.8155212402344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  302880.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1510792319313623e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4593518.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  82848.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6018.1279296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1823.787353515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8451.3310546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8673951274195133e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9341918.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19050014.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452645.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  574/6933] Loss: -820.8161 [iq: 10.2215,ans: 8.9503,interp: 8.8755,fusion: -848.8635]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2434433.25    \n",
      "module.ans_embedding.weight  dot:  1904853.0    \n",
      "module.lstm.weight_ih_l0  dot:  72103808.0    \n",
      "module.lstm.weight_hh_l0  dot:  21338196.0    \n",
      "module.lstm.bias_ih_l0  dot:  5090777.0    \n",
      "module.lstm.bias_hh_l0  dot:  5090777.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  58292384.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25155.80078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3571124.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3571124.0    \n",
      "module.adapter.frcn_linear.weight  dot:  132330224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  83752.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1644473.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  820.89697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1869985.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  25412492.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122949.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  58450.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  288.50982666015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  632788.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  10402983.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122949.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  263.83636474609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.09407043457031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  801.8228149414062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.192601986754198e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12596183.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19102256.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452646.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  575/6933] Loss: -696.1163 [iq: 11.6913,ans: 9.7179,interp: 10.3176,fusion: -727.8430]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4487341.0    \n",
      "module.ans_embedding.weight  dot:  1169392.625    \n",
      "module.lstm.weight_ih_l0  dot:  102409904.0    \n",
      "module.lstm.weight_hh_l0  dot:  23592304.0    \n",
      "module.lstm.bias_ih_l0  dot:  6917555.0    \n",
      "module.lstm.bias_hh_l0  dot:  6917555.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  50794432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24529.29296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4348186.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4348186.0    \n",
      "module.adapter.frcn_linear.weight  dot:  167198400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  117285.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1153503.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  632.5684204101562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1399929.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.954948286060244e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  31031556.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  154782.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  105412.109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  657.963134765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1077452.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  11200574.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  154782.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2221.01904296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  363.25018310546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8432.787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  12308304.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  23935340.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452646.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  576/6933] Loss: -784.7335 [iq: 11.6697,ans: 9.9833,interp: 10.2958,fusion: -816.6823]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1414808.25    \n",
      "module.ans_embedding.weight  dot:  1210232.0    \n",
      "module.lstm.weight_ih_l0  dot:  16052086.0    \n",
      "module.lstm.weight_hh_l0  dot:  6819184.0    \n",
      "module.lstm.bias_ih_l0  dot:  914528.25    \n",
      "module.lstm.bias_hh_l0  dot:  914528.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42301164.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  151478.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3252553.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3252553.0    \n",
      "module.adapter.frcn_linear.weight  dot:  110521232.0    \n",
      "module.adapter.frcn_linear.bias  dot:  71920.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1157293.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  739.860107421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1004350.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23319870.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  117272.515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33654.42578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1404.0147705078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  380447.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6280220.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  117272.515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1917.72265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  253.25914001464844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4528.994140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.861733072734296e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10457932.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15458110.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452647.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  577/6933] Loss: -814.4544 [iq: 10.1856,ans: 9.7460,interp: 9.1479,fusion: -843.5340]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4642687.5    \n",
      "module.ans_embedding.weight  dot:  1266599.75    \n",
      "module.lstm.weight_ih_l0  dot:  97585248.0    \n",
      "module.lstm.weight_hh_l0  dot:  16928180.0    \n",
      "module.lstm.bias_ih_l0  dot:  6377702.0    \n",
      "module.lstm.bias_hh_l0  dot:  6377702.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27654536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9468.19921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1123863.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1123863.875    \n",
      "module.adapter.frcn_linear.weight  dot:  149879456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  111117.265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  786055.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  499.15887451171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  586813.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  33325712.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  165374.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  90012.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  680.524658203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  963209.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9567125.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  165374.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  533.9913940429688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  244.0188446044922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  972.8701171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10275748.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17039156.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452647.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  578/6933] Loss: -790.7544 [iq: 11.3708,ans: 10.2092,interp: 10.8878,fusion: -823.2222]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1872580.875    \n",
      "module.ans_embedding.weight  dot:  1547135.75    \n",
      "module.lstm.weight_ih_l0  dot:  45475528.0    \n",
      "module.lstm.weight_hh_l0  dot:  6751736.5    \n",
      "module.lstm.bias_ih_l0  dot:  2814959.0    \n",
      "module.lstm.bias_hh_l0  dot:  2814959.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19502360.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  56649.28515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  483590.15625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  483590.15625    \n",
      "module.adapter.frcn_linear.weight  dot:  117669024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  77799.234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  929671.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  548.9619140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1104210.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.901959644281305e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23665472.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  111194.703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  73086.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1010.0311279296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1035703.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9795955924782902e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4736114.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  111194.703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  885.827880859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  115.96788787841797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4432.5419921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7899954.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9926418.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452648.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  579/6933] Loss: -822.8895 [iq: 8.9054,ans: 8.0612,interp: 8.0317,fusion: -847.8878]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6752364.5    \n",
      "module.ans_embedding.weight  dot:  1562663.5    \n",
      "module.lstm.weight_ih_l0  dot:  113458736.0    \n",
      "module.lstm.weight_hh_l0  dot:  8953020.0    \n",
      "module.lstm.bias_ih_l0  dot:  5226392.5    \n",
      "module.lstm.bias_hh_l0  dot:  5226392.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31739152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39951.09375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2850094.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2850094.5    \n",
      "module.adapter.frcn_linear.weight  dot:  98756064.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66250.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  498241.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  363.53729248046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  343092.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20771046.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  98050.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22457.392578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  438.19805908203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  256089.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4662362.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  98050.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9997.2890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1274.5582275390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  31205.990234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8176154.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15241156.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452648.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  580/6933] Loss: -819.2536 [iq: 9.2824,ans: 8.1954,interp: 7.9081,fusion: -844.6395]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1182991.0    \n",
      "module.ans_embedding.weight  dot:  1409259.0    \n",
      "module.lstm.weight_ih_l0  dot:  10189706.0    \n",
      "module.lstm.weight_hh_l0  dot:  1939374.5    \n",
      "module.lstm.bias_ih_l0  dot:  528717.25    \n",
      "module.lstm.bias_hh_l0  dot:  528717.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20809386.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31189.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  478263.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  478263.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  79500280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50412.34765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  604338.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  417.95916748046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  523065.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.676156433764845e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16261826.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73545.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31213.9453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  435.1861572265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  367201.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4346120.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73545.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1371.488037109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  399.3569641113281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2805.591064453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7215424.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10322081.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452648.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  581/6933] Loss: -840.6609 [iq: 9.6295,ans: 7.8137,interp: 8.4607,fusion: -866.5648]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  956617.6875    \n",
      "module.ans_embedding.weight  dot:  1068476.25    \n",
      "module.lstm.weight_ih_l0  dot:  27168722.0    \n",
      "module.lstm.weight_hh_l0  dot:  3244321.5    \n",
      "module.lstm.bias_ih_l0  dot:  1717184.0    \n",
      "module.lstm.bias_hh_l0  dot:  1717184.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20635828.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31198.14453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  993794.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  993794.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  90650880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  64071.38671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  438349.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  292.92230224609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  346217.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20272880.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  97981.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31491.62109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  695.1658935546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  390213.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7209913494298235e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4252669.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  97981.7890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  711.64404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  210.73805236816406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1329.507568359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7753146.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10144964.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452649.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  582/6933] Loss: -841.8591 [iq: 8.4317,ans: 7.5735,interp: 7.5712,fusion: -865.4355]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  223235.859375    \n",
      "module.ans_embedding.weight  dot:  1386069.25    \n",
      "module.lstm.weight_ih_l0  dot:  3995824.0    \n",
      "module.lstm.weight_hh_l0  dot:  1152599.125    \n",
      "module.lstm.bias_ih_l0  dot:  259535.265625    \n",
      "module.lstm.bias_hh_l0  dot:  259535.265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21604784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  56958.671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  546217.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  546217.125    \n",
      "module.adapter.frcn_linear.weight  dot:  73970976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48269.5546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  678593.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  372.05438232421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  569435.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18591962.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  85567.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15426.880859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  310.7449951171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  160185.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5421198.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  85567.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  693.8506469726562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  157.15170288085938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2548.4306640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8654616.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10491340.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452650.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  583/6933] Loss: -830.6541 [iq: 10.7854,ans: 8.4442,interp: 8.0627,fusion: -857.9464]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2838211.25    \n",
      "module.ans_embedding.weight  dot:  1133634.75    \n",
      "module.lstm.weight_ih_l0  dot:  62025832.0    \n",
      "module.lstm.weight_hh_l0  dot:  7752058.0    \n",
      "module.lstm.bias_ih_l0  dot:  4786405.0    \n",
      "module.lstm.bias_hh_l0  dot:  4786405.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20874162.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29981.59375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  515295.78125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  515295.78125    \n",
      "module.adapter.frcn_linear.weight  dot:  92747184.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58368.63671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1267991.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  593.2850341796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  959686.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.036295184865594e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22104440.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  101491.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37180.65625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  622.0869750976562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  231323.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4935977.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  101491.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1645.908935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  412.7508544921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5415.43359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.1391778065881226e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8212320.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10401800.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452650.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  584/6933] Loss: -845.6920 [iq: 10.8176,ans: 8.6653,interp: 9.8267,fusion: -875.0015]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4109642.5    \n",
      "module.ans_embedding.weight  dot:  1006705.3125    \n",
      "module.lstm.weight_ih_l0  dot:  25654730.0    \n",
      "module.lstm.weight_hh_l0  dot:  3335869.25    \n",
      "module.lstm.bias_ih_l0  dot:  1455200.25    \n",
      "module.lstm.bias_hh_l0  dot:  1455200.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18560002.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35384.8203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  503833.03125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  503833.03125    \n",
      "module.adapter.frcn_linear.weight  dot:  84480128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  54223.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  816570.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  421.71051025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  639540.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18804624.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  89400.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  36013.265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  827.2547607421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  404690.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.725290298461914e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5520506.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  89400.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  641.4057006835938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  145.78094482421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  987.853271484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7514381.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8621270.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452651.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  585/6933] Loss: -821.2127 [iq: 8.9906,ans: 8.2067,interp: 7.8698,fusion: -846.2798]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6853890.0    \n",
      "module.ans_embedding.weight  dot:  823670.4375    \n",
      "module.lstm.weight_ih_l0  dot:  92073504.0    \n",
      "module.lstm.weight_hh_l0  dot:  16004353.0    \n",
      "module.lstm.bias_ih_l0  dot:  5477893.0    \n",
      "module.lstm.bias_hh_l0  dot:  5477893.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18220690.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4220.07470703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1368658.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1368658.75    \n",
      "module.adapter.frcn_linear.weight  dot:  118987568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  78990.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  917158.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  595.5347900390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1064103.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25217564.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  116853.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  118902.59375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  959.2803955078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1520189.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6239929.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  116853.6484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9.076637268066406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.8173035383224487    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35.06597137451172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7392183.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9899272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452651.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  586/6933] Loss: -845.7867 [iq: 7.2076,ans: 6.8780,interp: 7.0137,fusion: -866.8861]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1626585.875    \n",
      "module.ans_embedding.weight  dot:  1305535.75    \n",
      "module.lstm.weight_ih_l0  dot:  38510680.0    \n",
      "module.lstm.weight_hh_l0  dot:  7258062.0    \n",
      "module.lstm.bias_ih_l0  dot:  2643483.5    \n",
      "module.lstm.bias_hh_l0  dot:  2643483.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32869748.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  104507.8515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5223636.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5223636.0    \n",
      "module.adapter.frcn_linear.weight  dot:  112299008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80838.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  447377.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  242.84832763671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  324205.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25366180.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  121515.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  45314.078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  585.968505859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  516621.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7464866.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  121515.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1613.009521484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  265.23388671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2359.09375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3656631381309126e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10523757.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24358272.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452652.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  587/6933] Loss: -841.4775 [iq: 7.8296,ans: 8.2591,interp: 8.0817,fusion: -865.6478]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  504504.75    \n",
      "module.ans_embedding.weight  dot:  1374786.0    \n",
      "module.lstm.weight_ih_l0  dot:  9362054.0    \n",
      "module.lstm.weight_hh_l0  dot:  1486753.75    \n",
      "module.lstm.bias_ih_l0  dot:  631846.75    \n",
      "module.lstm.bias_hh_l0  dot:  631846.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30999132.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19025.8203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2045940.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2045940.25    \n",
      "module.adapter.frcn_linear.weight  dot:  106444080.0    \n",
      "module.adapter.frcn_linear.bias  dot:  70908.2421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  668802.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  388.0157470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  539924.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24190532.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  108752.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34482.0    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  637.9412231445312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  389700.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4986099.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  108752.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  591.8775634765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  180.6578369140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  848.1053466796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3877787807814457e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8298663.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13595128.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452652.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  588/6933] Loss: -841.7358 [iq: 8.8384,ans: 8.4016,interp: 10.6224,fusion: -869.5982]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  478048.8125    \n",
      "module.ans_embedding.weight  dot:  1030949.25    \n",
      "module.lstm.weight_ih_l0  dot:  4433788.0    \n",
      "module.lstm.weight_hh_l0  dot:  3260184.25    \n",
      "module.lstm.bias_ih_l0  dot:  249952.515625    \n",
      "module.lstm.bias_hh_l0  dot:  249952.515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19041788.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9141.572265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  899872.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  899872.5    \n",
      "module.adapter.frcn_linear.weight  dot:  105033848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  67545.2109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  556887.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  325.7913513183594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  411033.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22850064.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  104439.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  40555.45703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1246.019287109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  603432.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6247243.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  104439.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  85.33037567138672    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.702730178833008    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  179.9566192626953    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7846402.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12134704.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452653.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  589/6933] Loss: -843.5729 [iq: 9.9070,ans: 8.9056,interp: 8.4533,fusion: -870.8387]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  688342.4375    \n",
      "module.ans_embedding.weight  dot:  1152332.75    \n",
      "module.lstm.weight_ih_l0  dot:  6455514.0    \n",
      "module.lstm.weight_hh_l0  dot:  1782748.0    \n",
      "module.lstm.bias_ih_l0  dot:  339050.21875    \n",
      "module.lstm.bias_hh_l0  dot:  339050.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25780320.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  47396.234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1230799.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1230799.0    \n",
      "module.adapter.frcn_linear.weight  dot:  88803392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  59637.1640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  748218.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  568.5827026367188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  796194.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20746700.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96527.5703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44920.01953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  880.3106079101562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  605051.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4935871.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96527.5703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2462.6708984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  655.279296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10113.6005859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7751730.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11001538.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452653.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  590/6933] Loss: -826.5056 [iq: 12.6581,ans: 9.7353,interp: 9.2897,fusion: -858.1887]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  455596.6875    \n",
      "module.ans_embedding.weight  dot:  678136.1875    \n",
      "module.lstm.weight_ih_l0  dot:  4314170.0    \n",
      "module.lstm.weight_hh_l0  dot:  2711207.25    \n",
      "module.lstm.bias_ih_l0  dot:  192404.90625    \n",
      "module.lstm.bias_hh_l0  dot:  192404.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18326708.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1199.7490234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  660722.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  660722.125    \n",
      "module.adapter.frcn_linear.weight  dot:  93471248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  63300.9375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  627334.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  361.6171569824219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  471312.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20654892.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95216.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31892.158203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  827.546630859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  415277.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5716348.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95216.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3.0046467781066895    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.6342692375183105    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7.800925254821777    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7844718.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8297676.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452654.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  591/6933] Loss: -842.6597 [iq: 11.1547,ans: 8.3084,interp: 8.2027,fusion: -870.3254]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3620198.5    \n",
      "module.ans_embedding.weight  dot:  1237065.5    \n",
      "module.lstm.weight_ih_l0  dot:  43326084.0    \n",
      "module.lstm.weight_hh_l0  dot:  6925321.5    \n",
      "module.lstm.bias_ih_l0  dot:  2282024.5    \n",
      "module.lstm.bias_hh_l0  dot:  2282024.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23600980.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4254.232421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1195577.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1195577.0    \n",
      "module.adapter.frcn_linear.weight  dot:  85215120.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53435.21484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1320837.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  776.3966674804688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1013720.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.022684490540996e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19341964.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  89184.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53961.125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  845.734619140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  603442.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4578818.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  89184.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  429.020751953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.15953826904297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  271.82354736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7371711.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12321816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452654.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  592/6933] Loss: -815.4409 [iq: 13.0747,ans: 9.1511,interp: 12.2048,fusion: -849.8715]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1872891.625    \n",
      "module.ans_embedding.weight  dot:  1559011.75    \n",
      "module.lstm.weight_ih_l0  dot:  22946222.0    \n",
      "module.lstm.weight_hh_l0  dot:  3381623.0    \n",
      "module.lstm.bias_ih_l0  dot:  1272715.75    \n",
      "module.lstm.bias_hh_l0  dot:  1272715.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33594848.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22967.06640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  789175.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  789175.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  107845744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72816.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  592588.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  278.43408203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  402377.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.456524038687348e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23806696.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  111823.5390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  48953.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  779.6721801757812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  613485.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.080570655176416e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5039167.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  111823.5390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  243.63235473632812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.039703369140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  274.9005126953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.355005493536737e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8370732.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13272793.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452655.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  593/6933] Loss: -784.2228 [iq: 11.6873,ans: 8.9543,interp: 8.9469,fusion: -813.8114]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1800069.625    \n",
      "module.ans_embedding.weight  dot:  744591.875    \n",
      "module.lstm.weight_ih_l0  dot:  21373472.0    \n",
      "module.lstm.weight_hh_l0  dot:  3930170.0    \n",
      "module.lstm.bias_ih_l0  dot:  1195755.25    \n",
      "module.lstm.bias_hh_l0  dot:  1195755.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15536495.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18902.8046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  452174.46875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  452174.46875    \n",
      "module.adapter.frcn_linear.weight  dot:  109018024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  70028.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  671978.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  411.0494079589844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  730122.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2283294381632004e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22754584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99874.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  67079.296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  641.397705078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  713746.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6218695.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99874.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  466.0382080078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  87.16708374023438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1625.104736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7073461.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8106935.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452655.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  594/6933] Loss: -852.2501 [iq: 7.3336,ans: 7.1203,interp: 7.1222,fusion: -873.8262]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  478794.1875    \n",
      "module.ans_embedding.weight  dot:  1164283.375    \n",
      "module.lstm.weight_ih_l0  dot:  12304839.0    \n",
      "module.lstm.weight_hh_l0  dot:  7659629.0    \n",
      "module.lstm.bias_ih_l0  dot:  762306.3125    \n",
      "module.lstm.bias_hh_l0  dot:  762306.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19728154.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23759.755859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  541294.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  541294.375    \n",
      "module.adapter.frcn_linear.weight  dot:  107052032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  73370.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  564155.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  332.7921142578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  425108.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1393589122453704e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24912664.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  110777.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23743.708984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  658.1806640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  337125.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7243733.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  110777.0234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  255.0858917236328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.646114349365234    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  819.7554931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8453916.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12188306.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452656.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  595/6933] Loss: -833.2371 [iq: 9.6903,ans: 8.5832,interp: 11.6282,fusion: -863.1388]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4305305.5    \n",
      "module.ans_embedding.weight  dot:  1563795.875    \n",
      "module.lstm.weight_ih_l0  dot:  49058400.0    \n",
      "module.lstm.weight_hh_l0  dot:  10456875.0    \n",
      "module.lstm.bias_ih_l0  dot:  2794826.5    \n",
      "module.lstm.bias_hh_l0  dot:  2794826.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49221960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  68310.9765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2661132.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2661132.0    \n",
      "module.adapter.frcn_linear.weight  dot:  204632704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  139423.96875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2755848.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1059.122314453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  3292188.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.87805368215777e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  42755052.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  188872.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  116751.4921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  909.074951171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1323968.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9011566.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  188872.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3027.279296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  595.9986572265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13039.853515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9692225.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13054489.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452656.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  596/6933] Loss: -814.2417 [iq: 10.0564,ans: 8.9935,interp: 9.6340,fusion: -842.9255]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1134946.625    \n",
      "module.ans_embedding.weight  dot:  1138586.5    \n",
      "module.lstm.weight_ih_l0  dot:  7395206.0    \n",
      "module.lstm.weight_hh_l0  dot:  3211063.0    \n",
      "module.lstm.bias_ih_l0  dot:  401720.15625    \n",
      "module.lstm.bias_hh_l0  dot:  401720.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29185884.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21190.458984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2520702.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2520702.5    \n",
      "module.adapter.frcn_linear.weight  dot:  100169024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  65373.2421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1333067.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  694.6309204101562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1283574.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.557435648981482e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21474392.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96329.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31297.78515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  804.759765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  451228.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3655957193113863e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6228487.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96329.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  375.2381896972656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  78.10132598876953    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  596.7417602539062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8414866.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14268848.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452657.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  597/6933] Loss: -828.1940 [iq: 10.2578,ans: 9.0804,interp: 9.2185,fusion: -856.7506]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  413557.5625    \n",
      "module.ans_embedding.weight  dot:  765854.0625    \n",
      "module.lstm.weight_ih_l0  dot:  6141895.5    \n",
      "module.lstm.weight_hh_l0  dot:  5246424.0    \n",
      "module.lstm.bias_ih_l0  dot:  382558.65625    \n",
      "module.lstm.bias_hh_l0  dot:  382558.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33419692.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4489.5859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2334225.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2334225.25    \n",
      "module.adapter.frcn_linear.weight  dot:  98962528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66022.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  876142.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  426.07177734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  854196.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22591096.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  104577.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35647.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  956.369140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  550632.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6316640.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  104577.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  52.21696472167969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.789953231811523    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  165.71701049804688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7985612998927536e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8805637.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13412634.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452658.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  598/6933] Loss: -868.0190 [iq: 8.6030,ans: 7.6980,interp: 7.6399,fusion: -891.9599]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  389433.34375    \n",
      "module.ans_embedding.weight  dot:  1466916.25    \n",
      "module.lstm.weight_ih_l0  dot:  5304786.5    \n",
      "module.lstm.weight_hh_l0  dot:  1418596.75    \n",
      "module.lstm.bias_ih_l0  dot:  265561.03125    \n",
      "module.lstm.bias_hh_l0  dot:  265561.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37061560.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8154.728515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4272677.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4272677.0    \n",
      "module.adapter.frcn_linear.weight  dot:  61639024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42945.5859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  580549.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  428.6001892089844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  706695.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15643601.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  74166.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12573.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  193.27951049804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  173996.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.5204160414868966e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3238045.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  74166.5078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  264.964111328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.888275146484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  902.370361328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8200501.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  16850762.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452658.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  599/6933] Loss: -854.5499 [iq: 9.5463,ans: 7.5784,interp: 7.2592,fusion: -878.9339]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  151858.71875    \n",
      "module.ans_embedding.weight  dot:  1198227.75    \n",
      "module.lstm.weight_ih_l0  dot:  7433584.5    \n",
      "module.lstm.weight_hh_l0  dot:  1693141.5    \n",
      "module.lstm.bias_ih_l0  dot:  606829.4375    \n",
      "module.lstm.bias_hh_l0  dot:  606829.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34751912.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30150.5    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1470889.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1470889.25    \n",
      "module.adapter.frcn_linear.weight  dot:  106226800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  74965.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  498793.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  251.80746459960938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  375819.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23532120.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107371.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23860.369140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  832.1722412109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  352044.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6842445.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107371.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  831.8690185546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  153.49053955078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2939.31884765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11520748.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13088289.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452659.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  600/6933] Loss: -840.9517 [iq: 10.5456,ans: 8.3680,interp: 9.5083,fusion: -869.3736]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  658075.875    \n",
      "module.ans_embedding.weight  dot:  1098887.25    \n",
      "module.lstm.weight_ih_l0  dot:  11292786.0    \n",
      "module.lstm.weight_hh_l0  dot:  2715042.75    \n",
      "module.lstm.bias_ih_l0  dot:  767334.3125    \n",
      "module.lstm.bias_hh_l0  dot:  767334.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23795532.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37497.1953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1194855.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1194855.875    \n",
      "module.adapter.frcn_linear.weight  dot:  117757824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  65677.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1716732.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  778.2054443359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1726028.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22802380.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94953.6640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24225.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  890.56494140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  272061.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6425617.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94953.6640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  589.7479248046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  112.72940063476562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1352.86767578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.893241119432787e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8753377.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10583631.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452659.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  601/6933] Loss: -790.8127 [iq: 12.2495,ans: 9.5380,interp: 10.4592,fusion: -823.0594]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  351171.53125    \n",
      "module.ans_embedding.weight  dot:  1361278.875    \n",
      "module.lstm.weight_ih_l0  dot:  3725453.75    \n",
      "module.lstm.weight_hh_l0  dot:  1116350.375    \n",
      "module.lstm.bias_ih_l0  dot:  185129.375    \n",
      "module.lstm.bias_hh_l0  dot:  185129.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20848046.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25758.63671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  615666.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  615666.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  85303312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57493.79296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  716789.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  469.9150695800781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  562507.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0668941285985056e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18517960.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  84878.2265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8639.3564453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  130.24652099609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  102324.546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4029823.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  84878.2265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  339.6972351074219    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  90.18993377685547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  817.4285888671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.672262990534364e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7396634.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10754341.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452660.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  602/6933] Loss: -824.8741 [iq: 10.3799,ans: 8.4048,interp: 8.7295,fusion: -852.3884]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  42115392.0    \n",
      "module.ans_embedding.weight  dot:  2573187.0    \n",
      "module.lstm.weight_ih_l0  dot:  898951936.0    \n",
      "module.lstm.weight_hh_l0  dot:  91365904.0    \n",
      "module.lstm.bias_ih_l0  dot:  44131732.0    \n",
      "module.lstm.bias_hh_l0  dot:  44131732.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  57460712.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  59742.7265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3402157.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3402157.75    \n",
      "module.adapter.frcn_linear.weight  dot:  87617008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  60944.31640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  622262.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  378.6104736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  492418.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19514896.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  92588.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24927.80859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  583.6727905273438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  195126.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5341157.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  92588.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  17246.611328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3094.58349609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  41573.87109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10275110.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14898085.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452660.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  603/6933] Loss: -802.4435 [iq: 10.0301,ans: 8.7965,interp: 11.2644,fusion: -832.5344]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  233166.640625    \n",
      "module.ans_embedding.weight  dot:  1599846.625    \n",
      "module.lstm.weight_ih_l0  dot:  1475648.25    \n",
      "module.lstm.weight_hh_l0  dot:  991060.25    \n",
      "module.lstm.bias_ih_l0  dot:  56478.609375    \n",
      "module.lstm.bias_hh_l0  dot:  56478.609375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21060208.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  54720.4375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  676491.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  676491.0    \n",
      "module.adapter.frcn_linear.weight  dot:  82502080.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58338.63671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  311380.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  176.6875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  259155.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.901959644281305e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17767128.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  83120.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17356.078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  483.2319030761719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  262134.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3795852.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  83120.0078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8267.5751953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1619.0791015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  39583.66796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7047226.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10655264.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452661.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  604/6933] Loss: -821.6671 [iq: 10.0107,ans: 8.2308,interp: 9.2539,fusion: -849.1625]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  142290.71875    \n",
      "module.ans_embedding.weight  dot:  1002695.875    \n",
      "module.lstm.weight_ih_l0  dot:  1514223.0    \n",
      "module.lstm.weight_hh_l0  dot:  749149.75    \n",
      "module.lstm.bias_ih_l0  dot:  67656.1875    \n",
      "module.lstm.bias_hh_l0  dot:  67656.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18162796.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71279.15625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1620346.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1620346.875    \n",
      "module.adapter.frcn_linear.weight  dot:  90055744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  56593.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1043917.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  606.620849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1434751.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20637216.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  90195.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13135.4140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  319.31597900390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  180626.046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4302312.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  90195.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2252.44775390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  357.60223388671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7363.58984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7408297026122455e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7180854.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11336825.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452661.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  605/6933] Loss: -831.6818 [iq: 8.4814,ans: 7.4817,interp: 7.5373,fusion: -855.1822]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5745749.0    \n",
      "module.ans_embedding.weight  dot:  1022930.625    \n",
      "module.lstm.weight_ih_l0  dot:  106448168.0    \n",
      "module.lstm.weight_hh_l0  dot:  13818892.0    \n",
      "module.lstm.bias_ih_l0  dot:  6406344.0    \n",
      "module.lstm.bias_hh_l0  dot:  6406344.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11976792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6434.56884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  686485.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  686485.625    \n",
      "module.adapter.frcn_linear.weight  dot:  100990888.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72251.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  682759.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  412.0212097167969    \n",
      "module.attflat_img.mlp.linear.weight  dot:  777762.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23679166.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  108444.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  102602.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  985.54931640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1363348.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5797867.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  108444.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.357144832611084    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.07521825283765793    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2.003420352935791    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.206324095117452e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6320284.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9440815.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452662.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  606/6933] Loss: -843.1527 [iq: 7.1062,ans: 6.3898,interp: 6.7349,fusion: -863.3836]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  265682.0    \n",
      "module.ans_embedding.weight  dot:  2240552.5    \n",
      "module.lstm.weight_ih_l0  dot:  4375391.0    \n",
      "module.lstm.weight_hh_l0  dot:  3751085.0    \n",
      "module.lstm.bias_ih_l0  dot:  242547.53125    \n",
      "module.lstm.bias_hh_l0  dot:  242547.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  64993736.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31680.76171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4911117.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4911117.0    \n",
      "module.adapter.frcn_linear.weight  dot:  89154992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  59263.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  626348.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  400.28375244140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  578458.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21344408.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  97786.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22122.1875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  351.8160095214844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  274172.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6695018.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  97786.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1002.8404541015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  228.69032287597656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1873.701171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1951328815484885e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10929864.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  17760060.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452662.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  607/6933] Loss: -810.1124 [iq: 11.5169,ans: 8.8703,interp: 9.2853,fusion: -839.7850]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  470449.3125    \n",
      "module.ans_embedding.weight  dot:  879090.5625    \n",
      "module.lstm.weight_ih_l0  dot:  3720897.5    \n",
      "module.lstm.weight_hh_l0  dot:  1363541.875    \n",
      "module.lstm.bias_ih_l0  dot:  171968.4375    \n",
      "module.lstm.bias_hh_l0  dot:  171968.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32920724.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3564.80224609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2356904.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2356904.5    \n",
      "module.adapter.frcn_linear.weight  dot:  83446800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  56727.97265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  559480.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.2890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  526671.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18816794.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86688.3125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16317.3984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  240.68206787109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  227196.515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.143885234952904e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3939393.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86688.3125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  22.392253875732422    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.348853588104248    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  44.60496520996094    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7952534.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11512514.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452663.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  608/6933] Loss: -844.0920 [iq: 7.9926,ans: 7.5954,interp: 7.4636,fusion: -867.1436]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4535110.0    \n",
      "module.ans_embedding.weight  dot:  1004436.6875    \n",
      "module.lstm.weight_ih_l0  dot:  59339620.0    \n",
      "module.lstm.weight_hh_l0  dot:  4634859.5    \n",
      "module.lstm.bias_ih_l0  dot:  2311706.75    \n",
      "module.lstm.bias_hh_l0  dot:  2311706.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16472538.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11207.900390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  581772.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  581772.75    \n",
      "module.adapter.frcn_linear.weight  dot:  103692224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  67664.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1054385.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  572.7943115234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1278346.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21829730.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96810.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38358.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1294.095458984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  562059.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6417212.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96810.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3312.54052734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  600.6651611328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8955.9140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  7187658.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10225697.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452663.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  609/6933] Loss: -842.2366 [iq: 10.2220,ans: 8.4691,interp: 8.9287,fusion: -869.8564]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  733993.0    \n",
      "module.ans_embedding.weight  dot:  1101804.75    \n",
      "module.lstm.weight_ih_l0  dot:  13623052.0    \n",
      "module.lstm.weight_hh_l0  dot:  1750863.0    \n",
      "module.lstm.bias_ih_l0  dot:  823275.25    \n",
      "module.lstm.bias_hh_l0  dot:  823275.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30266970.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18230.720703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  859756.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  859756.625    \n",
      "module.adapter.frcn_linear.weight  dot:  78631920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51240.02734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  579737.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  307.5384216308594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  485920.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17821836.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  81642.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20026.783203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  343.4795837402344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  219375.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781864042044617e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4051890.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  81642.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  428.25372314453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  145.8626251220703    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1521.075927734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7624807.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9071364.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452664.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  610/6933] Loss: -825.0184 [iq: 11.1796,ans: 9.1591,interp: 9.7352,fusion: -855.0923]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  407246.375    \n",
      "module.ans_embedding.weight  dot:  800060.75    \n",
      "module.lstm.weight_ih_l0  dot:  6010802.5    \n",
      "module.lstm.weight_hh_l0  dot:  1468926.5    \n",
      "module.lstm.bias_ih_l0  dot:  314750.53125    \n",
      "module.lstm.bias_hh_l0  dot:  314750.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17143492.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28196.484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  591003.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  591003.0    \n",
      "module.adapter.frcn_linear.weight  dot:  101808584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  67510.0390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  842551.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  449.62823486328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  739553.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22871694.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  106416.2890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21390.6640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  491.7638244628906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  254626.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.751221472863108e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6890623.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  106416.2890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  549.9609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  98.31541442871094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  796.6589965820312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8202610.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8561903.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452664.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  611/6933] Loss: -857.6496 [iq: 8.0763,ans: 7.5946,interp: 8.7912,fusion: -882.1118]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  238681.0    \n",
      "module.ans_embedding.weight  dot:  828559.1875    \n",
      "module.lstm.weight_ih_l0  dot:  1851403.75    \n",
      "module.lstm.weight_hh_l0  dot:  839092.75    \n",
      "module.lstm.bias_ih_l0  dot:  71145.4140625    \n",
      "module.lstm.bias_hh_l0  dot:  71145.4140625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19747052.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15868.0185546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1030748.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1030748.25    \n",
      "module.adapter.frcn_linear.weight  dot:  127308768.0    \n",
      "module.adapter.frcn_linear.bias  dot:  87804.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  727195.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  471.81842041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  558365.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25112364.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  122446.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  30838.46875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  816.1116943359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  541864.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.697963155806065e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6182563.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  122446.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3222.812255859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  576.79296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7412.10498046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7766189.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12001095.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452664.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  612/6933] Loss: -855.7170 [iq: 8.4657,ans: 8.4337,interp: 9.4581,fusion: -882.0745]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  879641.9375    \n",
      "module.ans_embedding.weight  dot:  1351174.75    \n",
      "module.lstm.weight_ih_l0  dot:  8982264.0    \n",
      "module.lstm.weight_hh_l0  dot:  2399696.0    \n",
      "module.lstm.bias_ih_l0  dot:  466073.8125    \n",
      "module.lstm.bias_hh_l0  dot:  466073.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22683548.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20482.38671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  646656.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  646656.375    \n",
      "module.adapter.frcn_linear.weight  dot:  84049088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57132.69921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  711134.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  366.06951904296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  616178.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16990208.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  82034.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  36543.00390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  628.953857421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  469167.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0219082469120622e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3915746.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  82034.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3497.14501953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  693.524658203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10473.119140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6335286.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9376412.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452665.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  613/6933] Loss: -830.4238 [iq: 9.8137,ans: 8.6828,interp: 8.9116,fusion: -857.8320]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8961676.0    \n",
      "module.ans_embedding.weight  dot:  1265429.0    \n",
      "module.lstm.weight_ih_l0  dot:  131991880.0    \n",
      "module.lstm.weight_hh_l0  dot:  16895096.0    \n",
      "module.lstm.bias_ih_l0  dot:  7523205.0    \n",
      "module.lstm.bias_hh_l0  dot:  7523205.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26104152.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17330.01953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3051846.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3051846.0    \n",
      "module.adapter.frcn_linear.weight  dot:  151347600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  97721.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1579661.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  792.330810546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1542526.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.080570655176416e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  28505398.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  127015.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  200030.59375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1756.9302978515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  2343863.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9960801839479245e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6481143.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  127015.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1028.68017578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  127.55154418945312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2650.06982421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7143082.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13597725.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452666.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  614/6933] Loss: -850.6866 [iq: 7.1277,ans: 6.9706,interp: 6.9065,fusion: -871.6914]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  598855.8125    \n",
      "module.ans_embedding.weight  dot:  726069.1875    \n",
      "module.lstm.weight_ih_l0  dot:  3977946.5    \n",
      "module.lstm.weight_hh_l0  dot:  1593640.5    \n",
      "module.lstm.bias_ih_l0  dot:  161031.9375    \n",
      "module.lstm.bias_hh_l0  dot:  161031.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16835854.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29351.189453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  892546.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  892546.75    \n",
      "module.adapter.frcn_linear.weight  dot:  85305376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53382.4140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  691504.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  362.6276550292969    \n",
      "module.attflat_img.mlp.linear.weight  dot:  518728.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19264762.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86160.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24190.7109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  615.6848754882812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  403660.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5288605936802924e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4898530.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86160.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  263.5089111328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  56.227943420410156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  691.05126953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.352074256530614e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7578024.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10116816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452667.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  615/6933] Loss: -861.4007 [iq: 11.4290,ans: 9.1676,interp: 9.0312,fusion: -891.0284]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1969253.75    \n",
      "module.ans_embedding.weight  dot:  1034259.75    \n",
      "module.lstm.weight_ih_l0  dot:  53290504.0    \n",
      "module.lstm.weight_hh_l0  dot:  6850398.5    \n",
      "module.lstm.bias_ih_l0  dot:  3406716.5    \n",
      "module.lstm.bias_hh_l0  dot:  3406716.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24705198.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12000.546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1143706.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1143706.25    \n",
      "module.adapter.frcn_linear.weight  dot:  88689880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51310.87890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  829101.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  451.419189453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  735706.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18085580.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76245.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  29238.87890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  500.0810546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  414282.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4099955.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76245.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  170.2724609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  36.65531921386719    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  567.0381469726562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7174173.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8019151.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452667.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  616/6933] Loss: -836.5869 [iq: 12.6813,ans: 9.2736,interp: 10.3573,fusion: -868.8990]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  939724.5    \n",
      "module.ans_embedding.weight  dot:  1069271.125    \n",
      "module.lstm.weight_ih_l0  dot:  24071972.0    \n",
      "module.lstm.weight_hh_l0  dot:  13600372.0    \n",
      "module.lstm.bias_ih_l0  dot:  1838716.625    \n",
      "module.lstm.bias_hh_l0  dot:  1838716.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31755594.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9917.8251953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2689726.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2689726.75    \n",
      "module.adapter.frcn_linear.weight  dot:  121539504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  79234.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  987898.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  540.1240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  636377.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23781876.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  110723.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31373.23828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1036.49560546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  505180.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4356836547667626e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5954028.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  110723.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  315.2620849609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.949913024902344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1259.1695556640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8185257.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15624210.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452668.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  617/6933] Loss: -850.6236 [iq: 9.0289,ans: 8.8809,interp: 9.0668,fusion: -877.6002]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1101638.625    \n",
      "module.ans_embedding.weight  dot:  1081746.0    \n",
      "module.lstm.weight_ih_l0  dot:  39460136.0    \n",
      "module.lstm.weight_hh_l0  dot:  8808328.0    \n",
      "module.lstm.bias_ih_l0  dot:  2533096.5    \n",
      "module.lstm.bias_hh_l0  dot:  2533096.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21171292.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  46048.859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1021756.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1021756.0    \n",
      "module.adapter.frcn_linear.weight  dot:  106648000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  71263.2890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1547858.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  835.281494140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1622297.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22397948.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  110036.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31068.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  595.1461181640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  468864.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7236911.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  110036.0390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  862.7940673828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  231.4860382080078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3948.214599609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9454660105111543e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9228518.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13117245.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452668.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  618/6933] Loss: -852.9733 [iq: 9.0040,ans: 9.2211,interp: 9.1776,fusion: -880.3760]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  702366.8125    \n",
      "module.ans_embedding.weight  dot:  1241571.5    \n",
      "module.lstm.weight_ih_l0  dot:  4138060.75    \n",
      "module.lstm.weight_hh_l0  dot:  1274239.875    \n",
      "module.lstm.bias_ih_l0  dot:  109359.453125    \n",
      "module.lstm.bias_hh_l0  dot:  109359.453125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54309248.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  68208.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5578119.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5578119.0    \n",
      "module.adapter.frcn_linear.weight  dot:  94754840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  74022.25    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  364575.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  264.2509765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  388120.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20983514.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  108199.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  29842.15625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1014.1200561523438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  503814.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4597439.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  108199.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  434.336669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  138.27200317382812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  645.0709228515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9066953.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15158430.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452668.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  619/6933] Loss: -859.4165 [iq: 7.7015,ans: 7.4376,interp: 8.7624,fusion: -883.3181]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  277752.5625    \n",
      "module.ans_embedding.weight  dot:  1150484.5    \n",
      "module.lstm.weight_ih_l0  dot:  5049055.0    \n",
      "module.lstm.weight_hh_l0  dot:  4579326.0    \n",
      "module.lstm.bias_ih_l0  dot:  292737.125    \n",
      "module.lstm.bias_hh_l0  dot:  292737.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28442794.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12257.05859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1525150.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1525150.625    \n",
      "module.adapter.frcn_linear.weight  dot:  75263584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  47251.1484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  473603.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  287.3166809082031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  551821.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17318920.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78531.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14773.8056640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  239.70486450195312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  157990.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.751221472863108e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3789149.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78531.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  24.123455047607422    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.8892345428466797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  89.40180969238281    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9503113.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10238643.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452669.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  620/6933] Loss: -846.0267 [iq: 7.0536,ans: 7.2873,interp: 6.4481,fusion: -866.8157]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  376189.5625    \n",
      "module.ans_embedding.weight  dot:  1245716.0    \n",
      "module.lstm.weight_ih_l0  dot:  1560586.5    \n",
      "module.lstm.weight_hh_l0  dot:  767049.3125    \n",
      "module.lstm.bias_ih_l0  dot:  38194.921875    \n",
      "module.lstm.bias_hh_l0  dot:  38194.921875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17421120.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3015.726318359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  504979.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  504979.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  65381860.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42345.3671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  448993.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  276.31097412109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  361318.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15221116.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70904.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15528.267578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  461.83258056640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  231492.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4077088.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70904.9296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  327.1290283203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  70.53575897216797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1434.114013671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5966484.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8196045.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452669.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  621/6933] Loss: -834.4156 [iq: 10.9710,ans: 8.7316,interp: 8.7982,fusion: -862.9164]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  146443.078125    \n",
      "module.ans_embedding.weight  dot:  1250065.0    \n",
      "module.lstm.weight_ih_l0  dot:  3235388.25    \n",
      "module.lstm.weight_hh_l0  dot:  1046326.875    \n",
      "module.lstm.bias_ih_l0  dot:  197174.09375    \n",
      "module.lstm.bias_hh_l0  dot:  197174.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29098560.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7400.31787109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2583235.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2583235.0    \n",
      "module.adapter.frcn_linear.weight  dot:  75579952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51386.47265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  368992.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  245.77577209472656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  294616.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17076228.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80542.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17517.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  534.5831298828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  307613.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4392490.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80542.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  89.96910095214844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.646717071533203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  474.775146484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7776610.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15509254.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452670.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  622/6933] Loss: -852.7190 [iq: 10.9844,ans: 8.6391,interp: 8.8202,fusion: -881.1628]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  672669.1875    \n",
      "module.ans_embedding.weight  dot:  693468.625    \n",
      "module.lstm.weight_ih_l0  dot:  6264363.0    \n",
      "module.lstm.weight_hh_l0  dot:  1705364.25    \n",
      "module.lstm.bias_ih_l0  dot:  275791.8125    \n",
      "module.lstm.bias_hh_l0  dot:  275791.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15076345.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17621.76171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  490055.78125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  490055.78125    \n",
      "module.adapter.frcn_linear.weight  dot:  78315616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52657.171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  641473.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  371.53851318359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  802867.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3648104868480004e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16584133.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76551.7734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23576.70703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  419.1020202636719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  352424.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3682361.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76551.7734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  909.4291381835938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  195.52938842773438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3895.52880859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.551115123125783e-17    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6288504.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7107088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452670.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  623/6933] Loss: -845.9506 [iq: 10.7215,ans: 8.2258,interp: 8.3269,fusion: -873.2248]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2760652.5    \n",
      "module.ans_embedding.weight  dot:  1193605.875    \n",
      "module.lstm.weight_ih_l0  dot:  89976496.0    \n",
      "module.lstm.weight_hh_l0  dot:  8380645.0    \n",
      "module.lstm.bias_ih_l0  dot:  5878954.0    \n",
      "module.lstm.bias_hh_l0  dot:  5878954.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14557826.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  129597.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  337886.78125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  337886.78125    \n",
      "module.adapter.frcn_linear.weight  dot:  86897968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  60334.8359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  497063.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  328.86407470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  356237.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.459313332627062e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18782660.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  90515.3046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  51429.14453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  526.6357421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  608199.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.143885234952904e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4062131.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  90515.3046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7216.2998046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1308.3671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  22287.185546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.004086117172847e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5666447.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6339524.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452671.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  624/6933] Loss: -832.1497 [iq: 9.5181,ans: 7.6272,interp: 8.2859,fusion: -857.5809]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  695406.0    \n",
      "module.ans_embedding.weight  dot:  1187427.75    \n",
      "module.lstm.weight_ih_l0  dot:  4000957.75    \n",
      "module.lstm.weight_hh_l0  dot:  3176393.25    \n",
      "module.lstm.bias_ih_l0  dot:  157704.046875    \n",
      "module.lstm.bias_hh_l0  dot:  157704.046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30692940.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  167731.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2090888.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2090888.125    \n",
      "module.adapter.frcn_linear.weight  dot:  86706224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  62746.2109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  515028.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  278.576416015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  374382.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18037204.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86587.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16240.369140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  235.936279296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  191506.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1151436158106662e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4990209.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86587.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8037.939453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1353.6063232421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  36551.8125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7458621.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11076344.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452671.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  625/6933] Loss: -857.7320 [iq: 8.1384,ans: 7.4866,interp: 7.2370,fusion: -880.5941]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  420838.375    \n",
      "module.ans_embedding.weight  dot:  1305576.75    \n",
      "module.lstm.weight_ih_l0  dot:  4820857.0    \n",
      "module.lstm.weight_hh_l0  dot:  1418719.25    \n",
      "module.lstm.bias_ih_l0  dot:  278312.40625    \n",
      "module.lstm.bias_hh_l0  dot:  278312.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23804136.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29635.982421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  886875.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  886875.75    \n",
      "module.adapter.frcn_linear.weight  dot:  78482240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51733.2421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  496335.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  277.06756591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  327308.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18461892.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  85467.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11809.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  318.90924072265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  160337.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.583054184578941e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4924023.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  85467.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  136.2577667236328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.917085647583008    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  298.86798095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8334704.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11258290.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452672.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  626/6933] Loss: -839.5991 [iq: 11.6205,ans: 9.6553,interp: 9.9120,fusion: -870.7868]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  996192.75    \n",
      "module.ans_embedding.weight  dot:  903209.75    \n",
      "module.lstm.weight_ih_l0  dot:  3618769.0    \n",
      "module.lstm.weight_hh_l0  dot:  1878674.875    \n",
      "module.lstm.bias_ih_l0  dot:  134354.625    \n",
      "module.lstm.bias_hh_l0  dot:  134354.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26366196.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  128397.9453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2390039.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2390039.25    \n",
      "module.adapter.frcn_linear.weight  dot:  74179072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50879.37890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  375667.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  211.14633178710938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  253515.40625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17093656.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  81581.5546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25049.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  604.8284912109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  408296.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3852202.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  81581.5546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1026.604736328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  165.11148071289062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4900.7998046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.6275870368408505e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6809257.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9665889.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452672.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  627/6933] Loss: -856.4646 [iq: 8.0824,ans: 7.8092,interp: 8.8355,fusion: -881.1917]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  190565.40625    \n",
      "module.ans_embedding.weight  dot:  1198925.875    \n",
      "module.lstm.weight_ih_l0  dot:  5060812.5    \n",
      "module.lstm.weight_hh_l0  dot:  3370996.75    \n",
      "module.lstm.bias_ih_l0  dot:  349757.8125    \n",
      "module.lstm.bias_hh_l0  dot:  349757.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21139160.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71402.921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  638186.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  638186.375    \n",
      "module.adapter.frcn_linear.weight  dot:  75235904.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52755.85546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  472621.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  273.5599365234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  375012.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19158932.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95485.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9147.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  214.00587463378906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  129446.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4311172.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95485.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  144941.0625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21673.35546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  172295.1875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  7140928.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8833586.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452673.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  628/6933] Loss: -850.1743 [iq: 8.9997,ans: 8.2724,interp: 9.2856,fusion: -876.7320]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  781445.625    \n",
      "module.ans_embedding.weight  dot:  1155567.25    \n",
      "module.lstm.weight_ih_l0  dot:  8358733.5    \n",
      "module.lstm.weight_hh_l0  dot:  4097548.0    \n",
      "module.lstm.bias_ih_l0  dot:  449742.78125    \n",
      "module.lstm.bias_hh_l0  dot:  449742.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22531784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  61778.953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  649770.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  649770.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  111767744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  78728.0859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  650383.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  415.8466796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  524080.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.533365765586495e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23808754.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  115740.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16061.099609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  252.19052124023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  233937.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5639825.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  115740.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  965.793212890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  153.34349060058594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3760.3759765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8227266.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9564426.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452673.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  629/6933] Loss: -837.8474 [iq: 10.0241,ans: 8.5628,interp: 9.9360,fusion: -866.3704]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  210452.890625    \n",
      "module.ans_embedding.weight  dot:  1388791.875    \n",
      "module.lstm.weight_ih_l0  dot:  6771081.5    \n",
      "module.lstm.weight_hh_l0  dot:  2238346.5    \n",
      "module.lstm.bias_ih_l0  dot:  471084.125    \n",
      "module.lstm.bias_hh_l0  dot:  471084.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37993904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  82615.1015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4370332.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4370332.0    \n",
      "module.adapter.frcn_linear.weight  dot:  95369280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66293.0546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  470648.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  271.728515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  486882.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19829924.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94592.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22368.470703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  414.6386413574219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  367298.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4231285.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94592.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  58050.1875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11569.60546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  182034.59375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0869430355775478e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7532941.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15624824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452674.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  630/6933] Loss: -850.2748 [iq: 9.7273,ans: 8.4203,interp: 8.4977,fusion: -876.9202]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2498478.25    \n",
      "module.ans_embedding.weight  dot:  1564658.625    \n",
      "module.lstm.weight_ih_l0  dot:  60395152.0    \n",
      "module.lstm.weight_hh_l0  dot:  9585629.0    \n",
      "module.lstm.bias_ih_l0  dot:  3208761.5    \n",
      "module.lstm.bias_hh_l0  dot:  3208761.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25870368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11196.759765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  528118.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  528118.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  76272176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50152.85546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  546086.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  376.62384033203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  458089.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18059264.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  84061.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18720.982421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  411.12408447265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  290166.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5284304.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  84061.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  45.118247985839844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.818896293640137    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  107.90995788574219    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7461879.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7225064.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452674.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  631/6933] Loss: -826.2516 [iq: 10.4905,ans: 8.7056,interp: 9.1366,fusion: -854.5843]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1305784.125    \n",
      "module.ans_embedding.weight  dot:  1156348.75    \n",
      "module.lstm.weight_ih_l0  dot:  4339232.0    \n",
      "module.lstm.weight_hh_l0  dot:  1935836.75    \n",
      "module.lstm.bias_ih_l0  dot:  132438.09375    \n",
      "module.lstm.bias_hh_l0  dot:  132438.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32108352.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3849.36279296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1800439.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1800439.25    \n",
      "module.adapter.frcn_linear.weight  dot:  65584776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42131.7265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  575563.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  335.867431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  490074.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14408025.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64915.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  39108.29296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  1025.045654296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  626852.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3637709.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64915.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18.535114288330078    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.8342509269714355    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  57.92737579345703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6224022.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8678894.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452675.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  632/6933] Loss: -878.8194 [iq: 7.0799,ans: 7.0418,interp: 7.0762,fusion: -900.0173]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  15885756.0    \n",
      "module.ans_embedding.weight  dot:  1184424.125    \n",
      "module.lstm.weight_ih_l0  dot:  362715040.0    \n",
      "module.lstm.weight_hh_l0  dot:  43930440.0    \n",
      "module.lstm.bias_ih_l0  dot:  22931412.0    \n",
      "module.lstm.bias_hh_l0  dot:  22931412.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23101744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10124.5205078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1355038.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1355038.0    \n",
      "module.adapter.frcn_linear.weight  dot:  102318352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  71910.5703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  912260.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  461.1632995605469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  766632.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22747252.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  107306.6171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  32545.34765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  385.0147705078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  451701.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8765802.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  107306.6171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  919.8380126953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  226.6901092529297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3196.5556640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7587643.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11148114.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452675.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  633/6933] Loss: -851.4013 [iq: 10.3092,ans: 9.0027,interp: 9.2233,fusion: -879.9364]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  862258.125    \n",
      "module.ans_embedding.weight  dot:  863819.75    \n",
      "module.lstm.weight_ih_l0  dot:  17823558.0    \n",
      "module.lstm.weight_hh_l0  dot:  2309794.25    \n",
      "module.lstm.bias_ih_l0  dot:  1105094.5    \n",
      "module.lstm.bias_hh_l0  dot:  1105094.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16315128.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7885.38720703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  601647.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  601647.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  106567424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72266.8671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  722979.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  385.12066650390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  595295.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22163692.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  105659.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  27333.83203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  438.26495361328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  415347.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5184535.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  105659.0390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  85.79876708984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.668964385986328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  310.91864013671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6318306.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7802292.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452676.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  634/6933] Loss: -850.7881 [iq: 8.9076,ans: 7.9443,interp: 8.2178,fusion: -875.8577]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  155730.0    \n",
      "module.ans_embedding.weight  dot:  1865848.625    \n",
      "module.lstm.weight_ih_l0  dot:  4561198.0    \n",
      "module.lstm.weight_hh_l0  dot:  2667747.0    \n",
      "module.lstm.bias_ih_l0  dot:  327896.5    \n",
      "module.lstm.bias_hh_l0  dot:  327896.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  51499576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20627.87109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8153598.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8153598.5    \n",
      "module.adapter.frcn_linear.weight  dot:  74633568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50772.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  619518.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  342.81121826171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  557993.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18532696.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  87005.8359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17858.416015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  367.6339416503906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  298073.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.798597157991026e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5741967.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  87005.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1655.5311279296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  347.4727783203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4690.37060546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9685873.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  24767746.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452676.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  635/6933] Loss: -844.6124 [iq: 7.7366,ans: 7.8243,interp: 7.4904,fusion: -867.6636]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1799343.375    \n",
      "module.ans_embedding.weight  dot:  1799511.5    \n",
      "module.lstm.weight_ih_l0  dot:  30288810.0    \n",
      "module.lstm.weight_hh_l0  dot:  3483498.5    \n",
      "module.lstm.bias_ih_l0  dot:  1868999.5    \n",
      "module.lstm.bias_hh_l0  dot:  1868999.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  44194040.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28831.59375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1515769.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1515769.75    \n",
      "module.adapter.frcn_linear.weight  dot:  115994984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  76082.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  835687.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  585.5399169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  603750.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22486474.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  103962.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  51710.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  686.91162109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  560537.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5501583.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  103962.2109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1005.174560546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  241.39744567871094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2726.47607421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4948931809376518e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7995666.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8776988.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452677.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  636/6933] Loss: -820.7197 [iq: 10.0241,ans: 8.4283,interp: 9.0678,fusion: -848.2399]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1773238.0    \n",
      "module.ans_embedding.weight  dot:  977620.4375    \n",
      "module.lstm.weight_ih_l0  dot:  34584328.0    \n",
      "module.lstm.weight_hh_l0  dot:  6762690.5    \n",
      "module.lstm.bias_ih_l0  dot:  2110087.0    \n",
      "module.lstm.bias_hh_l0  dot:  2110087.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32746722.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  54817.4140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2165938.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2165938.0    \n",
      "module.adapter.frcn_linear.weight  dot:  113869528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  75162.90625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  853677.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  438.6465148925781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  713916.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27314100.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  123995.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35692.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  571.8704223632812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  598683.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5047035617499205e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6300748.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  123995.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3678.568359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  561.1763305664062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10470.1005859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8661861.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11287604.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452677.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  637/6933] Loss: -851.4869 [iq: 8.5536,ans: 8.4092,interp: 8.2925,fusion: -876.7422]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  194235.875    \n",
      "module.ans_embedding.weight  dot:  851536.25    \n",
      "module.lstm.weight_ih_l0  dot:  1299308.875    \n",
      "module.lstm.weight_hh_l0  dot:  591275.1875    \n",
      "module.lstm.bias_ih_l0  dot:  50628.0859375    \n",
      "module.lstm.bias_hh_l0  dot:  50628.0859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12309877.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5623.87158203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  306449.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  306449.75    \n",
      "module.adapter.frcn_linear.weight  dot:  62700800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39606.5390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  483846.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.4775390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  376485.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13457383.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61683.56640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10408.4267578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  297.69549560546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  152401.390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3459442.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61683.56640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  238.41250610351562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.64935302734375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  864.2537841796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5716093.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6517031.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452678.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  638/6933] Loss: -854.2596 [iq: 10.0500,ans: 8.8460,interp: 8.6662,fusion: -881.8218]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  435071.75    \n",
      "module.ans_embedding.weight  dot:  1881914.75    \n",
      "module.lstm.weight_ih_l0  dot:  6257865.5    \n",
      "module.lstm.weight_hh_l0  dot:  3805820.0    \n",
      "module.lstm.bias_ih_l0  dot:  465781.375    \n",
      "module.lstm.bias_hh_l0  dot:  465781.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33728024.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37086.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1404385.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1404385.875    \n",
      "module.adapter.frcn_linear.weight  dot:  74123712.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48232.5234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  758236.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  430.1899108886719    \n",
      "module.attflat_img.mlp.linear.weight  dot:  670283.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18084176.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86015.8828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8997.2001953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  283.0606384277344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  123597.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.02446117834188e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5177657.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86015.8828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3540.593994140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  932.7902221679688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11774.01171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7573273.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13426868.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452678.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  639/6933] Loss: -837.3693 [iq: 7.6528,ans: 7.4837,interp: 7.5247,fusion: -860.0304]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  394210.9375    \n",
      "module.ans_embedding.weight  dot:  1086151.125    \n",
      "module.lstm.weight_ih_l0  dot:  8821157.0    \n",
      "module.lstm.weight_hh_l0  dot:  5387311.0    \n",
      "module.lstm.bias_ih_l0  dot:  507563.6875    \n",
      "module.lstm.bias_hh_l0  dot:  507563.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18162990.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7143.30810546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  638178.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  638178.375    \n",
      "module.adapter.frcn_linear.weight  dot:  70063160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51611.3828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  306101.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  186.67144775390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  256557.359375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  17502262.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  85473.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13395.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  342.67431640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  251114.953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3046843605479808e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6536933.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  85473.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  897.8265380859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  216.59132385253906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3884.619384765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7790365.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8293983.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452679.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  640/6933] Loss: -858.5413 [iq: 8.6514,ans: 8.1435,interp: 8.0550,fusion: -883.3912]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  237409.796875    \n",
      "module.ans_embedding.weight  dot:  1338189.25    \n",
      "module.lstm.weight_ih_l0  dot:  6050093.0    \n",
      "module.lstm.weight_hh_l0  dot:  1321129.5    \n",
      "module.lstm.bias_ih_l0  dot:  400752.09375    \n",
      "module.lstm.bias_hh_l0  dot:  400752.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  53618992.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37877.44921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5099542.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5099542.5    \n",
      "module.adapter.frcn_linear.weight  dot:  59727840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40320.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  331396.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  190.68206787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  342812.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15263484.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71683.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15438.962890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  362.9671630859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  278709.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3132954.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71683.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1212.425048828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  272.40948486328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1602.773193359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9286802.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14941183.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452680.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  641/6933] Loss: -851.7682 [iq: 6.2260,ans: 6.2470,interp: 5.8054,fusion: -870.0466]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1592456.625    \n",
      "module.ans_embedding.weight  dot:  733459.5    \n",
      "module.lstm.weight_ih_l0  dot:  34373512.0    \n",
      "module.lstm.weight_hh_l0  dot:  10005267.0    \n",
      "module.lstm.bias_ih_l0  dot:  2219120.0    \n",
      "module.lstm.bias_hh_l0  dot:  2219120.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15340625.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5398.02978515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  407781.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  407781.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  122325800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  87201.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  890997.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  453.43927001953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  739984.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23234792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  112951.7734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28401.921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  349.08489990234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  389285.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8645006.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  112951.7734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  74.32414245605469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.09363555908203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  169.16806030273438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7104342.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7974569.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452680.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  642/6933] Loss: -809.4845 [iq: 9.6809,ans: 8.7808,interp: 9.1883,fusion: -837.1345]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1247206.0    \n",
      "module.ans_embedding.weight  dot:  383491.0    \n",
      "module.lstm.weight_ih_l0  dot:  33313676.0    \n",
      "module.lstm.weight_hh_l0  dot:  7779567.0    \n",
      "module.lstm.bias_ih_l0  dot:  2253434.75    \n",
      "module.lstm.bias_hh_l0  dot:  2253434.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10667346.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3203.44580078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  339009.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  339009.375    \n",
      "module.adapter.frcn_linear.weight  dot:  132903216.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88159.03125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1009499.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  556.0470581054688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1124262.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26271634.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  115203.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  41448.83984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  678.6005859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  632225.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8464600.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  115203.5078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  43.15411376953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.487333297729492    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  88.8136978149414    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6335491.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7409389.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452680.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  643/6933] Loss: -784.4398 [iq: 10.3229,ans: 9.4455,interp: 10.9435,fusion: -815.1517]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  459796.53125    \n",
      "module.ans_embedding.weight  dot:  2147934.5    \n",
      "module.lstm.weight_ih_l0  dot:  14044514.0    \n",
      "module.lstm.weight_hh_l0  dot:  3540893.0    \n",
      "module.lstm.bias_ih_l0  dot:  886912.375    \n",
      "module.lstm.bias_hh_l0  dot:  886912.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32178982.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2017.6396484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1026787.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1026787.375    \n",
      "module.adapter.frcn_linear.weight  dot:  82815088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57872.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  534664.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  304.4192810058594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  389637.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  19722938.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  93968.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25368.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  600.2490234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  468150.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.701266683085123e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5850484.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  93968.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  23.46794319152832    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.609736442565918    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  74.37248229980469    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.535394613318203e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7557423.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9146270.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452681.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  644/6933] Loss: -869.6298 [iq: 9.4250,ans: 8.6019,interp: 8.8790,fusion: -896.5357]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1988604.0    \n",
      "module.ans_embedding.weight  dot:  1328473.75    \n",
      "module.lstm.weight_ih_l0  dot:  41728468.0    \n",
      "module.lstm.weight_hh_l0  dot:  6896262.0    \n",
      "module.lstm.bias_ih_l0  dot:  2515894.5    \n",
      "module.lstm.bias_hh_l0  dot:  2515894.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22519834.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12919.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  590325.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  590325.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  94897776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57549.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  564574.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  338.0335693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  469935.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.143885234952904e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21352336.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86649.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  32391.24609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  540.0526123046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  400172.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.533365765586495e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6548363.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86649.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  246.97689819335938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  66.47274780273438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  729.8909912109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7204085.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9738741.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452682.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  645/6933] Loss: -805.8245 [iq: 10.2994,ans: 8.3460,interp: 8.4711,fusion: -832.9410]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1488089.25    \n",
      "module.ans_embedding.weight  dot:  1114257.5    \n",
      "module.lstm.weight_ih_l0  dot:  32007070.0    \n",
      "module.lstm.weight_hh_l0  dot:  12061072.0    \n",
      "module.lstm.bias_ih_l0  dot:  2224540.25    \n",
      "module.lstm.bias_hh_l0  dot:  2224540.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28515908.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  45297.99609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1571998.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1571998.25    \n",
      "module.adapter.frcn_linear.weight  dot:  89268744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  59856.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  533236.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  340.25    \n",
      "module.attflat_img.mlp.linear.weight  dot:  553876.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21798772.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  100637.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26037.751953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  692.6514892578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  373856.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.407891189359361e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7042918.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  100637.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1209.3465576171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  489.98681640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2336.1552734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8219910.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9653140.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452682.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  646/6933] Loss: -804.8871 [iq: 11.0469,ans: 8.8841,interp: 8.8933,fusion: -833.7114]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2048752.25    \n",
      "module.ans_embedding.weight  dot:  1279900.5    \n",
      "module.lstm.weight_ih_l0  dot:  32981000.0    \n",
      "module.lstm.weight_hh_l0  dot:  3588031.5    \n",
      "module.lstm.bias_ih_l0  dot:  2224961.0    \n",
      "module.lstm.bias_hh_l0  dot:  2224961.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25789520.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  142093.15625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  890841.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  890841.0    \n",
      "module.adapter.frcn_linear.weight  dot:  82366832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52887.00390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  683807.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  361.9671630859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  588609.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18473594.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  79661.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17609.107421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  309.9293212890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  210329.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.74823388888035e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3727249.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  79661.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2447.824951171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  558.7816162109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3953.870361328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.765787929907674e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6638235.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6402122.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452682.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  647/6933] Loss: -819.9430 [iq: 9.4201,ans: 8.0568,interp: 8.1670,fusion: -845.5870]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1260978.75    \n",
      "module.ans_embedding.weight  dot:  1044033.875    \n",
      "module.lstm.weight_ih_l0  dot:  11745653.0    \n",
      "module.lstm.weight_hh_l0  dot:  2188930.25    \n",
      "module.lstm.bias_ih_l0  dot:  525138.375    \n",
      "module.lstm.bias_hh_l0  dot:  525138.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21411306.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9615.8486328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  549248.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  549248.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  63443064.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44033.1328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  458808.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  282.8782653808594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  430654.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.0859723665344063e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16527376.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73111.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9343.671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  192.186767578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144430.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0206804290646687e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5410761.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73111.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  89.04048156738281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  22.072370529174805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  142.5580291748047    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.2660098504020425e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7357755.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8668792.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452683.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  648/6933] Loss: -843.0262 [iq: 9.7004,ans: 8.5302,interp: 9.0356,fusion: -870.2923]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  655614.125    \n",
      "module.ans_embedding.weight  dot:  396959.03125    \n",
      "module.lstm.weight_ih_l0  dot:  11678416.0    \n",
      "module.lstm.weight_hh_l0  dot:  2538428.0    \n",
      "module.lstm.bias_ih_l0  dot:  748056.3125    \n",
      "module.lstm.bias_hh_l0  dot:  748056.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10976339.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10722.8271484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  289490.71875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  289490.71875    \n",
      "module.adapter.frcn_linear.weight  dot:  83910496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  55843.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  650404.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  378.6793212890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  486518.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20483190.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  89875.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17564.90234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  411.3249206542969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  336974.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3871840565116145e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5913968.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  89875.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  555.4164428710938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  117.00983428955078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2686.80517578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6489359.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6930847.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452683.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  649/6933] Loss: -891.7433 [iq: 8.3019,ans: 8.4145,interp: 8.2487,fusion: -916.7084]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3466027.25    \n",
      "module.ans_embedding.weight  dot:  649305.75    \n",
      "module.lstm.weight_ih_l0  dot:  50119632.0    \n",
      "module.lstm.weight_hh_l0  dot:  5755317.5    \n",
      "module.lstm.bias_ih_l0  dot:  3134125.25    \n",
      "module.lstm.bias_hh_l0  dot:  3134125.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32505960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  47814.33203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2122370.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2122370.0    \n",
      "module.adapter.frcn_linear.weight  dot:  109282032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  75504.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  825640.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  436.40582275390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  760319.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23211536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  99112.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17759.583984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  538.7958984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  299744.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7779849.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  99112.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  667.442138671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  144.427734375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1692.0050048828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  10449702.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12123312.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452683.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  650/6933] Loss: -821.3253 [iq: 10.4369,ans: 9.5301,interp: 9.5065,fusion: -850.7989]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1884049.625    \n",
      "module.ans_embedding.weight  dot:  826416.625    \n",
      "module.lstm.weight_ih_l0  dot:  32739480.0    \n",
      "module.lstm.weight_hh_l0  dot:  4748647.0    \n",
      "module.lstm.bias_ih_l0  dot:  1954971.5    \n",
      "module.lstm.bias_hh_l0  dot:  1954971.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18607632.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20603.38671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  822943.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  822943.75    \n",
      "module.adapter.frcn_linear.weight  dot:  106110432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72290.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  608202.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  377.76605224609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  451324.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23142972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  98599.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  42747.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  356.853271484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  539884.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5471104.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  98599.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  986.4049682617188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  170.53768920898438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2411.29736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7470457.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7735723.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452684.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  651/6933] Loss: -839.9896 [iq: 9.1403,ans: 8.6764,interp: 8.9500,fusion: -866.7562]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  174028.15625    \n",
      "module.ans_embedding.weight  dot:  905554.0625    \n",
      "module.lstm.weight_ih_l0  dot:  2418221.75    \n",
      "module.lstm.weight_hh_l0  dot:  2250481.25    \n",
      "module.lstm.bias_ih_l0  dot:  137769.390625    \n",
      "module.lstm.bias_hh_l0  dot:  137769.390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19794740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21135.15625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  477371.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  477371.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  59501856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37266.3515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  533512.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  292.50164794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  450848.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14345227.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60799.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17202.28125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  430.83782958984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  328323.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.822151484200731e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4479251.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60799.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  266.038818359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.29910659790039    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  445.05535888671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6540899.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6226903.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452684.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  652/6933] Loss: -881.3431 [iq: 8.3971,ans: 8.3591,interp: 10.1337,fusion: -908.2331]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1572287.75    \n",
      "module.ans_embedding.weight  dot:  964664.375    \n",
      "module.lstm.weight_ih_l0  dot:  29284454.0    \n",
      "module.lstm.weight_hh_l0  dot:  7562226.5    \n",
      "module.lstm.bias_ih_l0  dot:  1881399.5    \n",
      "module.lstm.bias_hh_l0  dot:  1881399.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22172032.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15268.4580078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  942774.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  942774.5    \n",
      "module.adapter.frcn_linear.weight  dot:  108312384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72890.640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  709914.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  393.7926940917969    \n",
      "module.attflat_img.mlp.linear.weight  dot:  584101.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  28543930.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  118338.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26238.37109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  570.3776245117188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  490996.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8605032.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  118338.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  603.43994140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  153.1024932861328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2538.126953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8616865.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13150961.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452685.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  653/6933] Loss: -868.4920 [iq: 9.3006,ans: 8.9152,interp: 8.6131,fusion: -895.3209]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  818697.875    \n",
      "module.ans_embedding.weight  dot:  1511418.25    \n",
      "module.lstm.weight_ih_l0  dot:  9101068.0    \n",
      "module.lstm.weight_hh_l0  dot:  1782135.25    \n",
      "module.lstm.bias_ih_l0  dot:  490208.0625    \n",
      "module.lstm.bias_hh_l0  dot:  490208.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27409484.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22491.869140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  671292.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  671292.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  80365760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53019.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  545709.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.2640380859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  393159.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20375946.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  85216.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37662.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  465.4532470703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  379121.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.370246304257307e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5028667.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  85216.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4836.529296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1090.185546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  18108.953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7235987.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11123056.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452685.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  654/6933] Loss: -845.1915 [iq: 9.4858,ans: 8.7082,interp: 8.7556,fusion: -872.1412]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  970012.8125    \n",
      "module.ans_embedding.weight  dot:  1623587.25    \n",
      "module.lstm.weight_ih_l0  dot:  10418619.0    \n",
      "module.lstm.weight_hh_l0  dot:  2076916.75    \n",
      "module.lstm.bias_ih_l0  dot:  613294.8125    \n",
      "module.lstm.bias_hh_l0  dot:  613294.8125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39475288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11681.1181640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1763053.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1763053.0    \n",
      "module.adapter.frcn_linear.weight  dot:  90596976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58221.57421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  743172.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  399.9605407714844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  591836.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23076370.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  93554.703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18270.78515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  468.9366149902344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  203065.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5004380.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  93554.703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  156.7136993408203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.73358917236328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  386.7684326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9454660105111543e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8414608.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12248510.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452686.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  655/6933] Loss: -818.6575 [iq: 11.0153,ans: 9.6227,interp: 9.6045,fusion: -848.9000]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  323816.53125    \n",
      "module.ans_embedding.weight  dot:  1378004.0    \n",
      "module.lstm.weight_ih_l0  dot:  5852925.0    \n",
      "module.lstm.weight_hh_l0  dot:  1256758.5    \n",
      "module.lstm.bias_ih_l0  dot:  336335.75    \n",
      "module.lstm.bias_hh_l0  dot:  336335.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  30196964.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  53875.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1820017.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1820017.625    \n",
      "module.adapter.frcn_linear.weight  dot:  65308384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40188.76953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  612631.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  336.5454406738281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  791081.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0825260687852278e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17215536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67678.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15376.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  280.6422119140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  293852.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4725863.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67678.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1165.566650390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  323.30511474609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2897.39794921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7852839.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11674032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452686.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  656/6933] Loss: -835.4411 [iq: 9.6225,ans: 8.5545,interp: 10.1209,fusion: -863.7390]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  124025.0703125    \n",
      "module.ans_embedding.weight  dot:  1452265.0    \n",
      "module.lstm.weight_ih_l0  dot:  2094088.125    \n",
      "module.lstm.weight_hh_l0  dot:  1270894.125    \n",
      "module.lstm.bias_ih_l0  dot:  118612.671875    \n",
      "module.lstm.bias_hh_l0  dot:  118612.671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21750082.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10235.3056640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1346039.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1346039.75    \n",
      "module.adapter.frcn_linear.weight  dot:  70427544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45969.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  619667.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  344.232421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  498529.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.609784471336752e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18127732.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72977.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14589.009765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  325.41180419921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  260299.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5367718.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72977.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  242.69607543945312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  74.33094787597656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  321.7628479003906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.060308059360977e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6484828.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10284417.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452687.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  657/6933] Loss: -868.7985 [iq: 8.9065,ans: 8.2176,interp: 8.2948,fusion: -894.2173]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1317233.625    \n",
      "module.ans_embedding.weight  dot:  781385.5    \n",
      "module.lstm.weight_ih_l0  dot:  15324085.0    \n",
      "module.lstm.weight_hh_l0  dot:  3313855.0    \n",
      "module.lstm.bias_ih_l0  dot:  866358.75    \n",
      "module.lstm.bias_hh_l0  dot:  866358.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19755792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5868.15869140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  613666.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  613666.125    \n",
      "module.adapter.frcn_linear.weight  dot:  91895640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  61838.5234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  672902.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  327.47930908203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  505881.15625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.734787115827203e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21574684.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  87307.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  53501.48046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  381.53375244140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  532845.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5244788.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  87307.6328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13.401731491088867    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.2696746587753296    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  66.32831573486328    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6738049.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7308933.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452687.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  658/6933] Loss: -845.4294 [iq: 9.9422,ans: 8.6178,interp: 9.1211,fusion: -873.1105]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  150224.453125    \n",
      "module.ans_embedding.weight  dot:  1320001.0    \n",
      "module.lstm.weight_ih_l0  dot:  3051487.75    \n",
      "module.lstm.weight_hh_l0  dot:  937941.4375    \n",
      "module.lstm.bias_ih_l0  dot:  225021.59375    \n",
      "module.lstm.bias_hh_l0  dot:  225021.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33201420.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13774.62109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3041269.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3041269.5    \n",
      "module.adapter.frcn_linear.weight  dot:  73995360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45859.28515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  315618.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  182.90142822265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  214372.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.1125182431424037e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18410388.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71159.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17052.73828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  700.879150390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  359503.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3777347.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71159.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  106.07351684570312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.413734436035156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  370.74578857421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7128072.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12435070.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452688.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  659/6933] Loss: -827.7892 [iq: 10.1078,ans: 8.5831,interp: 8.9133,fusion: -855.3934]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8247053.0    \n",
      "module.ans_embedding.weight  dot:  1384901.75    \n",
      "module.lstm.weight_ih_l0  dot:  143757760.0    \n",
      "module.lstm.weight_hh_l0  dot:  20509836.0    \n",
      "module.lstm.bias_ih_l0  dot:  8538687.0    \n",
      "module.lstm.bias_hh_l0  dot:  8538687.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27287672.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4272.5546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2361804.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2361804.0    \n",
      "module.adapter.frcn_linear.weight  dot:  182969440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  139203.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1297383.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  780.3781127929688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1309576.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  40260868.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  170304.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  154019.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  800.1357421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  1811234.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9254230210208334e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8807232.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  170304.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  38.611717224121094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.643083572387695    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  40.66252517700195    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6817613.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10373489.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452688.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  660/6933] Loss: -807.7492 [iq: 5.6390,ans: 5.9223,interp: 5.3459,fusion: -824.6564]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3200215.75    \n",
      "module.ans_embedding.weight  dot:  1136270.5    \n",
      "module.lstm.weight_ih_l0  dot:  78688368.0    \n",
      "module.lstm.weight_hh_l0  dot:  12065154.0    \n",
      "module.lstm.bias_ih_l0  dot:  4913910.0    \n",
      "module.lstm.bias_hh_l0  dot:  4913910.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28084512.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26830.759765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1330850.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1330850.25    \n",
      "module.adapter.frcn_linear.weight  dot:  153068416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  97075.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2139027.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  1081.45849609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2533329.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  32632660.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  132336.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  44146.15625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  676.3087768554688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  520954.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.555378710501827e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8636366.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  132336.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13271.9453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2382.60546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  42119.2265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  8471932.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10253090.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452689.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  661/6933] Loss: -821.2931 [iq: 10.2897,ans: 9.9318,interp: 12.6563,fusion: -854.1709]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  262872.5625    \n",
      "module.ans_embedding.weight  dot:  1207523.5    \n",
      "module.lstm.weight_ih_l0  dot:  3767741.5    \n",
      "module.lstm.weight_hh_l0  dot:  948504.8125    \n",
      "module.lstm.bias_ih_l0  dot:  215938.15625    \n",
      "module.lstm.bias_hh_l0  dot:  215938.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  49081016.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  163113.265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3445603.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3445603.0    \n",
      "module.adapter.frcn_linear.weight  dot:  114579896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  75934.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  815747.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  382.91290283203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  736445.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  24532056.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  100952.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20927.04296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  661.8673095703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  454009.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5252183.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  100952.0546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13631.27734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1889.3148193359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  40172.58984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.004086117172847e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9063921.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12826170.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452689.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  662/6933] Loss: -875.8456 [iq: 8.3354,ans: 7.9803,interp: 7.9319,fusion: -900.0932]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2880455.25    \n",
      "module.ans_embedding.weight  dot:  733872.75    \n",
      "module.lstm.weight_ih_l0  dot:  47804128.0    \n",
      "module.lstm.weight_hh_l0  dot:  7556210.5    \n",
      "module.lstm.bias_ih_l0  dot:  2803494.0    \n",
      "module.lstm.bias_hh_l0  dot:  2803494.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18478392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11072.3828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  786122.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  786122.5    \n",
      "module.adapter.frcn_linear.weight  dot:  92353680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  59110.01953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  867310.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  504.640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  711848.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  21821004.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  88935.140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  47387.5234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  417.0220642089844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  620228.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.751221472863108e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4959796.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  88935.140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  265.5302734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.347787857055664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1222.5986328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6183481.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7678099.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452690.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  663/6933] Loss: -857.0467 [iq: 9.0950,ans: 7.8444,interp: 7.9436,fusion: -881.9297]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  265238.4375    \n",
      "module.ans_embedding.weight  dot:  2308940.5    \n",
      "module.lstm.weight_ih_l0  dot:  3402397.5    \n",
      "module.lstm.weight_hh_l0  dot:  2332347.25    \n",
      "module.lstm.bias_ih_l0  dot:  186308.34375    \n",
      "module.lstm.bias_hh_l0  dot:  186308.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37380944.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  98098.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2495552.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2495552.25    \n",
      "module.adapter.frcn_linear.weight  dot:  47220288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30014.3984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  551491.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.39019775390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  463544.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.115907697472721e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13493972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54692.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6896.79052734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  185.3657989501953    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  46938.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4141578453272814e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3593007.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54692.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12240.8203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2288.1923828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  22815.27734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  8431103.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13357290.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452690.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  664/6933] Loss: -828.1133 [iq: 11.1938,ans: 9.0250,interp: 9.1511,fusion: -857.4832]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5299792.5    \n",
      "module.ans_embedding.weight  dot:  973143.125    \n",
      "module.lstm.weight_ih_l0  dot:  149453760.0    \n",
      "module.lstm.weight_hh_l0  dot:  21140524.0    \n",
      "module.lstm.bias_ih_l0  dot:  9378766.0    \n",
      "module.lstm.bias_hh_l0  dot:  9378766.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20715364.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35490.34375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1548790.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1548790.25    \n",
      "module.adapter.frcn_linear.weight  dot:  143973776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  98425.578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1216384.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  654.91455078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  936577.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1408701539039612e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  31072734.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  130257.8828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  68613.75    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  624.3716430664062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  994267.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5984080.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  130257.8828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1012.524169921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  205.44705200195312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2069.78662109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6901420.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9481522.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452691.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  665/6933] Loss: -827.1154 [iq: 9.9473,ans: 8.4327,interp: 8.6749,fusion: -854.1703]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3155743.75    \n",
      "module.ans_embedding.weight  dot:  1334381.25    \n",
      "module.lstm.weight_ih_l0  dot:  56781864.0    \n",
      "module.lstm.weight_hh_l0  dot:  6696465.5    \n",
      "module.lstm.bias_ih_l0  dot:  3331856.0    \n",
      "module.lstm.bias_hh_l0  dot:  3331856.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26032216.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15808.53515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1244940.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1244940.875    \n",
      "module.adapter.frcn_linear.weight  dot:  88012128.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58550.08984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  635921.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  352.13958740234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  397104.59375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22672398.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  93926.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  48580.8984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  771.6076049804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  534987.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5788951.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  93926.8515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  211.827880859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.061445236206055    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  389.59857177734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7305929.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10257342.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452692.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  666/6933] Loss: -846.1697 [iq: 9.8403,ans: 9.0898,interp: 8.8752,fusion: -873.9750]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  440910.75    \n",
      "module.ans_embedding.weight  dot:  1267657.0    \n",
      "module.lstm.weight_ih_l0  dot:  3261015.75    \n",
      "module.lstm.weight_hh_l0  dot:  839812.5    \n",
      "module.lstm.bias_ih_l0  dot:  156595.78125    \n",
      "module.lstm.bias_hh_l0  dot:  156595.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27835352.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11779.17578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2419523.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2419523.25    \n",
      "module.adapter.frcn_linear.weight  dot:  79818424.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52567.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  522857.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  342.8277587890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  394299.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20226932.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  84804.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9279.0048828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  283.9020080566406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  142952.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3194388631964102e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4173124.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  84804.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  57.86296844482422    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.41476058959961    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  151.62924194335938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7229878.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11457586.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452692.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  667/6933] Loss: -861.5282 [iq: 8.7753,ans: 7.8661,interp: 8.9032,fusion: -887.0729]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  145848.4375    \n",
      "module.ans_embedding.weight  dot:  1229873.875    \n",
      "module.lstm.weight_ih_l0  dot:  4221032.0    \n",
      "module.lstm.weight_hh_l0  dot:  1689524.25    \n",
      "module.lstm.bias_ih_l0  dot:  268127.0    \n",
      "module.lstm.bias_hh_l0  dot:  268127.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25550070.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23935.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1140949.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1140949.125    \n",
      "module.adapter.frcn_linear.weight  dot:  98558944.0    \n",
      "module.adapter.frcn_linear.bias  dot:  59102.9609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1071968.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  559.4305419921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  845934.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.022012944915332e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22728870.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94751.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8758.201171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  239.42727661132812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  140709.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6241268.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94751.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2068.249755859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  389.599365234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4773.84326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8264896.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13181824.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452693.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  668/6933] Loss: -856.5430 [iq: 9.0529,ans: 8.1671,interp: 9.8269,fusion: -883.5899]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  176971.84375    \n",
      "module.ans_embedding.weight  dot:  928029.75    \n",
      "module.lstm.weight_ih_l0  dot:  890865.625    \n",
      "module.lstm.weight_hh_l0  dot:  667381.375    \n",
      "module.lstm.bias_ih_l0  dot:  37212.3125    \n",
      "module.lstm.bias_hh_l0  dot:  37212.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21858128.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  209506.71875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1278971.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1278971.375    \n",
      "module.adapter.frcn_linear.weight  dot:  65204936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  49913.01171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  220699.09375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  140.39865112304688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  172605.59375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19336416.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86247.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16728.5    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  395.5650939941406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  340478.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0510348147363402e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4559087.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86247.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  45844.3984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7964.1650390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  161048.015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7628394.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9035640.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452693.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  669/6933] Loss: -886.7272 [iq: 8.2144,ans: 8.3704,interp: 7.8270,fusion: -911.1389]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4264790.5    \n",
      "module.ans_embedding.weight  dot:  930306.75    \n",
      "module.lstm.weight_ih_l0  dot:  82392776.0    \n",
      "module.lstm.weight_hh_l0  dot:  9192168.0    \n",
      "module.lstm.bias_ih_l0  dot:  3902159.0    \n",
      "module.lstm.bias_hh_l0  dot:  3902159.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  43717260.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41362.68359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2942734.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2942734.5    \n",
      "module.adapter.frcn_linear.weight  dot:  79961000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50593.30859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  637104.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  351.6337890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  489128.40625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17953646.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73984.625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12271.109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  218.9757843017578    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  126478.6484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5933079.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73984.625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  279.5750732421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.340911865234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  500.30572509765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.403677505455562e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9781538.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11014454.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452694.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  670/6933] Loss: -801.0385 [iq: 11.3425,ans: 9.4796,interp: 10.1818,fusion: -832.0423]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  372865.0625    \n",
      "module.ans_embedding.weight  dot:  1143513.5    \n",
      "module.lstm.weight_ih_l0  dot:  5506500.0    \n",
      "module.lstm.weight_hh_l0  dot:  3159990.5    \n",
      "module.lstm.bias_ih_l0  dot:  361389.25    \n",
      "module.lstm.bias_hh_l0  dot:  361389.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28874300.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19227.505859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1071486.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1071486.25    \n",
      "module.adapter.frcn_linear.weight  dot:  92088352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  70467.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  595378.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  338.9567565917969    \n",
      "module.attflat_img.mlp.linear.weight  dot:  571152.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20928534.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95836.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10519.6396484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  320.01690673828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  195377.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2965309653955046e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4947167.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95836.9296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  330.3983154296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  67.83131408691406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1231.2073974609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8600160.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10393467.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452694.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  671/6933] Loss: -879.7800 [iq: 8.5705,ans: 7.8355,interp: 7.6947,fusion: -903.8807]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  447993.8125    \n",
      "module.ans_embedding.weight  dot:  1030726.0    \n",
      "module.lstm.weight_ih_l0  dot:  3926143.5    \n",
      "module.lstm.weight_hh_l0  dot:  1357390.375    \n",
      "module.lstm.bias_ih_l0  dot:  169219.625    \n",
      "module.lstm.bias_hh_l0  dot:  169219.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18624924.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  60850.28515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  598487.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  598487.375    \n",
      "module.adapter.frcn_linear.weight  dot:  66919624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40832.51171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  596316.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  305.429443359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  616757.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.0595401767641306e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15395338.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62931.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19803.6875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  341.4105224609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  368003.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0206804290646687e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3951909.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62931.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3244.857177734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  636.5082397460938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5463.203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.377298440909726e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6296228.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6877926.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452695.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  672/6933] Loss: -852.6182 [iq: 10.2274,ans: 8.8293,interp: 9.3589,fusion: -881.0339]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  985233.375    \n",
      "module.ans_embedding.weight  dot:  1170546.5    \n",
      "module.lstm.weight_ih_l0  dot:  4717233.0    \n",
      "module.lstm.weight_hh_l0  dot:  1173827.0    \n",
      "module.lstm.bias_ih_l0  dot:  71340.171875    \n",
      "module.lstm.bias_hh_l0  dot:  71340.171875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19492170.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  92126.328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1091121.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1091121.125    \n",
      "module.adapter.frcn_linear.weight  dot:  58468092.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34711.1796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  444068.59375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  287.3759460449219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  327886.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.883965397719294e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15573161.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62159.0703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13100.595703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  369.5917663574219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  111230.3984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4250048.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62159.0703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  122685.578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15628.462890625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  148310.0    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6826533.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9808651.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452695.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  673/6933] Loss: -871.2688 [iq: 8.9267,ans: 8.0008,interp: 8.0797,fusion: -896.2760]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  202374.0    \n",
      "module.ans_embedding.weight  dot:  1071714.625    \n",
      "module.lstm.weight_ih_l0  dot:  2727580.0    \n",
      "module.lstm.weight_hh_l0  dot:  1494203.25    \n",
      "module.lstm.bias_ih_l0  dot:  153780.65625    \n",
      "module.lstm.bias_hh_l0  dot:  153780.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20937088.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  54344.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1272151.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1272151.375    \n",
      "module.adapter.frcn_linear.weight  dot:  68564688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45046.7421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  661996.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  365.882568359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  544986.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16958602.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73788.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21034.9765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  441.05303955078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  419668.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6216583.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73788.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  117621.84375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17727.5625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  132203.5    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.045164369083977e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7597236.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9029830.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452696.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  674/6933] Loss: -881.8002 [iq: 10.4847,ans: 9.4396,interp: 9.3605,fusion: -911.0850]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  561101.1875    \n",
      "module.ans_embedding.weight  dot:  865079.0    \n",
      "module.lstm.weight_ih_l0  dot:  6246529.0    \n",
      "module.lstm.weight_hh_l0  dot:  1266560.0    \n",
      "module.lstm.bias_ih_l0  dot:  246867.640625    \n",
      "module.lstm.bias_hh_l0  dot:  246867.640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21988956.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11892.0546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1413596.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1413596.875    \n",
      "module.adapter.frcn_linear.weight  dot:  61123976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36782.8828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  859840.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  493.3506774902344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  683361.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15239384.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61008.58984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24865.439453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  585.5813598632812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  491433.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4762724.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61008.58984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  115.79548645019531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.26715660095215    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  262.5682678222656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7219707.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8465628.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452697.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  675/6933] Loss: -892.6923 [iq: 8.1506,ans: 8.0530,interp: 8.6848,fusion: -917.5808]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2324543.0    \n",
      "module.ans_embedding.weight  dot:  1062957.625    \n",
      "module.lstm.weight_ih_l0  dot:  32822598.0    \n",
      "module.lstm.weight_hh_l0  dot:  4381054.0    \n",
      "module.lstm.bias_ih_l0  dot:  1841484.625    \n",
      "module.lstm.bias_hh_l0  dot:  1841484.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20292184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23694.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1092594.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1092594.75    \n",
      "module.adapter.frcn_linear.weight  dot:  63588212.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46743.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  432171.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  250.76239013671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  397570.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17139786.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  77084.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8801.763671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  90.66172790527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  146704.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4463504.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  77084.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1083.0401611328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  178.8084716796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3887.365966796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7160507.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9008906.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452697.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  676/6933] Loss: -883.0562 [iq: 8.1726,ans: 7.8403,interp: 8.9378,fusion: -908.0069]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4649171.0    \n",
      "module.ans_embedding.weight  dot:  879052.875    \n",
      "module.lstm.weight_ih_l0  dot:  98171456.0    \n",
      "module.lstm.weight_hh_l0  dot:  11815455.0    \n",
      "module.lstm.bias_ih_l0  dot:  5277312.0    \n",
      "module.lstm.bias_hh_l0  dot:  5277312.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14317052.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  133338.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  323089.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  323089.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  80137456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53124.8828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  751262.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  459.36676025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  604814.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17226780.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76581.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28267.544921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  236.4630126953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  468461.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.005503236228833e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5435856.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76581.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  683.4139404296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  140.78237915039062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1765.52734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5384647.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7035782.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452698.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  677/6933] Loss: -823.4711 [iq: 10.2585,ans: 9.2652,interp: 9.4203,fusion: -852.4151]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1649302.25    \n",
      "module.ans_embedding.weight  dot:  1011284.0    \n",
      "module.lstm.weight_ih_l0  dot:  38163344.0    \n",
      "module.lstm.weight_hh_l0  dot:  8406280.0    \n",
      "module.lstm.bias_ih_l0  dot:  2502689.5    \n",
      "module.lstm.bias_hh_l0  dot:  2502689.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31960920.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  42694.21875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3760650.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3760650.75    \n",
      "module.adapter.frcn_linear.weight  dot:  126967792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  88217.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  619988.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.14990234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  430354.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.71482053399086e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  27487032.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  113650.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38823.0    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  426.38177490234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  646088.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.640288236463675e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7987678.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  113650.7890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2616.37451171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  573.5167236328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9092.501953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7808270.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15480606.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452698.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  678/6933] Loss: -870.4659 [iq: 9.4467,ans: 8.9246,interp: 8.8882,fusion: -897.7254]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  163621.0    \n",
      "module.ans_embedding.weight  dot:  1399827.625    \n",
      "module.lstm.weight_ih_l0  dot:  4213943.0    \n",
      "module.lstm.weight_hh_l0  dot:  1664206.75    \n",
      "module.lstm.bias_ih_l0  dot:  263939.65625    \n",
      "module.lstm.bias_hh_l0  dot:  263939.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21365132.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13541.435546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1351620.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1351620.125    \n",
      "module.adapter.frcn_linear.weight  dot:  62359512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42958.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  453265.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  266.4697265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  329660.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15619584.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66974.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8615.373046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  126.84420013427734    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  150920.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4306394.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66974.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  726.5963745117188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  195.06381225585938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1434.698974609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6424830.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10865722.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452698.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  679/6933] Loss: -841.8351 [iq: 10.4609,ans: 9.0389,interp: 8.9381,fusion: -870.2729]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  987185.75    \n",
      "module.ans_embedding.weight  dot:  1076263.75    \n",
      "module.lstm.weight_ih_l0  dot:  14856826.0    \n",
      "module.lstm.weight_hh_l0  dot:  6502262.0    \n",
      "module.lstm.bias_ih_l0  dot:  931036.4375    \n",
      "module.lstm.bias_hh_l0  dot:  931036.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38034984.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35520.55078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3234267.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3234267.75    \n",
      "module.adapter.frcn_linear.weight  dot:  80172976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  60275.3828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  471917.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  245.10671997070312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  381652.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19790620.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  85768.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11607.271484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  216.74139404296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  205838.203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4485669.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  85768.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2198.4677734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  522.5994262695312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  12040.244140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9144982.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13459392.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452698.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  680/6933] Loss: -861.1927 [iq: 8.5330,ans: 7.8585,interp: 7.4350,fusion: -885.0192]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  875058.875    \n",
      "module.ans_embedding.weight  dot:  1370892.75    \n",
      "module.lstm.weight_ih_l0  dot:  12006249.0    \n",
      "module.lstm.weight_hh_l0  dot:  2324928.0    \n",
      "module.lstm.bias_ih_l0  dot:  655667.6875    \n",
      "module.lstm.bias_hh_l0  dot:  655667.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17687282.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22747.31640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  353332.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  353332.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  57151876.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36967.2734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  550058.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  364.1827087402344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  477664.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  14373981.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60763.6953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11740.951171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  149.82212829589844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  156573.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4318724.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60763.6953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  315.090576171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.65018463134766    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  980.0540771484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5874446.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6716069.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452699.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  681/6933] Loss: -865.7567 [iq: 8.9794,ans: 7.4962,interp: 7.7789,fusion: -890.0111]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  175905.984375    \n",
      "module.ans_embedding.weight  dot:  1118781.5    \n",
      "module.lstm.weight_ih_l0  dot:  6573508.5    \n",
      "module.lstm.weight_hh_l0  dot:  1101704.75    \n",
      "module.lstm.bias_ih_l0  dot:  454722.875    \n",
      "module.lstm.bias_hh_l0  dot:  454722.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23535516.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10043.712890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1572895.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1572895.0    \n",
      "module.adapter.frcn_linear.weight  dot:  66173672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48029.0546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  282469.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  163.758544921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  213254.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16589293.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72513.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9743.9267578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  228.80587768554688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  180931.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1173605091462377e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4182367.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72513.1640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  906.40185546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  165.81594848632812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2015.0440673828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6653780.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9587518.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452699.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  682/6933] Loss: -891.0256 [iq: 8.7214,ans: 7.8035,interp: 8.0925,fusion: -915.6430]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  432603.8125    \n",
      "module.ans_embedding.weight  dot:  1171520.375    \n",
      "module.lstm.weight_ih_l0  dot:  5969039.0    \n",
      "module.lstm.weight_hh_l0  dot:  1653746.25    \n",
      "module.lstm.bias_ih_l0  dot:  351489.1875    \n",
      "module.lstm.bias_hh_l0  dot:  351489.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29967380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  49354.984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1263411.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1263411.75    \n",
      "module.adapter.frcn_linear.weight  dot:  60402904.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37682.8359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  566389.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  281.3976745605469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  550106.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  16047112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65863.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10759.4013671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  92.89546203613281    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  146015.984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3846648.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65863.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  128.05288696289062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.750754356384277    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  437.510498046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8448298.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9088167.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452700.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  683/6933] Loss: -851.6017 [iq: 9.7956,ans: 8.0757,interp: 8.7942,fusion: -878.2672]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  519860.46875    \n",
      "module.ans_embedding.weight  dot:  1299934.25    \n",
      "module.lstm.weight_ih_l0  dot:  11368152.0    \n",
      "module.lstm.weight_hh_l0  dot:  3923000.75    \n",
      "module.lstm.bias_ih_l0  dot:  783779.75    \n",
      "module.lstm.bias_hh_l0  dot:  783779.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25267786.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30141.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1861536.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1861536.75    \n",
      "module.adapter.frcn_linear.weight  dot:  92796208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  64939.84765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  683151.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  356.20172119140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  476194.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20591848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86226.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26219.400390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  517.263916015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  510184.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.25611529458547e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5780521.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86226.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8767.4619140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1622.801513671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  12681.294921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7576058.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12375655.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452700.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  684/6933] Loss: -854.6578 [iq: 9.3074,ans: 8.9692,interp: 9.5813,fusion: -882.5157]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  18293290.0    \n",
      "module.ans_embedding.weight  dot:  1110248.625    \n",
      "module.lstm.weight_ih_l0  dot:  230958272.0    \n",
      "module.lstm.weight_hh_l0  dot:  26751844.0    \n",
      "module.lstm.bias_ih_l0  dot:  14467758.0    \n",
      "module.lstm.bias_hh_l0  dot:  14467758.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16954966.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31897.38671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  424859.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  424859.875    \n",
      "module.adapter.frcn_linear.weight  dot:  58393304.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36741.28515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  526833.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  312.45556640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  412819.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14837110.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60066.58203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34645.96484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  586.9139404296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  439583.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.197442310920451e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4658994.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60066.58203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5571.5    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1603.8917236328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13295.314453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.617106696969131e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5845354.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6314089.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452701.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  685/6933] Loss: -873.4520 [iq: 8.0524,ans: 7.5422,interp: 9.5686,fusion: -898.6152]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  333981.28125    \n",
      "module.ans_embedding.weight  dot:  2206876.0    \n",
      "module.lstm.weight_ih_l0  dot:  10927610.0    \n",
      "module.lstm.weight_hh_l0  dot:  4515124.5    \n",
      "module.lstm.bias_ih_l0  dot:  772851.875    \n",
      "module.lstm.bias_hh_l0  dot:  772851.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  58133392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11098.583984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  8654877.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  8654877.0    \n",
      "module.adapter.frcn_linear.weight  dot:  85998704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58754.15625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  371097.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  220.3911895751953    \n",
      "module.attflat_img.mlp.linear.weight  dot:  296754.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23435656.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  96353.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17304.359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  268.2452087402344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  355404.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.412207322777249e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5401793.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  96353.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  224.0401611328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.01065444946289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  859.2330322265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9742125.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  25145016.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452701.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  686/6933] Loss: -846.5872 [iq: 9.2108,ans: 8.2031,interp: 8.9882,fusion: -872.9893]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  899465.9375    \n",
      "module.ans_embedding.weight  dot:  848821.375    \n",
      "module.lstm.weight_ih_l0  dot:  9971755.0    \n",
      "module.lstm.weight_hh_l0  dot:  1502563.5    \n",
      "module.lstm.bias_ih_l0  dot:  545021.625    \n",
      "module.lstm.bias_hh_l0  dot:  545021.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15693848.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28814.642578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  482436.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  482436.375    \n",
      "module.adapter.frcn_linear.weight  dot:  49070900.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32194.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  377768.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  210.88414001464844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  359328.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14593668.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57409.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11498.42578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  128.27963256835938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  246622.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3743362.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57409.55859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  487.0602111816406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  89.90937042236328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1635.5770263671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6050927.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5749077.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452702.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  687/6933] Loss: -899.4985 [iq: 8.6070,ans: 7.1806,interp: 6.9024,fusion: -922.1885]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  378792.46875    \n",
      "module.ans_embedding.weight  dot:  1184673.75    \n",
      "module.lstm.weight_ih_l0  dot:  13852952.0    \n",
      "module.lstm.weight_hh_l0  dot:  2081493.875    \n",
      "module.lstm.bias_ih_l0  dot:  869053.125    \n",
      "module.lstm.bias_hh_l0  dot:  869053.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23824930.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8685.40234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1179378.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1179378.125    \n",
      "module.adapter.frcn_linear.weight  dot:  87690144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  63862.12109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  433095.03125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  261.6888427734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  325825.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.459273673593998e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23358688.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95868.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13042.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  206.5333251953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  198789.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.154951855321997e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5964751.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95868.4765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  51.83415222167969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.404809951782227    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  175.171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7541544.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10689588.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452702.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  688/6933] Loss: -857.7175 [iq: 10.8393,ans: 8.1047,interp: 8.1293,fusion: -884.7907]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  500428.1875    \n",
      "module.ans_embedding.weight  dot:  1493290.875    \n",
      "module.lstm.weight_ih_l0  dot:  4000875.0    \n",
      "module.lstm.weight_hh_l0  dot:  1538367.0    \n",
      "module.lstm.bias_ih_l0  dot:  217812.5625    \n",
      "module.lstm.bias_hh_l0  dot:  217812.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20227484.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71603.8203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  465987.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  465987.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  57810260.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36938.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  546241.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  317.741455078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  417526.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.533365765586495e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15567079.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63013.12109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17792.771484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  227.46832275390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  202401.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.46265255252365e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4546488.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63013.12109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  990.3121337890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  167.35702514648438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1373.196044921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6437164.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7657572.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452702.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  689/6933] Loss: -842.5166 [iq: 10.4151,ans: 7.3402,interp: 8.5578,fusion: -868.8298]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  955621.5    \n",
      "module.ans_embedding.weight  dot:  1207138.0    \n",
      "module.lstm.weight_ih_l0  dot:  14730518.0    \n",
      "module.lstm.weight_hh_l0  dot:  2143024.25    \n",
      "module.lstm.bias_ih_l0  dot:  832348.125    \n",
      "module.lstm.bias_hh_l0  dot:  832348.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20614336.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17278.515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  682473.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  682473.375    \n",
      "module.adapter.frcn_linear.weight  dot:  70130168.0    \n",
      "module.adapter.frcn_linear.bias  dot:  47586.3046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  577735.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  350.77374267578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  487521.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.049596544879023e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16247428.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65144.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22822.4921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  320.72381591796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  333722.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4892748.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65144.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  313.93280029296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  80.8503646850586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1010.0401611328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6086261.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7984841.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452703.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  690/6933] Loss: -855.6744 [iq: 11.1339,ans: 8.4175,interp: 8.9915,fusion: -884.2172]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1072565.125    \n",
      "module.ans_embedding.weight  dot:  1044812.0    \n",
      "module.lstm.weight_ih_l0  dot:  10178134.0    \n",
      "module.lstm.weight_hh_l0  dot:  2539187.0    \n",
      "module.lstm.bias_ih_l0  dot:  496431.65625    \n",
      "module.lstm.bias_hh_l0  dot:  496431.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27836642.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  83657.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2124134.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2124134.5    \n",
      "module.adapter.frcn_linear.weight  dot:  73723752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45944.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  925445.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  436.00506591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  834851.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19532452.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73001.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28879.73046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  352.4504089355469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  472541.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.32271132619644e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4501592.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73001.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  47161.34375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9757.66796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  107080.09375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6405524.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9602807.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452703.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  691/6933] Loss: -836.4028 [iq: 10.3625,ans: 8.9344,interp: 8.9445,fusion: -864.6442]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  481271.3125    \n",
      "module.ans_embedding.weight  dot:  1444769.875    \n",
      "module.lstm.weight_ih_l0  dot:  7491053.5    \n",
      "module.lstm.weight_hh_l0  dot:  2156988.25    \n",
      "module.lstm.bias_ih_l0  dot:  422794.40625    \n",
      "module.lstm.bias_hh_l0  dot:  422794.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19561358.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9910.6865234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  742438.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  742438.25    \n",
      "module.adapter.frcn_linear.weight  dot:  110507584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80813.9453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  645803.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.18218994140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  556813.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24594544.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  98857.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17739.83203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  306.428955078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  298894.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8010268831858411e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5532211.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  98857.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  40.633544921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.730563163757324    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  50.960792541503906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6379728.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8438232.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452703.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  692/6933] Loss: -838.3643 [iq: 8.9898,ans: 8.5671,interp: 11.3511,fusion: -867.2724]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2517676.75    \n",
      "module.ans_embedding.weight  dot:  1565372.375    \n",
      "module.lstm.weight_ih_l0  dot:  38752288.0    \n",
      "module.lstm.weight_hh_l0  dot:  6637241.0    \n",
      "module.lstm.bias_ih_l0  dot:  2155130.5    \n",
      "module.lstm.bias_hh_l0  dot:  2155130.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20055232.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38382.99609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  395838.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  395838.25    \n",
      "module.adapter.frcn_linear.weight  dot:  90761072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  63593.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  439628.90625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  225.5314178466797    \n",
      "module.attflat_img.mlp.linear.weight  dot:  293181.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.2041075327433646e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  23039104.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  92400.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  62793.7265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  594.861572265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  857784.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8417267710901797e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6445519.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  92400.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2468.210205078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  550.6220703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9210.884765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.220446049250313e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5947353.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7118155.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452704.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  693/6933] Loss: -874.4679 [iq: 10.1318,ans: 9.0797,interp: 10.0894,fusion: -903.7687]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  290954.6875    \n",
      "module.ans_embedding.weight  dot:  1159564.0    \n",
      "module.lstm.weight_ih_l0  dot:  3325628.0    \n",
      "module.lstm.weight_hh_l0  dot:  1797311.75    \n",
      "module.lstm.bias_ih_l0  dot:  173133.109375    \n",
      "module.lstm.bias_hh_l0  dot:  173133.109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22669488.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  112558.484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  696518.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  696518.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  105489920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80108.046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  378792.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  224.91567993164062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  279044.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  22301164.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  93794.859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13991.5849609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  166.9285888671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  248842.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.901959644281305e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6122098.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  93794.859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13551.6357421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1280.509765625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  19286.046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5680900.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6922251.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452704.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  694/6933] Loss: -867.2985 [iq: 8.6845,ans: 7.9050,interp: 7.4139,fusion: -891.3019]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5850095.0    \n",
      "module.ans_embedding.weight  dot:  798285.625    \n",
      "module.lstm.weight_ih_l0  dot:  104660368.0    \n",
      "module.lstm.weight_hh_l0  dot:  9260833.0    \n",
      "module.lstm.bias_ih_l0  dot:  5996365.5    \n",
      "module.lstm.bias_hh_l0  dot:  5996365.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13025294.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20118.619140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  322713.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  322713.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  48240824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30378.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  374604.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  180.8330078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  363650.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  13403309.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  51360.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17166.958984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  439.743408203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  124467.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.15278850091272e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4709951.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  51360.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  323.41021728515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.10015106201172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  654.8908081054688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.194245199571014e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5875820.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5067683.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452705.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  695/6933] Loss: -871.0782 [iq: 9.2417,ans: 7.9165,interp: 8.3111,fusion: -896.5476]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  478654.90625    \n",
      "module.ans_embedding.weight  dot:  1359109.875    \n",
      "module.lstm.weight_ih_l0  dot:  5242209.5    \n",
      "module.lstm.weight_hh_l0  dot:  3390371.25    \n",
      "module.lstm.bias_ih_l0  dot:  166569.125    \n",
      "module.lstm.bias_hh_l0  dot:  166569.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14505252.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  68238.0546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  718515.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  718515.875    \n",
      "module.adapter.frcn_linear.weight  dot:  59236072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40146.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  325634.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  195.0213623046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  251903.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16566218.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66499.421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9708.60546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  163.10487365722656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  174445.984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5091378.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66499.421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2019.376953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  594.21435546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8628.5546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6069155.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7657853.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452705.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  696/6933] Loss: -894.1150 [iq: 10.2719,ans: 8.2503,interp: 8.1389,fusion: -920.7761]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3608397.0    \n",
      "module.ans_embedding.weight  dot:  746950.5    \n",
      "module.lstm.weight_ih_l0  dot:  69136624.0    \n",
      "module.lstm.weight_hh_l0  dot:  6919904.0    \n",
      "module.lstm.bias_ih_l0  dot:  4219318.0    \n",
      "module.lstm.bias_hh_l0  dot:  4219318.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16179200.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21181.283203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  661555.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  661555.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  83990776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53322.4296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  910837.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  496.2810974121094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1100945.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.496097633615136e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21051866.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  79834.3515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34289.859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  387.0173034667969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  401002.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5711804.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  79834.3515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  640.5621337890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  99.11902618408203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2715.024169921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.566746732351021e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6341899.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6950256.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452706.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  697/6933] Loss: -851.8936 [iq: 11.3047,ans: 9.5575,interp: 10.3761,fusion: -883.1318]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7ff5d44b7da0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/slam/dl/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/slam/dl/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 909, in _shutdown_workers\n",
      "    q.close()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 134, in close\n",
      "    self._reader.close()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 177, in close\n",
      "    Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "self._close()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  334255.375    \n",
      "module.ans_embedding.weight  dot:  694835.125    \n",
      "module.lstm.weight_ih_l0  dot:  7246703.5    \n",
      "module.lstm.weight_hh_l0  dot:  3916434.5    \n",
      "module.lstm.bias_ih_l0  dot:  478191.40625    \n",
      "module.lstm.bias_hh_l0  dot:  478191.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24813380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  53702.375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1569560.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1569560.5    \n",
      "module.adapter.frcn_linear.weight  dot:  102881712.0    \n",
      "module.adapter.frcn_linear.bias  dot:  72228.234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  470463.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  268.7312927246094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  378182.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23416112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94498.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17882.78515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  168.43951416015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  303402.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7736436.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94498.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  141753.046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18509.9296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  137371.96875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8101167.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11316672.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452706.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  698/6933] Loss: -914.7293 [iq: 7.7629,ans: 8.0189,interp: 8.0840,fusion: -938.5952]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  498298.21875    \n",
      "module.ans_embedding.weight  dot:  1205179.75    \n",
      "module.lstm.weight_ih_l0  dot:  8203836.0    \n",
      "module.lstm.weight_hh_l0  dot:  5016104.0    \n",
      "module.lstm.bias_ih_l0  dot:  505122.125    \n",
      "module.lstm.bias_hh_l0  dot:  505122.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14529534.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22344.96484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  414614.21875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  414614.21875    \n",
      "module.adapter.frcn_linear.weight  dot:  82849408.0    \n",
      "module.adapter.frcn_linear.bias  dot:  55528.1796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  733800.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  412.72412109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  618734.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20999792.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  83165.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20457.513671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  280.42193603515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  386856.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5691820.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  83165.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  17.55426788330078    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.6546452045440674    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  42.8663330078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5863456.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5901479.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452707.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  699/6933] Loss: -881.2838 [iq: 8.3979,ans: 8.2224,interp: 8.6179,fusion: -906.5220]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  636372.5625    \n",
      "module.ans_embedding.weight  dot:  930362.0    \n",
      "module.lstm.weight_ih_l0  dot:  16205439.0    \n",
      "module.lstm.weight_hh_l0  dot:  5468034.5    \n",
      "module.lstm.bias_ih_l0  dot:  993452.875    \n",
      "module.lstm.bias_hh_l0  dot:  993452.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15342764.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12391.189453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  631349.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  631349.0    \n",
      "module.adapter.frcn_linear.weight  dot:  73364160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45996.80859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  767766.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  325.77392578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  892957.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  19130732.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72359.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18273.767578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  296.3375244140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  310290.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.9133177526528016e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5856769.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72359.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  265.3807373046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  61.302696228027344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1108.7236328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.972111694063642e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6683567.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7396875.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452707.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  700/6933] Loss: -860.0902 [iq: 8.6476,ans: 8.5025,interp: 8.6772,fusion: -885.9174]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  214810.1875    \n",
      "module.ans_embedding.weight  dot:  864854.4375    \n",
      "module.lstm.weight_ih_l0  dot:  7667607.5    \n",
      "module.lstm.weight_hh_l0  dot:  8641042.0    \n",
      "module.lstm.bias_ih_l0  dot:  528169.875    \n",
      "module.lstm.bias_hh_l0  dot:  528169.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23322478.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  46556.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  708299.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  708299.125    \n",
      "module.adapter.frcn_linear.weight  dot:  82973936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  55214.80078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  712662.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  372.04742431640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  632959.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18807082.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75010.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8749.0869140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  125.24505615234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  175533.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5633794.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75010.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  943.51611328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  302.47271728515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1703.3990478515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6802386.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8236653.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452708.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  701/6933] Loss: -870.5039 [iq: 9.0190,ans: 8.3030,interp: 8.8199,fusion: -896.6458]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  337616.125    \n",
      "module.ans_embedding.weight  dot:  1199423.75    \n",
      "module.lstm.weight_ih_l0  dot:  6823693.0    \n",
      "module.lstm.weight_hh_l0  dot:  4133291.25    \n",
      "module.lstm.bias_ih_l0  dot:  454573.9375    \n",
      "module.lstm.bias_hh_l0  dot:  454573.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19654054.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20675.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  700560.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  700560.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  48269380.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28047.9921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  390118.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  219.08993530273438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  349834.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14134048.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53133.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14783.6318359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  209.35601806640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  292194.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5663772501284257e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4099399.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53133.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  107.88127899169922    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.737220764160156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  311.45843505859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6295554.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7526249.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452709.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  702/6933] Loss: -880.3372 [iq: 8.8037,ans: 8.2899,interp: 8.0269,fusion: -905.4577]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  527834.8125    \n",
      "module.ans_embedding.weight  dot:  852838.4375    \n",
      "module.lstm.weight_ih_l0  dot:  8808361.0    \n",
      "module.lstm.weight_hh_l0  dot:  4656353.0    \n",
      "module.lstm.bias_ih_l0  dot:  522807.1875    \n",
      "module.lstm.bias_hh_l0  dot:  522807.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27528670.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20316.28515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2343847.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2343847.25    \n",
      "module.adapter.frcn_linear.weight  dot:  77553872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52682.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  923615.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  518.2650146484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  888423.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5067947717616335e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21823832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94292.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17677.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  252.56712341308594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  301108.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6897683.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94292.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1064.803466796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  199.15415954589844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6042.9482421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8639832.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14270318.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452709.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  703/6933] Loss: -839.5571 [iq: 8.8835,ans: 8.3146,interp: 9.4445,fusion: -866.1997]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  321463.0    \n",
      "module.ans_embedding.weight  dot:  1016709.25    \n",
      "module.lstm.weight_ih_l0  dot:  11589143.0    \n",
      "module.lstm.weight_hh_l0  dot:  5618362.0    \n",
      "module.lstm.bias_ih_l0  dot:  728125.0    \n",
      "module.lstm.bias_hh_l0  dot:  728125.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21720916.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9289.3154296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1582738.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1582738.5    \n",
      "module.adapter.frcn_linear.weight  dot:  56954756.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36269.7265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  638908.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  324.74200439453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  587702.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.7762894205807243e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16172990.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63046.72265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13769.943359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  112.47638702392578    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  226153.921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5043871.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63046.72265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  247.50360107421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.74559783935547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  789.2406005859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5667880.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8822234.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452710.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  704/6933] Loss: -867.9968 [iq: 11.4499,ans: 9.4314,interp: 9.2155,fusion: -898.0936]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  439638.15625    \n",
      "module.ans_embedding.weight  dot:  988190.625    \n",
      "module.lstm.weight_ih_l0  dot:  4935232.0    \n",
      "module.lstm.weight_hh_l0  dot:  1995377.75    \n",
      "module.lstm.bias_ih_l0  dot:  278685.375    \n",
      "module.lstm.bias_hh_l0  dot:  278685.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25224582.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  49684.9765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1206483.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1206483.5    \n",
      "module.adapter.frcn_linear.weight  dot:  51889560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30777.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  650484.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  367.08551025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  619512.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13700484.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54885.3046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21323.0625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  291.09197998046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  327419.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3853154.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54885.3046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3170.69970703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  435.1181335449219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4948.89208984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6016994.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9596180.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452710.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  705/6933] Loss: -863.9040 [iq: 10.1343,ans: 8.4823,interp: 8.9127,fusion: -891.4333]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  366278.53125    \n",
      "module.ans_embedding.weight  dot:  1672576.375    \n",
      "module.lstm.weight_ih_l0  dot:  2285976.0    \n",
      "module.lstm.weight_hh_l0  dot:  1777115.25    \n",
      "module.lstm.bias_ih_l0  dot:  87154.796875    \n",
      "module.lstm.bias_hh_l0  dot:  87154.796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  39514492.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31395.4296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2376096.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2376096.0    \n",
      "module.adapter.frcn_linear.weight  dot:  59517356.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37182.02734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  475199.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  312.791259765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  316039.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15296841.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59911.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7207.48828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  113.74422454833984    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  96665.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4815391.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59911.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  103.94922637939453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.196950912475586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  327.178466796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.121650077846482e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6873697.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9133524.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452711.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  706/6933] Loss: -829.6216 [iq: 10.7298,ans: 8.9425,interp: 9.5518,fusion: -858.8457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1684360.75    \n",
      "module.ans_embedding.weight  dot:  910776.0    \n",
      "module.lstm.weight_ih_l0  dot:  4121100.75    \n",
      "module.lstm.weight_hh_l0  dot:  1151777.5    \n",
      "module.lstm.bias_ih_l0  dot:  183622.375    \n",
      "module.lstm.bias_hh_l0  dot:  183622.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25885040.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  27799.27734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1702040.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1702040.25    \n",
      "module.adapter.frcn_linear.weight  dot:  88140768.0    \n",
      "module.adapter.frcn_linear.bias  dot:  56622.4140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  965153.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  406.025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  975839.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.531525625381619e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19691196.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  77500.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22385.89453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  441.6717529296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  355127.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4467462.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  77500.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  774.055908203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  157.58419799804688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2184.44287109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6026789.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7136111.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452711.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  707/6933] Loss: -869.7222 [iq: 7.0018,ans: 7.1642,interp: 7.6999,fusion: -891.5881]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  282856.25    \n",
      "module.ans_embedding.weight  dot:  884980.125    \n",
      "module.lstm.weight_ih_l0  dot:  1376319.75    \n",
      "module.lstm.weight_hh_l0  dot:  1878781.25    \n",
      "module.lstm.bias_ih_l0  dot:  55043.41796875    \n",
      "module.lstm.bias_hh_l0  dot:  55043.41796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19239292.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  42670.70703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  800493.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  800493.375    \n",
      "module.adapter.frcn_linear.weight  dot:  63916880.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42280.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  379661.65625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  212.304931640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  284587.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15310472.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61964.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12995.2138671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  197.01852416992188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  258530.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4790937.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61964.2109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  359.4744567871094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  76.0428237915039    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1516.8206787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6185940.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8556010.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452712.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  708/6933] Loss: -841.3532 [iq: 9.7953,ans: 9.2001,interp: 10.8073,fusion: -871.1559]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  457501.71875    \n",
      "module.ans_embedding.weight  dot:  603107.375    \n",
      "module.lstm.weight_ih_l0  dot:  8254188.5    \n",
      "module.lstm.weight_hh_l0  dot:  5349190.0    \n",
      "module.lstm.bias_ih_l0  dot:  509056.59375    \n",
      "module.lstm.bias_hh_l0  dot:  509056.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21851244.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10358.0849609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1309944.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1309944.875    \n",
      "module.adapter.frcn_linear.weight  dot:  67964792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44826.36328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  520699.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  250.44296264648438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  361688.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18150100.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70579.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14928.12890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  251.21139526367188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  292606.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.530065542800003e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6721227.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70579.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29.604816436767578    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.4136223793029785    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  89.74429321289062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7393984.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9205244.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452712.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  709/6933] Loss: -917.1335 [iq: 9.8493,ans: 8.8321,interp: 8.5816,fusion: -944.3965]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  365704.375    \n",
      "module.ans_embedding.weight  dot:  542437.0    \n",
      "module.lstm.weight_ih_l0  dot:  12781021.0    \n",
      "module.lstm.weight_hh_l0  dot:  6085881.0    \n",
      "module.lstm.bias_ih_l0  dot:  814132.25    \n",
      "module.lstm.bias_hh_l0  dot:  814132.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11752912.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18885.0    \n",
      "module.ans_lstm.bias_ih_l0  dot:  359596.65625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  359596.65625    \n",
      "module.adapter.frcn_linear.weight  dot:  62889760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41202.26171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  549491.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  314.69232177734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  467560.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19377548.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  78071.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20415.44921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  454.9351806640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  407444.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5454528.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  78071.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  720.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  144.52142333984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4525.6875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5294936.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5335096.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452713.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  710/6933] Loss: -893.7761 [iq: 10.1546,ans: 8.8706,interp: 8.7084,fusion: -921.5096]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  792575.625    \n",
      "module.ans_embedding.weight  dot:  3012396.5    \n",
      "module.lstm.weight_ih_l0  dot:  8069774.5    \n",
      "module.lstm.weight_hh_l0  dot:  7982718.0    \n",
      "module.lstm.bias_ih_l0  dot:  538982.75    \n",
      "module.lstm.bias_hh_l0  dot:  538982.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  81847816.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22740.49609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2922161.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2922161.5    \n",
      "module.adapter.frcn_linear.weight  dot:  76340688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51700.1796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  577891.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  280.8282470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  480715.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.112745616817847e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18123760.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71486.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13140.83203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  250.08526611328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  214315.046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.5811922316497657e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5701467.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71486.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  200.24453735351562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  41.12842559814453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  514.312744140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9398455.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10671041.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452713.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  711/6933] Loss: -838.1105 [iq: 9.4183,ans: 8.0708,interp: 8.1890,fusion: -863.7887]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2060079.625    \n",
      "module.ans_embedding.weight  dot:  305928.96875    \n",
      "module.lstm.weight_ih_l0  dot:  5423412.5    \n",
      "module.lstm.weight_hh_l0  dot:  2272492.5    \n",
      "module.lstm.bias_ih_l0  dot:  171454.4375    \n",
      "module.lstm.bias_hh_l0  dot:  171454.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14188051.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13834.39453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  963959.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  963959.5    \n",
      "module.adapter.frcn_linear.weight  dot:  96238440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  70396.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  593871.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  391.09747314453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  563945.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22988810.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  95067.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20185.73046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  433.491455078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  360332.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7099163.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  95067.0390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  654.8875122070312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  157.06382751464844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1042.8740234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5530500.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7706723.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452714.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  712/6933] Loss: -885.0441 [iq: 7.4868,ans: 7.5696,interp: 8.0958,fusion: -908.1962]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1635738.375    \n",
      "module.ans_embedding.weight  dot:  1641823.625    \n",
      "module.lstm.weight_ih_l0  dot:  30410580.0    \n",
      "module.lstm.weight_hh_l0  dot:  4103892.5    \n",
      "module.lstm.bias_ih_l0  dot:  1686069.875    \n",
      "module.lstm.bias_hh_l0  dot:  1686069.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21735888.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32298.99609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  483229.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  483229.0    \n",
      "module.adapter.frcn_linear.weight  dot:  76634752.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51757.28515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  372094.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  235.42591857910156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  269923.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20857378.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  82081.8984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7532.994140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  93.812255859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  100266.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4961350.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  82081.8984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2151.3173828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  481.90667724609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10043.212890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6707268.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8478430.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452714.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  713/6933] Loss: -837.5160 [iq: 10.2418,ans: 8.3641,interp: 8.2915,fusion: -864.4134]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  599467.5    \n",
      "module.ans_embedding.weight  dot:  1178454.5    \n",
      "module.lstm.weight_ih_l0  dot:  7788325.5    \n",
      "module.lstm.weight_hh_l0  dot:  3394273.5    \n",
      "module.lstm.bias_ih_l0  dot:  464429.09375    \n",
      "module.lstm.bias_hh_l0  dot:  464429.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24306470.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40125.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1603807.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1603807.0    \n",
      "module.adapter.frcn_linear.weight  dot:  64802980.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40231.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  623190.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.9790344238281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  442129.40625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19610320.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75850.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12481.7353515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  352.3687744140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  227815.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.865476744773332e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5621427.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75850.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1032.700927734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  180.36968994140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2413.862548828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7090256.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9268927.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452715.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  714/6933] Loss: -870.8115 [iq: 9.9583,ans: 8.5246,interp: 8.5548,fusion: -897.8492]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  635818.25    \n",
      "module.ans_embedding.weight  dot:  1126886.5    \n",
      "module.lstm.weight_ih_l0  dot:  23465600.0    \n",
      "module.lstm.weight_hh_l0  dot:  22579966.0    \n",
      "module.lstm.bias_ih_l0  dot:  1837991.75    \n",
      "module.lstm.bias_hh_l0  dot:  1837991.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20865072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  136410.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1515828.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1515828.75    \n",
      "module.adapter.frcn_linear.weight  dot:  68240400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46195.69921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  797842.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  411.6072692871094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  835493.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20563052.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  83571.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17423.12109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  256.65966796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  249142.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6725697.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  83571.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3173.8232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  492.49078369140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  21879.078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5477175.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7149485.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452716.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  715/6933] Loss: -846.9897 [iq: 8.4907,ans: 8.0571,interp: 7.7908,fusion: -871.3282]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  988181.0625    \n",
      "module.ans_embedding.weight  dot:  917290.5625    \n",
      "module.lstm.weight_ih_l0  dot:  13078736.0    \n",
      "module.lstm.weight_hh_l0  dot:  7412571.0    \n",
      "module.lstm.bias_ih_l0  dot:  907726.875    \n",
      "module.lstm.bias_hh_l0  dot:  907726.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18860584.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31379.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1275105.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1275105.125    \n",
      "module.adapter.frcn_linear.weight  dot:  60446824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40520.8046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  309537.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  153.88853454589844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  225012.578125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17657542.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68221.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8957.9970703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  214.59461975097656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  161428.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.915943125321064e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5248982.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68221.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  410.89068603515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  81.37998962402344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  819.7208251953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5758050.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7946821.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452716.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  716/6933] Loss: -864.6670 [iq: 7.2391,ans: 7.3730,interp: 6.9491,fusion: -886.2283]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  599387.0625    \n",
      "module.ans_embedding.weight  dot:  779594.375    \n",
      "module.lstm.weight_ih_l0  dot:  6224292.0    \n",
      "module.lstm.weight_hh_l0  dot:  3091823.5    \n",
      "module.lstm.bias_ih_l0  dot:  367755.75    \n",
      "module.lstm.bias_hh_l0  dot:  367755.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37590212.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21062.37890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3300071.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3300071.0    \n",
      "module.adapter.frcn_linear.weight  dot:  62355052.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40814.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  452527.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  229.70144653320312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  403686.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17637420.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66898.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15605.5302734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  358.36572265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  286112.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1173605091462377e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4385650.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66898.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3841.280029296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  911.48388671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10375.8564453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7543812.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11614496.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452717.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  717/6933] Loss: -859.6080 [iq: 7.6526,ans: 7.6997,interp: 9.2739,fusion: -884.2341]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  194424.234375    \n",
      "module.ans_embedding.weight  dot:  2120403.75    \n",
      "module.lstm.weight_ih_l0  dot:  2231397.75    \n",
      "module.lstm.weight_hh_l0  dot:  1131102.875    \n",
      "module.lstm.bias_ih_l0  dot:  112089.3828125    \n",
      "module.lstm.bias_hh_l0  dot:  112089.3828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  38424012.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6076.9853515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2080867.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2080867.25    \n",
      "module.adapter.frcn_linear.weight  dot:  58594560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32869.765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  752883.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  398.61505126953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  593137.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15218718.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56831.3828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8136.4384765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  224.70008850097656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  124491.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.913989298278466e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4495648.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56831.3828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  332.2847595214844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  62.285682678222656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1054.21875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7120513.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10391808.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452717.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  718/6933] Loss: -841.6510 [iq: 10.2847,ans: 8.8236,interp: 9.7800,fusion: -870.5393]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  604957.0    \n",
      "module.ans_embedding.weight  dot:  2125901.5    \n",
      "module.lstm.weight_ih_l0  dot:  3607255.0    \n",
      "module.lstm.weight_hh_l0  dot:  1373122.25    \n",
      "module.lstm.bias_ih_l0  dot:  138846.71875    \n",
      "module.lstm.bias_hh_l0  dot:  138846.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  103688608.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  299796.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  10258474.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  10258474.0    \n",
      "module.adapter.frcn_linear.weight  dot:  65932808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38049.8046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  779921.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  398.55145263671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  655763.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.648829291108996e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18496128.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67791.5390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20108.38671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  420.3865661621094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  448391.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2960867934452835e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4726411.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67791.5390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29626.40234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7159.576171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  169586.84375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  11661040.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  19411738.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452717.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  719/6933] Loss: -889.6299 [iq: 7.8955,ans: 7.1629,interp: 7.0765,fusion: -911.7649]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  323608.5625    \n",
      "module.ans_embedding.weight  dot:  829027.875    \n",
      "module.lstm.weight_ih_l0  dot:  2354357.0    \n",
      "module.lstm.weight_hh_l0  dot:  1072938.375    \n",
      "module.lstm.bias_ih_l0  dot:  88626.40625    \n",
      "module.lstm.bias_hh_l0  dot:  88626.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11802862.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12326.58203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  997386.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  997386.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  55641792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38906.265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  551815.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  338.28125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  466998.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  14966040.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60421.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10635.5439453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  194.7327880859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  197676.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4526063.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60421.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  149.79513549804688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.20915412902832    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  852.2495727539062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5589811.0    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.attflat_ans.linear_merge.bias  dot:  7024037.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452718.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  720/6933] Loss: -896.5134 [iq: 8.6588,ans: 6.8414,interp: 6.6136,fusion: -918.6271]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  609018.9375    \n",
      "module.ans_embedding.weight  dot:  519447.875    \n",
      "module.lstm.weight_ih_l0  dot:  2842037.5    \n",
      "module.lstm.weight_hh_l0  dot:  1538429.625    \n",
      "module.lstm.bias_ih_l0  dot:  123340.203125    \n",
      "module.lstm.bias_hh_l0  dot:  123340.203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9650690.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38345.984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  333796.03125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  333796.03125    \n",
      "module.adapter.frcn_linear.weight  dot:  51857840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33705.36328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  387694.59375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  201.14898681640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  360701.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17227292.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66631.6796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15028.3896484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  238.3734130859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  265609.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0520474208751693e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4764637.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66631.6796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1830.13427734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  301.0586242675781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4262.390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.298783551348606e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6411766.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5961118.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452719.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  721/6933] Loss: -870.0282 [iq: 11.1167,ans: 8.0695,interp: 8.6351,fusion: -897.8495]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5168323.5    \n",
      "module.ans_embedding.weight  dot:  1192817.5    \n",
      "module.lstm.weight_ih_l0  dot:  26017416.0    \n",
      "module.lstm.weight_hh_l0  dot:  4098001.0    \n",
      "module.lstm.bias_ih_l0  dot:  340276.625    \n",
      "module.lstm.bias_hh_l0  dot:  340276.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24720850.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39567.0390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1015299.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1015299.625    \n",
      "module.adapter.frcn_linear.weight  dot:  60382552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36864.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  899297.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  416.796142578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  842529.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.883965397719294e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15502510.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57192.80078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14011.416015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  192.140625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  163166.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4278740.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57192.80078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1431.011962890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  240.41204833984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8045.1171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.8689051962137455e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5963920.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7289995.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452719.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  722/6933] Loss: -845.1769 [iq: 10.2852,ans: 8.3201,interp: 8.3378,fusion: -872.1201]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2207477.25    \n",
      "module.ans_embedding.weight  dot:  1445360.125    \n",
      "module.lstm.weight_ih_l0  dot:  45920780.0    \n",
      "module.lstm.weight_hh_l0  dot:  13856893.0    \n",
      "module.lstm.bias_ih_l0  dot:  3221821.5    \n",
      "module.lstm.bias_hh_l0  dot:  3221821.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36884960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22305.66015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5383370.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5383370.5    \n",
      "module.adapter.frcn_linear.weight  dot:  67341936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46389.91015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  401733.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  211.6132049560547    \n",
      "module.attflat_img.mlp.linear.weight  dot:  365242.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.5710992203094065e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20233428.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80685.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  38061.625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  250.48397827148438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  431768.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.87805368215777e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5597249.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80685.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9224.7099609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1996.16162109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  18487.3984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7112128.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15721110.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452719.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  723/6933] Loss: -861.1558 [iq: 8.0082,ans: 8.2153,interp: 8.3312,fusion: -885.7105]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  49552612.0    \n",
      "module.ans_embedding.weight  dot:  1001448.25    \n",
      "module.lstm.weight_ih_l0  dot:  632329600.0    \n",
      "module.lstm.weight_hh_l0  dot:  60042788.0    \n",
      "module.lstm.bias_ih_l0  dot:  38244008.0    \n",
      "module.lstm.bias_hh_l0  dot:  38244008.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23779096.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13186.501953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1382743.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1382743.0    \n",
      "module.adapter.frcn_linear.weight  dot:  73762920.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48340.89453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  666389.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  331.5462951660156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  750013.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18743384.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67617.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35763.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  336.33251953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  273825.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9877575141435955e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4504826.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67617.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.20648193359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  81.97428894042969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  833.8046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.035971308738226e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6535066.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8911855.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452719.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  724/6933] Loss: -870.2999 [iq: 7.3285,ans: 7.1983,interp: 8.3728,fusion: -893.1995]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  32363370.0    \n",
      "module.ans_embedding.weight  dot:  888466.8125    \n",
      "module.lstm.weight_ih_l0  dot:  387512704.0    \n",
      "module.lstm.weight_hh_l0  dot:  34928156.0    \n",
      "module.lstm.bias_ih_l0  dot:  24362152.0    \n",
      "module.lstm.bias_hh_l0  dot:  24362152.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17560374.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  70386.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  563964.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  563964.25    \n",
      "module.adapter.frcn_linear.weight  dot:  54874568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34131.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  460179.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  228.14297485351562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  317731.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15774856.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58054.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  40953.3203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  604.398681640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  483340.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4263825.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58054.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11725.896484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2544.0517578125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35915.0703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3656631381309126e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5985187.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5920780.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452720.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  725/6933] Loss: -866.7043 [iq: 7.9351,ans: 7.4515,interp: 9.2917,fusion: -891.3826]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  428163.65625    \n",
      "module.ans_embedding.weight  dot:  1006819.625    \n",
      "module.lstm.weight_ih_l0  dot:  4512913.0    \n",
      "module.lstm.weight_hh_l0  dot:  910164.875    \n",
      "module.lstm.bias_ih_l0  dot:  264159.59375    \n",
      "module.lstm.bias_hh_l0  dot:  264159.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15010480.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12285.962890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  284044.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  284044.875    \n",
      "module.adapter.frcn_linear.weight  dot:  55816704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33649.7734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  571308.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.2694091796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  510585.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15322038.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56461.5859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7197.130859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  153.97813415527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  132839.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4843367.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56461.5859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1001.1774291992188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  223.20172119140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1476.9520263671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5599028.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5936544.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452721.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  726/6933] Loss: -873.2074 [iq: 8.8012,ans: 7.6573,interp: 7.9883,fusion: -897.6542]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3501213.75    \n",
      "module.ans_embedding.weight  dot:  1044305.75    \n",
      "module.lstm.weight_ih_l0  dot:  61807180.0    \n",
      "module.lstm.weight_hh_l0  dot:  37558768.0    \n",
      "module.lstm.bias_ih_l0  dot:  4288152.0    \n",
      "module.lstm.bias_hh_l0  dot:  4288152.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13093698.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20252.9609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  227701.15625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  227701.15625    \n",
      "module.adapter.frcn_linear.weight  dot:  72515296.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46060.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  453578.96875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.9444885253906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  372072.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20320806.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76048.6171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18220.96484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  413.14727783203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  395062.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.893753041164018e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6475774.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76048.6171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  512.355712890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  129.1119842529297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  692.8533935546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4900200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4836429.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452721.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  727/6933] Loss: -912.4173 [iq: 8.6438,ans: 7.6899,interp: 8.1314,fusion: -936.8824]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  281277.125    \n",
      "module.ans_embedding.weight  dot:  1113274.0    \n",
      "module.lstm.weight_ih_l0  dot:  4093032.25    \n",
      "module.lstm.weight_hh_l0  dot:  2702791.0    \n",
      "module.lstm.bias_ih_l0  dot:  67355.7734375    \n",
      "module.lstm.bias_hh_l0  dot:  67355.7734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17142650.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29496.23828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  638639.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  638639.625    \n",
      "module.adapter.frcn_linear.weight  dot:  61856384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38729.71875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  504010.03125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  264.206787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  511266.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15811242.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58719.0859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13985.935546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  254.0397186279297    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  279594.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.035971308738226e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4865229.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58719.0859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2792.3544921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  693.0999755859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2763.83837890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5198308.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5867282.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452722.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  728/6933] Loss: -885.3501 [iq: 9.1122,ans: 7.3105,interp: 8.2161,fusion: -909.9889]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  441747.875    \n",
      "module.ans_embedding.weight  dot:  773872.9375    \n",
      "module.lstm.weight_ih_l0  dot:  7053608.0    \n",
      "module.lstm.weight_hh_l0  dot:  1138470.5    \n",
      "module.lstm.bias_ih_l0  dot:  493217.5    \n",
      "module.lstm.bias_hh_l0  dot:  493217.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19443548.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  80674.859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1873270.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1873270.25    \n",
      "module.adapter.frcn_linear.weight  dot:  53789640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33906.48828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  364293.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  210.58493041992188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  301513.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13473054.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50178.3203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11038.595703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  258.58966064453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  254176.671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3397736.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50178.3203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3822.2998046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  692.9921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13123.044921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5376679.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7542130.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452722.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  729/6933] Loss: -906.2445 [iq: 9.5162,ans: 7.4620,interp: 7.3841,fusion: -930.6068]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  222150.4375    \n",
      "module.ans_embedding.weight  dot:  1036812.25    \n",
      "module.lstm.weight_ih_l0  dot:  7617531.0    \n",
      "module.lstm.weight_hh_l0  dot:  3083547.5    \n",
      "module.lstm.bias_ih_l0  dot:  471149.09375    \n",
      "module.lstm.bias_hh_l0  dot:  471149.09375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18892990.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23482.77734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1153286.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1153286.0    \n",
      "module.adapter.frcn_linear.weight  dot:  64553612.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41294.98046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  868395.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  394.9307556152344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  906883.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6427748050773516e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16338566.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61816.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13889.8603515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  197.6598663330078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  301778.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.801471055136062e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6116581.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61816.6328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  632.7651977539062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  177.7007293701172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1243.231689453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5797728.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8252054.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452723.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  730/6933] Loss: -884.5275 [iq: 9.6351,ans: 7.9023,interp: 7.9753,fusion: -910.0403]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2670713.5    \n",
      "module.ans_embedding.weight  dot:  1244197.5    \n",
      "module.lstm.weight_ih_l0  dot:  42760784.0    \n",
      "module.lstm.weight_hh_l0  dot:  5438572.0    \n",
      "module.lstm.bias_ih_l0  dot:  2871126.0    \n",
      "module.lstm.bias_hh_l0  dot:  2871126.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18716966.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23017.0078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1582996.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1582996.0    \n",
      "module.adapter.frcn_linear.weight  dot:  63545328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40983.2734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  301690.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  189.75323486328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  243458.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15813810.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60973.69140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16116.732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  238.2728271484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  201239.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8010268831858411e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4485693.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60973.69140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1169.013916015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  197.8588409423828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4185.89453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6021450.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9507473.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452723.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  731/6933] Loss: -855.7207 [iq: 10.1613,ans: 8.5977,interp: 9.3855,fusion: -883.8652]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  349280.78125    \n",
      "module.ans_embedding.weight  dot:  1760881.0    \n",
      "module.lstm.weight_ih_l0  dot:  12041444.0    \n",
      "module.lstm.weight_hh_l0  dot:  2591395.0    \n",
      "module.lstm.bias_ih_l0  dot:  827589.0    \n",
      "module.lstm.bias_hh_l0  dot:  827589.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22948216.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4132.37353515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  747327.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  747327.75    \n",
      "module.adapter.frcn_linear.weight  dot:  72252616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46562.07421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  717588.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  411.4419860839844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  724514.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17905114.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70026.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15155.7724609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  185.12298583984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  293287.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5314461.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70026.5078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  148.79574584960938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.07654571533203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  328.829833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6504456.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8754526.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452723.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  732/6933] Loss: -842.7147 [iq: 9.7644,ans: 8.5022,interp: 10.1441,fusion: -871.1254]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  750566.8125    \n",
      "module.ans_embedding.weight  dot:  1233709.75    \n",
      "module.lstm.weight_ih_l0  dot:  6614801.0    \n",
      "module.lstm.weight_hh_l0  dot:  4714286.0    \n",
      "module.lstm.bias_ih_l0  dot:  156112.15625    \n",
      "module.lstm.bias_hh_l0  dot:  156112.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21720062.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  100413.6015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1031078.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1031078.5    \n",
      "module.adapter.frcn_linear.weight  dot:  69482496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48025.6171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  421152.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  218.62445068359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  309079.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19144332.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76491.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12662.607421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  332.7781982421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  208278.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7209913494298235e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7611095.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76491.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13066.65234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2599.55908203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  42158.234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.801159469991489e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6131119.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8645140.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452723.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  733/6933] Loss: -839.0916 [iq: 9.2601,ans: 8.0954,interp: 10.8932,fusion: -867.3403]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  17729918.0    \n",
      "module.ans_embedding.weight  dot:  755923.75    \n",
      "module.lstm.weight_ih_l0  dot:  149456160.0    \n",
      "module.lstm.weight_hh_l0  dot:  14464337.0    \n",
      "module.lstm.bias_ih_l0  dot:  6323511.5    \n",
      "module.lstm.bias_hh_l0  dot:  6323511.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13892740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10530.4296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  296385.09375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  296385.09375    \n",
      "module.adapter.frcn_linear.weight  dot:  54993208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33474.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  376205.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  217.39932250976562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  314668.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15384936.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56844.8984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20286.361328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  361.05865478515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  305822.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5114102.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56844.8984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  490.88134765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  77.9424057006836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2178.29052734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.220446049250313e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5776579.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4981008.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452724.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  734/6933] Loss: -867.5905 [iq: 8.9361,ans: 8.2356,interp: 8.5725,fusion: -893.3347]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1226957.625    \n",
      "module.ans_embedding.weight  dot:  1996056.5    \n",
      "module.lstm.weight_ih_l0  dot:  14172276.0    \n",
      "module.lstm.weight_hh_l0  dot:  2749606.0    \n",
      "module.lstm.bias_ih_l0  dot:  705787.0625    \n",
      "module.lstm.bias_hh_l0  dot:  705787.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  64728760.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17587.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5986434.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5986434.5    \n",
      "module.adapter.frcn_linear.weight  dot:  44665024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29635.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  384847.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  198.65936279296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  336095.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12314118.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47256.58203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10695.3984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  261.83050537109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  163258.734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3370993201533565e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3107300.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47256.58203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1088.446533203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  350.83209228515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4345.5478515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  8276952.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15389207.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452725.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  735/6933] Loss: -832.2966 [iq: 9.3717,ans: 7.7803,interp: 8.1803,fusion: -857.6288]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1229143.0    \n",
      "module.ans_embedding.weight  dot:  860505.875    \n",
      "module.lstm.weight_ih_l0  dot:  14162932.0    \n",
      "module.lstm.weight_hh_l0  dot:  8579880.0    \n",
      "module.lstm.bias_ih_l0  dot:  828116.625    \n",
      "module.lstm.bias_hh_l0  dot:  828116.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16624201.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  48506.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  656919.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  656919.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  53401648.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32917.7109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  492014.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  286.2374267578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  351427.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15440882.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56546.96484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13612.853515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  185.63223266601562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  151311.953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0668941285985056e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5556494.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56546.96484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  14939.123046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2480.760986328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  50004.953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5610765.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5399543.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452725.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  736/6933] Loss: -859.8885 [iq: 10.2578,ans: 8.0731,interp: 8.0527,fusion: -886.2720]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  483687.6875    \n",
      "module.ans_embedding.weight  dot:  857823.125    \n",
      "module.lstm.weight_ih_l0  dot:  8521145.0    \n",
      "module.lstm.weight_hh_l0  dot:  5495264.0    \n",
      "module.lstm.bias_ih_l0  dot:  515793.375    \n",
      "module.lstm.bias_hh_l0  dot:  515793.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27472564.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  62872.171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1970781.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1970781.125    \n",
      "module.adapter.frcn_linear.weight  dot:  57694084.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32675.92578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  667223.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  327.31732177734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  580956.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17431972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61122.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22055.3359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  302.5572509765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  450725.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6772626.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61122.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2564.443115234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1026.21533203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4684.2607421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7588288.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7408274.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452726.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  737/6933] Loss: -860.3220 [iq: 11.2772,ans: 9.0445,interp: 8.8271,fusion: -889.4708]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  380047.375    \n",
      "module.ans_embedding.weight  dot:  737958.0625    \n",
      "module.lstm.weight_ih_l0  dot:  11256060.0    \n",
      "module.lstm.weight_hh_l0  dot:  3750023.75    \n",
      "module.lstm.bias_ih_l0  dot:  794134.6875    \n",
      "module.lstm.bias_hh_l0  dot:  794134.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17035322.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  51164.48828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  495169.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  495169.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  76214056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52869.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  520834.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  295.6751708984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  407301.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19487372.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73930.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6863.404296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  83.08686828613281    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  130166.0703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781295607856009e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6158118.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73930.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  100617.5625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12909.3876953125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  205672.84375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6156505.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5789464.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452726.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  738/6933] Loss: -897.3643 [iq: 11.5049,ans: 9.1037,interp: 9.2513,fusion: -927.2242]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14656313.0    \n",
      "module.ans_embedding.weight  dot:  1054421.375    \n",
      "module.lstm.weight_ih_l0  dot:  184556800.0    \n",
      "module.lstm.weight_hh_l0  dot:  18358132.0    \n",
      "module.lstm.bias_ih_l0  dot:  11957432.0    \n",
      "module.lstm.bias_hh_l0  dot:  11957432.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18713508.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29750.5546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  923432.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  923432.5    \n",
      "module.adapter.frcn_linear.weight  dot:  80899104.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52765.34765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  687129.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  325.27117919921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  730673.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22012016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80128.140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  69785.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  420.70904541015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  720343.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5704396.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80128.140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2581.7685546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  579.7803955078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10737.283203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.214229214014267e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6040755.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7441798.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452726.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  739/6933] Loss: -857.6900 [iq: 9.1147,ans: 7.8625,interp: 8.5113,fusion: -883.1785]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5038969.0    \n",
      "module.ans_embedding.weight  dot:  1356046.25    \n",
      "module.lstm.weight_ih_l0  dot:  51932896.0    \n",
      "module.lstm.weight_hh_l0  dot:  4199905.5    \n",
      "module.lstm.bias_ih_l0  dot:  3379847.5    \n",
      "module.lstm.bias_hh_l0  dot:  3379847.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21725612.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36301.765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  465027.28125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  465027.28125    \n",
      "module.adapter.frcn_linear.weight  dot:  79207936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50551.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  501760.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  289.3765869140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  378680.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  21521436.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75554.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  50256.9609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  546.0794677734375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  543054.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5497912.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75554.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  296.665771484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  53.01214599609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1059.7369384765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6944194.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6866671.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452727.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  740/6933] Loss: -876.2002 [iq: 9.3480,ans: 8.5633,interp: 8.9012,fusion: -903.0127]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  140693.0625    \n",
      "module.ans_embedding.weight  dot:  1398277.375    \n",
      "module.lstm.weight_ih_l0  dot:  1169505.0    \n",
      "module.lstm.weight_hh_l0  dot:  1103255.0    \n",
      "module.lstm.bias_ih_l0  dot:  51405.53125    \n",
      "module.lstm.bias_hh_l0  dot:  51405.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  47399432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17186.380859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4687501.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4687501.0    \n",
      "module.adapter.frcn_linear.weight  dot:  57542816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38746.78515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  408510.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  218.72103881835938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  338077.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18802336.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69568.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5623.61376953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  151.44454956054688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  126100.3359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.617106696969131e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5304544.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69568.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  308.0440368652344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  78.71963500976562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1220.8197021484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8656892.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15886974.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452727.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  741/6933] Loss: -869.9392 [iq: 7.8820,ans: 7.7998,interp: 9.5322,fusion: -895.1532]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  558965.25    \n",
      "module.ans_embedding.weight  dot:  1478046.75    \n",
      "module.lstm.weight_ih_l0  dot:  3784678.0    \n",
      "module.lstm.weight_hh_l0  dot:  1240190.0    \n",
      "module.lstm.bias_ih_l0  dot:  166476.65625    \n",
      "module.lstm.bias_hh_l0  dot:  166476.65625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36889560.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25198.83203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2988941.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2988941.25    \n",
      "module.adapter.frcn_linear.weight  dot:  58907344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35180.4296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  685479.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  315.3251037597656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1014449.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16684004.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60301.3828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18263.859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  531.1176147460938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  391354.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4276265.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60301.3828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1204.983154296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  260.302490234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8414.2783203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6961282.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10932440.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452728.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  742/6933] Loss: -849.5450 [iq: 7.2595,ans: 7.1458,interp: 8.2096,fusion: -872.1599]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  56528.0078125    \n",
      "module.ans_embedding.weight  dot:  1398508.25    \n",
      "module.lstm.weight_ih_l0  dot:  705750.125    \n",
      "module.lstm.weight_hh_l0  dot:  579132.3125    \n",
      "module.lstm.bias_ih_l0  dot:  40283.37109375    \n",
      "module.lstm.bias_hh_l0  dot:  40283.37109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42669476.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5478.19921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3257697.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3257697.5    \n",
      "module.adapter.frcn_linear.weight  dot:  51643000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33616.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  387995.53125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  201.48397827148438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  280133.28125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7209913494298235e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14810326.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54523.58984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5019.17724609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.792049407958984    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  95111.3515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5067947717616335e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3185544.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54523.58984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1159.753173828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  220.66046142578125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2771.373046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.004086117172847e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6683717.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10247701.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452728.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  743/6933] Loss: -847.1837 [iq: 7.8786,ans: 7.1103,interp: 7.3563,fusion: -869.5289]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  332465.96875    \n",
      "module.ans_embedding.weight  dot:  1316805.625    \n",
      "module.lstm.weight_ih_l0  dot:  9947996.0    \n",
      "module.lstm.weight_hh_l0  dot:  5309121.5    \n",
      "module.lstm.bias_ih_l0  dot:  613166.125    \n",
      "module.lstm.bias_hh_l0  dot:  613166.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24985076.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  76763.140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1250687.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1250687.25    \n",
      "module.adapter.frcn_linear.weight  dot:  49433512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32885.26171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  448998.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  219.89349365234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  379748.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14524673.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54469.671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8612.4775390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  170.42642211914062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  180948.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6176295.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54469.671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  173584.6875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23085.61328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  162898.0625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6383934.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6382048.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452729.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  744/6933] Loss: -876.3409 [iq: 11.5352,ans: 9.3289,interp: 9.9966,fusion: -907.2017]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4294958.0    \n",
      "module.ans_embedding.weight  dot:  1148415.0    \n",
      "module.lstm.weight_ih_l0  dot:  41677672.0    \n",
      "module.lstm.weight_hh_l0  dot:  5576280.0    \n",
      "module.lstm.bias_ih_l0  dot:  2462753.0    \n",
      "module.lstm.bias_hh_l0  dot:  2462753.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23814900.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33712.078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1495154.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1495154.5    \n",
      "module.adapter.frcn_linear.weight  dot:  73200832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48591.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  702226.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  380.852294921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  531381.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19104928.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72209.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15001.796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  238.51756286621094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  197114.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5137580905720824e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4169293.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72209.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3.964146852493286    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.1065833568572998    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9.282514572143555    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6032329.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7404205.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452729.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  745/6933] Loss: -893.0223 [iq: 9.3503,ans: 7.5259,interp: 7.4813,fusion: -917.3799]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  237466.96875    \n",
      "module.ans_embedding.weight  dot:  1170134.875    \n",
      "module.lstm.weight_ih_l0  dot:  1112778.75    \n",
      "module.lstm.weight_hh_l0  dot:  522269.875    \n",
      "module.lstm.bias_ih_l0  dot:  27070.58203125    \n",
      "module.lstm.bias_hh_l0  dot:  27070.58203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19887302.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  70665.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1032395.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1032395.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  64793656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38084.1484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  742223.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  378.2802429199219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  774031.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16890190.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58383.7734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11131.861328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  184.31173706054688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  226143.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.760846187920833e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4387707.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58383.7734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1318.8232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  247.67608642578125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3874.93408203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5856841.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7451381.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452730.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  746/6933] Loss: -878.6606 [iq: 10.3681,ans: 8.2960,interp: 7.8956,fusion: -905.2204]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  752282.875    \n",
      "module.ans_embedding.weight  dot:  802949.5    \n",
      "module.lstm.weight_ih_l0  dot:  9967492.0    \n",
      "module.lstm.weight_hh_l0  dot:  1605863.0    \n",
      "module.lstm.bias_ih_l0  dot:  644938.75    \n",
      "module.lstm.bias_hh_l0  dot:  644938.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17186684.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37996.01171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  701412.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  701412.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  94351664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66220.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1369730.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  749.2396240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1232495.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  20971670.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  81774.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13144.626953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  300.22723388671875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  281048.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6575540939811617e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5430628.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  81774.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  497.5442810058594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  119.29270935058594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1162.1859130859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.705036691480927e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5057132.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5349716.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452730.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  747/6933] Loss: -884.6299 [iq: 10.2515,ans: 8.2470,interp: 9.3551,fusion: -912.4834]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  183424.09375    \n",
      "module.ans_embedding.weight  dot:  582193.9375    \n",
      "module.lstm.weight_ih_l0  dot:  4066056.25    \n",
      "module.lstm.weight_hh_l0  dot:  849527.6875    \n",
      "module.lstm.bias_ih_l0  dot:  270227.03125    \n",
      "module.lstm.bias_hh_l0  dot:  270227.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14638592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6492.52734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1131133.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1131133.0    \n",
      "module.adapter.frcn_linear.weight  dot:  85570248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  63995.13671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  689733.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  310.0396728515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  584215.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6427748050773516e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20857748.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  82798.90625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8385.8603515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  150.268798828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  187624.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5272663.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  82798.90625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  763.30810546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  200.53860473632812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2988.813720703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5630799.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7544108.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452731.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  748/6933] Loss: -899.5581 [iq: 8.4624,ans: 7.5912,interp: 8.1427,fusion: -923.7544]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2027428.5    \n",
      "module.ans_embedding.weight  dot:  538230.5    \n",
      "module.lstm.weight_ih_l0  dot:  8575619.0    \n",
      "module.lstm.weight_hh_l0  dot:  3069919.5    \n",
      "module.lstm.bias_ih_l0  dot:  329709.625    \n",
      "module.lstm.bias_hh_l0  dot:  329709.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12699050.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10386.6142578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  433836.71875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  433836.71875    \n",
      "module.adapter.frcn_linear.weight  dot:  66851676.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42069.00390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  480290.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  249.2095947265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  392616.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17525770.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64745.25390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24174.62890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  351.48406982421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  399983.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5380295.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64745.25390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  102.24181365966797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.793588638305664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  205.45785522460938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6422418980255316e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5129712.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5661975.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452731.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  749/6933] Loss: -886.6512 [iq: 9.5526,ans: 9.1736,interp: 10.7470,fusion: -916.1245]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5638087.5    \n",
      "module.ans_embedding.weight  dot:  956656.5    \n",
      "module.lstm.weight_ih_l0  dot:  51897288.0    \n",
      "module.lstm.weight_hh_l0  dot:  20594164.0    \n",
      "module.lstm.bias_ih_l0  dot:  2831093.5    \n",
      "module.lstm.bias_hh_l0  dot:  2831093.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21582396.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10491.861328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1068080.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1068080.25    \n",
      "module.adapter.frcn_linear.weight  dot:  58377164.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36835.8046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  424274.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  255.56869506835938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  348986.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16509482.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61050.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35979.08203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  350.3145751953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  395351.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.439936335780658e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6166317.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61050.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  136.80941772460938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.396080017089844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  449.74957275390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6576465.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8835806.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452731.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  750/6933] Loss: -852.0934 [iq: 8.9239,ans: 8.8448,interp: 11.4244,fusion: -881.2865]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  222155.40625    \n",
      "module.ans_embedding.weight  dot:  465101.34375    \n",
      "module.lstm.weight_ih_l0  dot:  1974983.875    \n",
      "module.lstm.weight_hh_l0  dot:  1101715.75    \n",
      "module.lstm.bias_ih_l0  dot:  95765.03125    \n",
      "module.lstm.bias_hh_l0  dot:  95765.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12250972.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30189.69140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  584233.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  584233.875    \n",
      "module.adapter.frcn_linear.weight  dot:  67051792.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42801.09765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  777460.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  408.9269104003906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  771598.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18774402.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65900.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12182.9658203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  288.4214172363281    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  288944.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4711296.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65900.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  87.36529541015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.503711700439453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  196.65370178222656    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5315100.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6750663.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452732.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  751/6933] Loss: -910.9355 [iq: 9.5958,ans: 8.6810,interp: 8.2376,fusion: -937.4500]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2146329.5    \n",
      "module.ans_embedding.weight  dot:  837634.5    \n",
      "module.lstm.weight_ih_l0  dot:  23375548.0    \n",
      "module.lstm.weight_hh_l0  dot:  8329983.0    \n",
      "module.lstm.bias_ih_l0  dot:  1472857.0    \n",
      "module.lstm.bias_hh_l0  dot:  1472857.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32297572.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  157402.9375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2397111.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2397111.5    \n",
      "module.adapter.frcn_linear.weight  dot:  54044064.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35120.7890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  607677.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  282.4871826171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  437030.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16780272.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62319.6796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16519.302734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  313.90936279296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  259542.734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.42746647624881e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3995539.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62319.6796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  156036.125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31893.89453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  520334.6875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7096631.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9696288.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452732.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  752/6933] Loss: -883.0563 [iq: 9.0606,ans: 8.1563,interp: 8.0487,fusion: -908.3220]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6154615.0    \n",
      "module.ans_embedding.weight  dot:  914423.375    \n",
      "module.lstm.weight_ih_l0  dot:  57253824.0    \n",
      "module.lstm.weight_hh_l0  dot:  8178323.0    \n",
      "module.lstm.bias_ih_l0  dot:  3287413.0    \n",
      "module.lstm.bias_hh_l0  dot:  3287413.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23616552.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24471.380859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1474331.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1474331.25    \n",
      "module.adapter.frcn_linear.weight  dot:  61683072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35130.046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1233748.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  609.8734130859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  945166.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18309126.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63474.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  24767.60546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  265.92181396484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  265750.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2290968243178213e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4298956.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63474.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  467.52801513671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  67.29976654052734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1388.07763671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7268568.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6856235.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452733.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  753/6933] Loss: -841.8713 [iq: 8.6754,ans: 7.3254,interp: 7.7015,fusion: -865.5735]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3806732.0    \n",
      "module.ans_embedding.weight  dot:  1102049.5    \n",
      "module.lstm.weight_ih_l0  dot:  71396144.0    \n",
      "module.lstm.weight_hh_l0  dot:  7751151.0    \n",
      "module.lstm.bias_ih_l0  dot:  3401765.0    \n",
      "module.lstm.bias_hh_l0  dot:  3401765.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17196024.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30219.193359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1252467.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1252467.25    \n",
      "module.adapter.frcn_linear.weight  dot:  65821592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  43496.9296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  689457.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  391.1988830566406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  586782.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15514701.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57955.69140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9189.2099609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  129.35479736328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  164728.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4796940.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57955.69140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6195.69970703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1656.1922607421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  33744.0390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5359015.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6847755.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452733.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  754/6933] Loss: -912.7473 [iq: 7.6143,ans: 7.1046,interp: 6.9959,fusion: -934.4621]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  427362.28125    \n",
      "module.ans_embedding.weight  dot:  641809.125    \n",
      "module.lstm.weight_ih_l0  dot:  3275070.0    \n",
      "module.lstm.weight_hh_l0  dot:  1583698.875    \n",
      "module.lstm.bias_ih_l0  dot:  167325.6875    \n",
      "module.lstm.bias_hh_l0  dot:  167325.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12114166.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23842.5703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  390012.09375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  390012.09375    \n",
      "module.adapter.frcn_linear.weight  dot:  52841436.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28869.662109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  706123.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  332.78546142578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  501494.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14241151.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50329.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12411.794921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  315.9317626953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  265100.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5182679.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50329.8203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  452.18109130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  103.43547058105469    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  945.1121826171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5031770.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5063902.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452734.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  755/6933] Loss: -853.8071 [iq: 11.1297,ans: 8.9669,interp: 9.1540,fusion: -883.0577]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  907862.0    \n",
      "module.ans_embedding.weight  dot:  556849.0625    \n",
      "module.lstm.weight_ih_l0  dot:  32485670.0    \n",
      "module.lstm.weight_hh_l0  dot:  12385551.0    \n",
      "module.lstm.bias_ih_l0  dot:  2121445.75    \n",
      "module.lstm.bias_hh_l0  dot:  2121445.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12326266.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5954.1396484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  463062.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  463062.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  84308472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  56504.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1032052.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  494.8546142578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  865743.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21486754.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  79018.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14184.1572265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  184.23672485351562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  271972.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7780448.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  79018.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  259.46710205078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.448036193847656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1561.87744140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6019311.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6222144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452734.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  756/6933] Loss: -873.6321 [iq: 9.9949,ans: 8.8125,interp: 9.1134,fusion: -901.5529]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1806770.75    \n",
      "module.ans_embedding.weight  dot:  864156.125    \n",
      "module.lstm.weight_ih_l0  dot:  7301265.5    \n",
      "module.lstm.weight_hh_l0  dot:  1013042.5    \n",
      "module.lstm.bias_ih_l0  dot:  249624.25    \n",
      "module.lstm.bias_hh_l0  dot:  249624.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17785000.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16546.33984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1109710.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1109710.5    \n",
      "module.adapter.frcn_linear.weight  dot:  60620832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36995.6953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1017118.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  497.391845703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1050856.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.047922968515195e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15747983.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56880.9375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19628.2265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  322.95208740234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  327746.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.822151484200731e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4047485.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56880.9375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  338.76678466796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  77.93730163574219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1418.9776611328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5618859.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8246731.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452734.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  757/6933] Loss: -860.9789 [iq: 7.9360,ans: 7.1750,interp: 7.6499,fusion: -883.7398]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  42554.84765625    \n",
      "module.ans_embedding.weight  dot:  1230518.75    \n",
      "module.lstm.weight_ih_l0  dot:  1174128.25    \n",
      "module.lstm.weight_hh_l0  dot:  635028.375    \n",
      "module.lstm.bias_ih_l0  dot:  79271.59375    \n",
      "module.lstm.bias_hh_l0  dot:  79271.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22012902.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21363.47265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1329118.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1329118.375    \n",
      "module.adapter.frcn_linear.weight  dot:  61248788.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41216.67578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  556263.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  258.8296203613281    \n",
      "module.attflat_img.mlp.linear.weight  dot:  463578.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.606537787476555e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16016355.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57222.9921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2507.436767578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.65707015991211    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  38805.8828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.070823595408001e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4421150.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57222.9921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.564993858337402    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.2492923736572266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  16.297876358032227    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5709536.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8191654.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452735.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  758/6933] Loss: -871.8135 [iq: 9.1330,ans: 7.8279,interp: 8.4757,fusion: -897.2501]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  901270.875    \n",
      "module.ans_embedding.weight  dot:  1148731.25    \n",
      "module.lstm.weight_ih_l0  dot:  26888568.0    \n",
      "module.lstm.weight_hh_l0  dot:  15798011.0    \n",
      "module.lstm.bias_ih_l0  dot:  1911686.25    \n",
      "module.lstm.bias_hh_l0  dot:  1911686.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20689272.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3834.42724609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  757361.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  757361.75    \n",
      "module.adapter.frcn_linear.weight  dot:  84882672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  61979.89453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  461951.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  267.4405517578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  407583.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.1125182431424037e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25489884.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  94290.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13731.927734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  186.14468383789062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  282295.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.459313332627062e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  12571271.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  94290.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  26.753055572509766    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.470523357391357    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37.288352966308594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.551115123125783e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6952910.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7713027.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452735.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  759/6933] Loss: -875.1813 [iq: 10.2652,ans: 9.3324,interp: 10.1388,fusion: -904.9177]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2457379.5    \n",
      "module.ans_embedding.weight  dot:  519094.25    \n",
      "module.lstm.weight_ih_l0  dot:  27800516.0    \n",
      "module.lstm.weight_hh_l0  dot:  5093908.0    \n",
      "module.lstm.bias_ih_l0  dot:  1603266.0    \n",
      "module.lstm.bias_hh_l0  dot:  1603266.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12023592.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  65278.31640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  780242.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  780242.25    \n",
      "module.adapter.frcn_linear.weight  dot:  48186100.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28891.017578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  684610.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  320.0899353027344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  655572.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15426602.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55413.2265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14371.912109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  201.8648681640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  241731.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.751221472863108e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4654643.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55413.2265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1405.644287109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  429.7330322265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3545.97265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4970104.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5526707.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452736.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  760/6933] Loss: -887.2744 [iq: 8.9595,ans: 8.3148,interp: 8.7181,fusion: -913.2668]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1221028.875    \n",
      "module.ans_embedding.weight  dot:  2035163.5    \n",
      "module.lstm.weight_ih_l0  dot:  29987652.0    \n",
      "module.lstm.weight_hh_l0  dot:  21291992.0    \n",
      "module.lstm.bias_ih_l0  dot:  1996559.75    \n",
      "module.lstm.bias_hh_l0  dot:  1996559.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  37759320.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12202.56640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2009395.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2009395.25    \n",
      "module.adapter.frcn_linear.weight  dot:  64028452.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42718.5390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  407561.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  224.28558349609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  283524.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20894794.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76881.9765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10825.34375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  171.96066284179688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  151653.984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8559362.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76881.9765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  427.6993103027344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  91.08087921142578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  701.546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.594813170413545e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7141757.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9403878.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452736.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  761/6933] Loss: -887.1519 [iq: 10.3901,ans: 8.9242,interp: 10.1245,fusion: -916.5907]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  240882.5    \n",
      "module.ans_embedding.weight  dot:  1350006.25    \n",
      "module.lstm.weight_ih_l0  dot:  6687079.5    \n",
      "module.lstm.weight_hh_l0  dot:  6235604.0    \n",
      "module.lstm.bias_ih_l0  dot:  445176.75    \n",
      "module.lstm.bias_hh_l0  dot:  445176.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19338388.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13309.8212890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  572445.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  572445.0    \n",
      "module.adapter.frcn_linear.weight  dot:  57395840.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39719.6953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  339776.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  187.87637329101562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  278968.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.822151484200731e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19090510.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71740.828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8141.3779296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  93.9326171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  149671.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.386926543200389e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7237664.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71740.828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  376.231689453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.77799987792969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  672.0986328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5863320.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8569333.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452736.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  762/6933] Loss: -871.4205 [iq: 10.1489,ans: 9.3776,interp: 9.6003,fusion: -900.5473]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1685547.875    \n",
      "module.ans_embedding.weight  dot:  456332.59375    \n",
      "module.lstm.weight_ih_l0  dot:  13762054.0    \n",
      "module.lstm.weight_hh_l0  dot:  2048006.5    \n",
      "module.lstm.bias_ih_l0  dot:  782302.75    \n",
      "module.lstm.bias_hh_l0  dot:  782302.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10641751.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13587.955078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  476977.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  476977.25    \n",
      "module.adapter.frcn_linear.weight  dot:  64066452.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44479.921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  501760.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  360.2259216308594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  539162.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16803532.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62875.00390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19635.01171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  374.2003479003906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  325732.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6100502.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62875.00390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  308.7483215332031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  59.4658088684082    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1204.5474853515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5590964.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5453183.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452737.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  763/6933] Loss: -908.4900 [iq: 8.2853,ans: 7.4838,interp: 7.2560,fusion: -931.5151]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  534002.5625    \n",
      "module.ans_embedding.weight  dot:  671735.375    \n",
      "module.lstm.weight_ih_l0  dot:  12425194.0    \n",
      "module.lstm.weight_hh_l0  dot:  1930418.0    \n",
      "module.lstm.bias_ih_l0  dot:  730241.4375    \n",
      "module.lstm.bias_hh_l0  dot:  730241.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11754585.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6190.560546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  357722.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  357722.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  54554376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33806.46484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  606117.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  338.00860595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  487075.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15489614.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54619.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9799.8681640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  192.92431640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  178445.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.70142663794104e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5416756.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54619.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  114.31673431396484    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  29.540590286254883    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  383.68084716796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4916570.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5027743.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452737.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  764/6933] Loss: -860.5530 [iq: 10.0019,ans: 8.7272,interp: 8.9997,fusion: -888.2818]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3146841.25    \n",
      "module.ans_embedding.weight  dot:  901293.0625    \n",
      "module.lstm.weight_ih_l0  dot:  27958048.0    \n",
      "module.lstm.weight_hh_l0  dot:  6100940.0    \n",
      "module.lstm.bias_ih_l0  dot:  1586115.375    \n",
      "module.lstm.bias_hh_l0  dot:  1586115.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25301290.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  61035.7265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1868060.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1868060.875    \n",
      "module.adapter.frcn_linear.weight  dot:  58275180.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37926.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  618962.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  327.2608642578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  418695.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16357336.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57818.15234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28133.927734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  343.6838684082031    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  434928.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4168121.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57818.15234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  41341.82421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8595.271484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  104530.3984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5367201.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6825181.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452738.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  765/6933] Loss: -871.0795 [iq: 8.7246,ans: 7.7189,interp: 7.4478,fusion: -894.9708]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5841638.5    \n",
      "module.ans_embedding.weight  dot:  670678.9375    \n",
      "module.lstm.weight_ih_l0  dot:  79241592.0    \n",
      "module.lstm.weight_hh_l0  dot:  8404464.0    \n",
      "module.lstm.bias_ih_l0  dot:  4967516.5    \n",
      "module.lstm.bias_hh_l0  dot:  4967516.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16676213.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38399.21875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  633659.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  633659.75    \n",
      "module.adapter.frcn_linear.weight  dot:  91297360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  61092.2265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1476909.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  604.3178100585938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1586159.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.725290298461914e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21703784.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76488.984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  67203.203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  622.9209594726562    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  902640.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5966901.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76488.984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  463.0529479980469    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  80.61087036132812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  954.0628662109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5511514.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5630358.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452738.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  766/6933] Loss: -878.4046 [iq: 9.3927,ans: 8.1805,interp: 8.2999,fusion: -904.2777]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  358555.75    \n",
      "module.ans_embedding.weight  dot:  620883.125    \n",
      "module.lstm.weight_ih_l0  dot:  14323420.0    \n",
      "module.lstm.weight_hh_l0  dot:  12456397.0    \n",
      "module.lstm.bias_ih_l0  dot:  842067.0    \n",
      "module.lstm.bias_hh_l0  dot:  842067.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14230087.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25946.216796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  908873.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  908873.375    \n",
      "module.adapter.frcn_linear.weight  dot:  63847044.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40100.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  523325.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  302.28375244140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  453222.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5547706172801554e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17552704.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59232.19140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8307.126953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  197.74520874023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144123.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.798597157991026e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5144620.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59232.19140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1721.175048828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  472.4459533691406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2418.6474609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5420648.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5504386.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452739.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  767/6933] Loss: -852.9082 [iq: 8.7082,ans: 7.8830,interp: 8.3843,fusion: -877.8837]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1824023.375    \n",
      "module.ans_embedding.weight  dot:  964870.375    \n",
      "module.lstm.weight_ih_l0  dot:  45929716.0    \n",
      "module.lstm.weight_hh_l0  dot:  9366000.0    \n",
      "module.lstm.bias_ih_l0  dot:  3169045.5    \n",
      "module.lstm.bias_hh_l0  dot:  3169045.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20503616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25513.76171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2473034.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2473034.75    \n",
      "module.adapter.frcn_linear.weight  dot:  77789528.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57284.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  704189.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  358.44268798828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  773959.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23093772.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  84893.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17922.509765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  226.1475067138672    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  316334.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.5688929023745e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7324514.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  84893.1484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1911.453369140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  403.90472412109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6375.73681640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6238662.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9676346.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452739.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  768/6933] Loss: -889.5228 [iq: 8.1614,ans: 8.3230,interp: 9.1169,fusion: -915.1241]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1411044.625    \n",
      "module.ans_embedding.weight  dot:  1323495.625    \n",
      "module.lstm.weight_ih_l0  dot:  4189661.0    \n",
      "module.lstm.weight_hh_l0  dot:  1767204.125    \n",
      "module.lstm.bias_ih_l0  dot:  58265.3203125    \n",
      "module.lstm.bias_hh_l0  dot:  58265.3203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  33869812.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16371.0703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2447504.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2447504.0    \n",
      "module.adapter.frcn_linear.weight  dot:  81988816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51809.76171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1058081.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  417.4949951171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  783372.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21740256.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76237.921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10913.3193359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  165.8641357421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  199060.984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.496097633615136e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8354467.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76237.921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  320.7642517089844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.54209899902344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  942.7039794921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8654601.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13362244.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452740.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  769/6933] Loss: -860.1632 [iq: 9.7066,ans: 8.8220,interp: 10.2111,fusion: -888.9030]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5723218.0    \n",
      "module.ans_embedding.weight  dot:  873337.875    \n",
      "module.lstm.weight_ih_l0  dot:  53798760.0    \n",
      "module.lstm.weight_hh_l0  dot:  6276849.0    \n",
      "module.lstm.bias_ih_l0  dot:  3034769.25    \n",
      "module.lstm.bias_hh_l0  dot:  3034769.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12996432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  67430.9609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  549429.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  549429.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  54795412.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34197.92578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  487388.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  253.5897674560547    \n",
      "module.attflat_img.mlp.linear.weight  dot:  515198.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  16039818.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55588.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19151.357421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  281.7772216796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  333976.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.701266683085123e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4384907.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55588.0078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2294.05322265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  533.18310546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10235.56640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5356138.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5349008.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452740.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  770/6933] Loss: -904.3022 [iq: 7.3583,ans: 7.1290,interp: 7.5019,fusion: -926.2914]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7716711.5    \n",
      "module.ans_embedding.weight  dot:  1620944.25    \n",
      "module.lstm.weight_ih_l0  dot:  115052424.0    \n",
      "module.lstm.weight_hh_l0  dot:  11852506.0    \n",
      "module.lstm.bias_ih_l0  dot:  7575664.0    \n",
      "module.lstm.bias_hh_l0  dot:  7575664.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  71711256.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  89815.03125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5389400.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5389400.0    \n",
      "module.adapter.frcn_linear.weight  dot:  116109368.0    \n",
      "module.adapter.frcn_linear.bias  dot:  80176.0390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  983962.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.1484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1361388.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29997286.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  102572.5546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37826.453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  297.90362548828125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  535369.1875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.697963155806065e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7916104.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  102572.5546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5013.7861328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  822.4342041015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  33393.640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9466521.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12835936.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452741.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  771/6933] Loss: -870.9133 [iq: 8.1781,ans: 7.2322,interp: 7.4559,fusion: -893.7795]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14619991.0    \n",
      "module.ans_embedding.weight  dot:  481448.0    \n",
      "module.lstm.weight_ih_l0  dot:  104355328.0    \n",
      "module.lstm.weight_hh_l0  dot:  8337083.0    \n",
      "module.lstm.bias_ih_l0  dot:  3987974.5    \n",
      "module.lstm.bias_hh_l0  dot:  3987974.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10620232.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25128.64453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  249613.015625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  249613.015625    \n",
      "module.adapter.frcn_linear.weight  dot:  44014376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25630.4765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  792627.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  424.5657958984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  728427.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1923702913918532e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  13357711.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47059.72265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13312.396484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  255.1116943359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  142760.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4238572.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47059.72265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  455.2860107421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  89.93700408935547    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1626.927734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4548311.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4129286.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452742.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  772/6933] Loss: -881.6490 [iq: 9.0661,ans: 7.9206,interp: 7.4999,fusion: -906.1357]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6643171.0    \n",
      "module.ans_embedding.weight  dot:  1106016.625    \n",
      "module.lstm.weight_ih_l0  dot:  70066968.0    \n",
      "module.lstm.weight_hh_l0  dot:  14430928.0    \n",
      "module.lstm.bias_ih_l0  dot:  4309225.0    \n",
      "module.lstm.bias_hh_l0  dot:  4309225.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27520656.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15326.5087890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1693022.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1693022.625    \n",
      "module.adapter.frcn_linear.weight  dot:  81743088.0    \n",
      "module.adapter.frcn_linear.bias  dot:  58472.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  464935.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  261.00701904296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  383272.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26499868.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  98636.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23587.375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  305.6162109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  302563.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.532783451504656e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6244990.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  98636.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  547.4063720703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  119.72866821289062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2432.14599609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7331237.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7494727.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452742.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  773/6933] Loss: -831.9991 [iq: 8.2103,ans: 6.9147,interp: 6.8405,fusion: -853.9646]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1129252.125    \n",
      "module.ans_embedding.weight  dot:  1067101.125    \n",
      "module.lstm.weight_ih_l0  dot:  45518568.0    \n",
      "module.lstm.weight_hh_l0  dot:  8545788.0    \n",
      "module.lstm.bias_ih_l0  dot:  3143177.0    \n",
      "module.lstm.bias_hh_l0  dot:  3143177.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17710994.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12591.22265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  275278.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  275278.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  105611440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  69799.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1266010.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  527.2647705078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1400691.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.87805368215777e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  25225068.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  87254.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17505.31640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  364.1161804199219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  260448.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.726267211983213e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8973962.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  87254.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  388.55096435546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  112.04261779785156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1564.947265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5805363.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5181850.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452743.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  774/6933] Loss: -842.7607 [iq: 11.4617,ans: 9.6139,interp: 10.1405,fusion: -873.9768]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6981764.0    \n",
      "module.ans_embedding.weight  dot:  708899.75    \n",
      "module.lstm.weight_ih_l0  dot:  123263520.0    \n",
      "module.lstm.weight_hh_l0  dot:  13978884.0    \n",
      "module.lstm.bias_ih_l0  dot:  7813314.0    \n",
      "module.lstm.bias_hh_l0  dot:  7813314.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16123978.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15746.5302734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  469618.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  469618.375    \n",
      "module.adapter.frcn_linear.weight  dot:  90411688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  65000.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  702190.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  383.3670654296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  731054.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24457972.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  87090.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  21679.681640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  355.580322265625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  341073.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7916140.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  87090.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  358.24700927734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.54466247558594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  737.57080078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.091749078976136e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5174101.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5151645.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452743.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  775/6933] Loss: -849.2111 [iq: 10.9255,ans: 9.2896,interp: 9.5154,fusion: -878.9416]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3355985.0    \n",
      "module.ans_embedding.weight  dot:  682670.9375    \n",
      "module.lstm.weight_ih_l0  dot:  13551973.0    \n",
      "module.lstm.weight_hh_l0  dot:  2172976.0    \n",
      "module.lstm.bias_ih_l0  dot:  495538.34375    \n",
      "module.lstm.bias_hh_l0  dot:  495538.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18166324.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8810.89453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1122946.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1122946.375    \n",
      "module.adapter.frcn_linear.weight  dot:  54337536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37524.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  254267.078125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  149.18771362304688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  166610.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18603582.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65377.04296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11532.29296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  240.63975524902344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172879.734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5824579.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65377.04296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  257.1675720214844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.672122955322266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  807.5421142578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5964227.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8914905.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452744.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  776/6933] Loss: -846.8157 [iq: 9.1758,ans: 9.0968,interp: 10.5279,fusion: -875.6163]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11091568.0    \n",
      "module.ans_embedding.weight  dot:  910195.625    \n",
      "module.lstm.weight_ih_l0  dot:  126817280.0    \n",
      "module.lstm.weight_hh_l0  dot:  12396150.0    \n",
      "module.lstm.bias_ih_l0  dot:  7848628.0    \n",
      "module.lstm.bias_hh_l0  dot:  7848628.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26436990.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20776.09375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1155532.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1155532.5    \n",
      "module.adapter.frcn_linear.weight  dot:  65205120.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42550.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  629628.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  335.4017639160156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  535712.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.1154423747211695e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18294146.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63475.1328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  37156.09375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  460.1424255371094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  667801.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6575540939811617e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5182245.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63475.1328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  281.0250244140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.162452697753906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1555.606201171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6284034.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7756958.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452744.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  777/6933] Loss: -875.3110 [iq: 9.7382,ans: 8.9402,interp: 9.5465,fusion: -903.5358]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  485372.09375    \n",
      "module.ans_embedding.weight  dot:  1120334.875    \n",
      "module.lstm.weight_ih_l0  dot:  4248907.0    \n",
      "module.lstm.weight_hh_l0  dot:  1991802.5    \n",
      "module.lstm.bias_ih_l0  dot:  187793.484375    \n",
      "module.lstm.bias_hh_l0  dot:  187793.484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18574120.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  97719.8125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1050321.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1050321.125    \n",
      "module.adapter.frcn_linear.weight  dot:  49504520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30760.57421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  283345.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  138.48265075683594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  253382.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16197030.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52907.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12756.61328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  178.99864196777344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  163526.390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.266986929404084e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5383650.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52907.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  17680.166015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3296.016357421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  64926.51953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6165728.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7240217.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452744.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  778/6933] Loss: -883.5321 [iq: 9.7315,ans: 8.2287,interp: 8.4686,fusion: -909.9610]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2725528.75    \n",
      "module.ans_embedding.weight  dot:  535494.75    \n",
      "module.lstm.weight_ih_l0  dot:  35313936.0    \n",
      "module.lstm.weight_hh_l0  dot:  9065263.0    \n",
      "module.lstm.bias_ih_l0  dot:  2095880.75    \n",
      "module.lstm.bias_hh_l0  dot:  2095880.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17269644.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17684.3515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1114918.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1114918.875    \n",
      "module.adapter.frcn_linear.weight  dot:  62607240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41514.12109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  732015.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  358.50830078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  602477.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.584762791637331e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20044174.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67927.9609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19079.67578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  209.19398498535156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  292532.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5559303.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67927.9609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  32.74278259277344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.778842926025391    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  82.11040496826172    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5499830.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7247289.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452745.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  779/6933] Loss: -918.6464 [iq: 8.6783,ans: 7.6386,interp: 7.9172,fusion: -942.8805]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8455849.0    \n",
      "module.ans_embedding.weight  dot:  654177.125    \n",
      "module.lstm.weight_ih_l0  dot:  85985200.0    \n",
      "module.lstm.weight_hh_l0  dot:  10692842.0    \n",
      "module.lstm.bias_ih_l0  dot:  5122244.0    \n",
      "module.lstm.bias_hh_l0  dot:  5122244.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12370664.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  64190.3515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  402246.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  402246.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  106467728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  69503.2265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  793480.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  357.19677734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  725738.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.90333446173463e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26024720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  83199.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  36793.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  308.37152099609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  569911.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1004885891452432e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6824276.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  83199.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1046.9232177734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  215.44769287109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1635.7117919921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9984014443252818e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5248049.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4530462.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452745.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  780/6933] Loss: -885.7862 [iq: 8.3970,ans: 7.2857,interp: 8.0700,fusion: -909.5389]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  667582.0    \n",
      "module.ans_embedding.weight  dot:  1290185.5    \n",
      "module.lstm.weight_ih_l0  dot:  11348455.0    \n",
      "module.lstm.weight_hh_l0  dot:  1991171.875    \n",
      "module.lstm.bias_ih_l0  dot:  705611.75    \n",
      "module.lstm.bias_hh_l0  dot:  705611.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25710116.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11511.03515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1702875.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1702875.0    \n",
      "module.adapter.frcn_linear.weight  dot:  59706932.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35623.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  585248.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  244.58529663085938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  645984.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18669396.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58394.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9795.455078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  89.08381652832031    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  107710.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4528596.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58394.5078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  80.39089965820312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.287664413452148    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  201.784423828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6409676.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7969006.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452746.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  781/6933] Loss: -839.4949 [iq: 8.0918,ans: 7.5154,interp: 7.8588,fusion: -862.9609]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  351957.59375    \n",
      "module.ans_embedding.weight  dot:  981807.1875    \n",
      "module.lstm.weight_ih_l0  dot:  2530137.25    \n",
      "module.lstm.weight_hh_l0  dot:  593260.125    \n",
      "module.lstm.bias_ih_l0  dot:  101442.3359375    \n",
      "module.lstm.bias_hh_l0  dot:  101442.3359375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22134234.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33197.61328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1435522.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1435522.75    \n",
      "module.adapter.frcn_linear.weight  dot:  61177704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39993.8046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  394510.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  245.0817108154297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  278978.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19329436.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66144.109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10595.7109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  194.40321350097656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  187197.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5322805.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66144.109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  429.01715087890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  91.61866760253906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1356.317138671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7315423.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  11130334.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452746.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  782/6933] Loss: -855.1958 [iq: 11.1502,ans: 9.5678,interp: 11.6569,fusion: -887.5708]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  143885.90625    \n",
      "module.ans_embedding.weight  dot:  1525013.25    \n",
      "module.lstm.weight_ih_l0  dot:  1101572.625    \n",
      "module.lstm.weight_hh_l0  dot:  794166.5    \n",
      "module.lstm.bias_ih_l0  dot:  16712.447265625    \n",
      "module.lstm.bias_hh_l0  dot:  16712.447265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20851168.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12879.548828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  429669.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  429669.25    \n",
      "module.adapter.frcn_linear.weight  dot:  53144184.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31768.78125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  704766.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  368.1365966796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  689045.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  15304880.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52745.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6355.294921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  128.9850311279297    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  53675.8359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.266986929404084e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5234314.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52745.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  714.3795166015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  127.98423767089844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3391.02734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5556152.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6769530.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452747.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  783/6933] Loss: -837.7764 [iq: 10.6376,ans: 9.3924,interp: 9.9715,fusion: -867.7779]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2591520.0    \n",
      "module.ans_embedding.weight  dot:  874854.875    \n",
      "module.lstm.weight_ih_l0  dot:  24703408.0    \n",
      "module.lstm.weight_hh_l0  dot:  3365775.5    \n",
      "module.lstm.bias_ih_l0  dot:  1439463.75    \n",
      "module.lstm.bias_hh_l0  dot:  1439463.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13308808.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4611.20654296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  315476.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  315476.75    \n",
      "module.adapter.frcn_linear.weight  dot:  52741484.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32241.01953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  649523.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  395.5623779296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  744489.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  14816005.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  51080.8046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11584.4755859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  178.49041748046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  223590.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9878322038712213e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5179846.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  51080.8046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.5239902138710022    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.08460031449794769    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2.32304048538208    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.150236124355615e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4893938.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4379224.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452747.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  784/6933] Loss: -908.1454 [iq: 7.2701,ans: 6.8473,interp: 6.8736,fusion: -929.1364]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2642213.75    \n",
      "module.ans_embedding.weight  dot:  579101.625    \n",
      "module.lstm.weight_ih_l0  dot:  16540118.0    \n",
      "module.lstm.weight_hh_l0  dot:  2751466.25    \n",
      "module.lstm.bias_ih_l0  dot:  888082.5    \n",
      "module.lstm.bias_hh_l0  dot:  888082.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12787665.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9480.1435546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  742872.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  742872.625    \n",
      "module.adapter.frcn_linear.weight  dot:  56498272.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33003.5703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  602123.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  324.4964294433594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  519621.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18060920.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60511.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17335.09375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  268.6691589355469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  344354.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.035971308738226e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5284888.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60511.4609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  878.6337890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  148.5819091796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3462.782958984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5111524.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6138389.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452747.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  785/6933] Loss: -916.8038 [iq: 8.2469,ans: 7.6192,interp: 7.9268,fusion: -940.5966]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2908168.75    \n",
      "module.ans_embedding.weight  dot:  1963360.0    \n",
      "module.lstm.weight_ih_l0  dot:  51565392.0    \n",
      "module.lstm.weight_hh_l0  dot:  8081048.0    \n",
      "module.lstm.bias_ih_l0  dot:  3330055.75    \n",
      "module.lstm.bias_hh_l0  dot:  3330055.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27538898.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12648.740234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  438782.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  438782.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  68152464.0    \n",
      "module.adapter.frcn_linear.bias  dot:  49926.6796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  607080.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  355.8760986328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  687335.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  19361406.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70607.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18091.34375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  269.44854736328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  188835.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.251056230306858e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4447762.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70607.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  66.24061584472656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.568175315856934    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  250.1994171142578    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.0397906414236786e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6395618.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6204821.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452748.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  786/6933] Loss: -844.5461 [iq: 9.8682,ans: 8.4291,interp: 8.0274,fusion: -870.8708]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  241347.015625    \n",
      "module.ans_embedding.weight  dot:  308946.375    \n",
      "module.lstm.weight_ih_l0  dot:  7569313.0    \n",
      "module.lstm.weight_hh_l0  dot:  2640505.5    \n",
      "module.lstm.bias_ih_l0  dot:  518859.4375    \n",
      "module.lstm.bias_hh_l0  dot:  518859.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13875034.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21230.84765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  713848.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  713848.75    \n",
      "module.adapter.frcn_linear.weight  dot:  75156496.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50026.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1392981.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  560.4512329101562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1521307.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21195816.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75816.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12937.40234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  238.8522491455078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  319657.0    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7737897.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75816.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3260.539306640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  653.9768676757812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  10183.025390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5777258.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5717442.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452748.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  787/6933] Loss: -890.0514 [iq: 9.3077,ans: 8.5056,interp: 8.4538,fusion: -916.3185]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  321169.84375    \n",
      "module.ans_embedding.weight  dot:  633794.0625    \n",
      "module.lstm.weight_ih_l0  dot:  6554297.0    \n",
      "module.lstm.weight_hh_l0  dot:  1263916.75    \n",
      "module.lstm.bias_ih_l0  dot:  390358.5625    \n",
      "module.lstm.bias_hh_l0  dot:  390358.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12166664.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2112.348388671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  360925.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  360925.875    \n",
      "module.adapter.frcn_linear.weight  dot:  65348796.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48927.32421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  336379.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  183.13436889648438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  292530.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.0595401767641306e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17361836.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64769.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10796.625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  241.46310424804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  205974.328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6782641.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64769.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  114.12344360351562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.171707153320312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  400.2489013671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4906862.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4764445.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452749.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  788/6933] Loss: -915.2238 [iq: 7.6054,ans: 7.6248,interp: 8.3729,fusion: -938.8268]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7836517.0    \n",
      "module.ans_embedding.weight  dot:  1095338.25    \n",
      "module.lstm.weight_ih_l0  dot:  65140224.0    \n",
      "module.lstm.weight_hh_l0  dot:  6107284.0    \n",
      "module.lstm.bias_ih_l0  dot:  3660773.5    \n",
      "module.lstm.bias_hh_l0  dot:  3660773.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14385012.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28765.12109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  438612.90625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  438612.90625    \n",
      "module.adapter.frcn_linear.weight  dot:  70411552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  48093.09375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  703476.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  404.44989013671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  655549.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20110072.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71013.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23886.09765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  325.27972412109375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  331684.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.184347173781134e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4466116.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71013.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1439.563720703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  276.5372314453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8992.84765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2683409877922713e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5167662.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5407149.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452749.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  789/6933] Loss: -836.1598 [iq: 9.7866,ans: 9.2912,interp: 10.1456,fusion: -865.3832]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1171744.75    \n",
      "module.ans_embedding.weight  dot:  747909.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3151781.5    \n",
      "module.lstm.weight_hh_l0  dot:  934692.1875    \n",
      "module.lstm.bias_ih_l0  dot:  61221.37109375    \n",
      "module.lstm.bias_hh_l0  dot:  61221.37109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18515968.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13712.1640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  914121.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  914121.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  50348620.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26558.6796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  980753.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  454.53265380859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  723980.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14512004.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  46488.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13322.6533203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  264.56304931640625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  250089.296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4483055.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  46488.1484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  240.83116149902344    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  68.50186920166016    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  559.6578369140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6205347.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7585739.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452749.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  790/6933] Loss: -876.0169 [iq: 10.2753,ans: 9.1668,interp: 9.8276,fusion: -905.2866]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1264137.25    \n",
      "module.ans_embedding.weight  dot:  1930678.125    \n",
      "module.lstm.weight_ih_l0  dot:  23723966.0    \n",
      "module.lstm.weight_hh_l0  dot:  3052392.5    \n",
      "module.lstm.bias_ih_l0  dot:  1541209.25    \n",
      "module.lstm.bias_hh_l0  dot:  1541209.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  42311236.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4426.0048828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  546014.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  546014.125    \n",
      "module.adapter.frcn_linear.weight  dot:  63389456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36747.5625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  633421.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.08935546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  467098.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.0595401767641306e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17437820.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56948.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16124.7373046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  125.94792175292969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  200461.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.2878590395266656e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4074270.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56948.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  114.932861328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.55501937866211    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  556.81787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.177842084067244e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6061674.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5929841.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452750.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  791/6933] Loss: -848.9828 [iq: 8.6544,ans: 7.4378,interp: 7.5806,fusion: -872.6556]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  18756840.0    \n",
      "module.ans_embedding.weight  dot:  1010076.1875    \n",
      "module.lstm.weight_ih_l0  dot:  296554464.0    \n",
      "module.lstm.weight_hh_l0  dot:  73821976.0    \n",
      "module.lstm.bias_ih_l0  dot:  16398914.0    \n",
      "module.lstm.bias_hh_l0  dot:  16398914.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13937799.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6749.466796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  391551.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  391551.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  85652312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  55051.6171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1006522.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  388.6968994140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1040373.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21190268.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  72884.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25912.09765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  225.23699951171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  257389.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8242317878502945e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7877056.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  72884.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  64.16598510742188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9.59486198425293    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  240.59849548339844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4781655.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5134292.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452750.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  792/6933] Loss: -877.9110 [iq: 10.6926,ans: 9.5099,interp: 9.5068,fusion: -907.6202]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  19044644.0    \n",
      "module.ans_embedding.weight  dot:  454673.9375    \n",
      "module.lstm.weight_ih_l0  dot:  365068416.0    \n",
      "module.lstm.weight_hh_l0  dot:  32130972.0    \n",
      "module.lstm.bias_ih_l0  dot:  19633352.0    \n",
      "module.lstm.bias_hh_l0  dot:  19633352.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17228882.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28431.046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  782366.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  782366.625    \n",
      "module.adapter.frcn_linear.weight  dot:  75482192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  54187.4765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  500589.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  264.1996154785156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  360775.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3568666190622025e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19993862.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73338.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15605.55078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  148.197509765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  249510.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.9487916814759956e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6445962.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73338.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  336.0003356933594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.54353332519531    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  813.1392822265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.941169447405855e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6039876.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5958921.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452751.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  793/6933] Loss: -935.8946 [iq: 8.6194,ans: 8.3406,interp: 7.8745,fusion: -960.7291]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2429436.5    \n",
      "module.ans_embedding.weight  dot:  810716.25    \n",
      "module.lstm.weight_ih_l0  dot:  46949228.0    \n",
      "module.lstm.weight_hh_l0  dot:  10740971.0    \n",
      "module.lstm.bias_ih_l0  dot:  2917860.5    \n",
      "module.lstm.bias_hh_l0  dot:  2917860.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22458072.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  112862.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1471057.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1471057.625    \n",
      "module.adapter.frcn_linear.weight  dot:  73115864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45022.2890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  751863.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  348.87725830078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  996694.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1461906979093328e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21777816.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75108.640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  33208.12890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  262.07965087890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  394828.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6783380.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75108.640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3235.96826171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  889.25537109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8703.638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6599650.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6365855.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452751.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  794/6933] Loss: -912.1584 [iq: 7.8179,ans: 7.4273,interp: 7.8900,fusion: -935.2936]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  375290.25    \n",
      "module.ans_embedding.weight  dot:  1258032.75    \n",
      "module.lstm.weight_ih_l0  dot:  2006729.375    \n",
      "module.lstm.weight_hh_l0  dot:  2611791.75    \n",
      "module.lstm.bias_ih_l0  dot:  106580.546875    \n",
      "module.lstm.bias_hh_l0  dot:  106580.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24366130.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  90704.7734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1081616.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1081616.25    \n",
      "module.adapter.frcn_linear.weight  dot:  79811944.0    \n",
      "module.adapter.frcn_linear.bias  dot:  43449.08984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2393401.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  908.7457885742188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2585677.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.967194738332182e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18464484.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61173.4609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11318.990234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.08631896972656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172106.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6384645.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61173.4609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2187.72412109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  546.239990234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7784.3701171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.827338611652522e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6621496.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7955060.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452752.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  795/6933] Loss: -863.1993 [iq: 9.8225,ans: 9.1708,interp: 10.1138,fusion: -892.3063]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  355185.6875    \n",
      "module.ans_embedding.weight  dot:  1806663.25    \n",
      "module.lstm.weight_ih_l0  dot:  4039255.0    \n",
      "module.lstm.weight_hh_l0  dot:  1006149.625    \n",
      "module.lstm.bias_ih_l0  dot:  134612.53125    \n",
      "module.lstm.bias_hh_l0  dot:  134612.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18027744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40215.0078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  414273.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  414273.5    \n",
      "module.adapter.frcn_linear.weight  dot:  42560368.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27470.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  390089.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  182.9523162841797    \n",
      "module.attflat_img.mlp.linear.weight  dot:  325085.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13110359.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47288.75390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7203.11376953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  151.07632446289062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  128280.421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.87805368215777e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3222019.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47288.75390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13040.630859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2978.199951171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  63622.8203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4546837.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4567518.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452752.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  796/6933] Loss: -870.5936 [iq: 9.3004,ans: 8.0102,interp: 9.0391,fusion: -896.9433]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  462846.125    \n",
      "module.ans_embedding.weight  dot:  458867.25    \n",
      "module.lstm.weight_ih_l0  dot:  2981738.25    \n",
      "module.lstm.weight_hh_l0  dot:  959992.25    \n",
      "module.lstm.bias_ih_l0  dot:  117606.296875    \n",
      "module.lstm.bias_hh_l0  dot:  117606.296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11595946.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10305.5390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  470848.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  470848.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  40587200.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20208.76953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  674492.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  364.58624267578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  526696.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.424748567544157e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  11697704.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  39298.48046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10618.83203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  163.18553161621094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  195449.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5663772501284257e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3875585.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  39298.48046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  372.02734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.63795471191406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2727.601318359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4440740.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4448742.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452753.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  797/6933] Loss: -897.3619 [iq: 8.5974,ans: 8.3909,interp: 7.9538,fusion: -922.3040]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  800772.5    \n",
      "module.ans_embedding.weight  dot:  439594.25    \n",
      "module.lstm.weight_ih_l0  dot:  1988487.25    \n",
      "module.lstm.weight_hh_l0  dot:  796786.0    \n",
      "module.lstm.bias_ih_l0  dot:  34860.2421875    \n",
      "module.lstm.bias_hh_l0  dot:  34860.2421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13961722.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14066.0107421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  614552.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  614552.5    \n",
      "module.adapter.frcn_linear.weight  dot:  56970216.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36084.24609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  410559.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  200.23497009277344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  270159.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15970666.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54732.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8101.23388671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  133.7698211669922    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  163321.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.3024783735745586e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6895378.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54732.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  237.5413818359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  40.574501037597656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  697.82568359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5440153.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5768417.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452753.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  798/6933] Loss: -872.7429 [iq: 10.1788,ans: 8.4195,interp: 8.1738,fusion: -899.5151]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  394289.1875    \n",
      "module.ans_embedding.weight  dot:  239938.28125    \n",
      "module.lstm.weight_ih_l0  dot:  6022665.0    \n",
      "module.lstm.weight_hh_l0  dot:  2747099.5    \n",
      "module.lstm.bias_ih_l0  dot:  376856.0    \n",
      "module.lstm.bias_hh_l0  dot:  376856.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7715660.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5936.2607421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  174329.34375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  174329.34375    \n",
      "module.adapter.frcn_linear.weight  dot:  51235856.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35402.94140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  374987.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  188.76573181152344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  285909.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1256418019911507e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14647587.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  51394.26171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7041.2412109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  86.23985290527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  136116.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4496492894977564e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6072367.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  51394.26171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  77.61213684082031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.98414421081543    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  305.15899658203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.355671213649657e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4312928.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3691798.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452754.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  799/6933] Loss: -878.4634 [iq: 11.6819,ans: 9.1668,interp: 8.6747,fusion: -907.9868]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  390706.40625    \n",
      "module.ans_embedding.weight  dot:  446727.1875    \n",
      "module.lstm.weight_ih_l0  dot:  7906385.0    \n",
      "module.lstm.weight_hh_l0  dot:  10697182.0    \n",
      "module.lstm.bias_ih_l0  dot:  740039.0    \n",
      "module.lstm.bias_hh_l0  dot:  740039.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15706791.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10514.009765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  564149.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  564149.375    \n",
      "module.adapter.frcn_linear.weight  dot:  56936336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39501.37890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  427882.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  237.82821655273438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  326249.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18123640.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63179.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9393.33203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  165.00796508789062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  180625.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3656631381309126e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5153372.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63179.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  523.5772705078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  122.30792236328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1021.33349609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.596878433460461e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5550664.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5259356.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452754.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  800/6933] Loss: -903.8906 [iq: 11.0095,ans: 8.4772,interp: 9.2196,fusion: -932.5969]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1181607.25    \n",
      "module.ans_embedding.weight  dot:  1263924.25    \n",
      "module.lstm.weight_ih_l0  dot:  11569427.0    \n",
      "module.lstm.weight_hh_l0  dot:  6807991.5    \n",
      "module.lstm.bias_ih_l0  dot:  613184.625    \n",
      "module.lstm.bias_hh_l0  dot:  613184.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54693872.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  133680.234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4435728.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4435728.0    \n",
      "module.adapter.frcn_linear.weight  dot:  55660640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35782.2109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  381267.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  208.84742736816406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  338679.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16675560.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55957.53515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15210.001953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  308.16876220703125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  293009.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.648829291108996e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5496339.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55957.53515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18882.49609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1812.0556640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  88563.890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7553981.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12419144.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452755.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  801/6933] Loss: -868.3284 [iq: 11.0571,ans: 9.2698,interp: 9.3497,fusion: -898.0051]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  655983.0    \n",
      "module.ans_embedding.weight  dot:  1265793.125    \n",
      "module.lstm.weight_ih_l0  dot:  8662746.0    \n",
      "module.lstm.weight_hh_l0  dot:  2526815.25    \n",
      "module.lstm.bias_ih_l0  dot:  475016.5625    \n",
      "module.lstm.bias_hh_l0  dot:  475016.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16150094.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25001.98046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  424473.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  424473.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  50610532.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32079.46484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  517893.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  247.13177490234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  608160.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16918384.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56618.45703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11898.37109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  122.34838104248047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  181635.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5069120.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56618.45703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1065.613525390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  283.33721923828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2596.18701171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6212831.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7726032.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452755.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  802/6933] Loss: -911.1217 [iq: 8.4617,ans: 7.5910,interp: 8.4013,fusion: -935.5757]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  281221.0625    \n",
      "module.ans_embedding.weight  dot:  1929343.25    \n",
      "module.lstm.weight_ih_l0  dot:  10124132.0    \n",
      "module.lstm.weight_hh_l0  dot:  4214521.0    \n",
      "module.lstm.bias_ih_l0  dot:  773969.75    \n",
      "module.lstm.bias_hh_l0  dot:  773969.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  48234104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  59635.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5545598.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5545598.0    \n",
      "module.adapter.frcn_linear.weight  dot:  58393852.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38481.73828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  490395.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  244.96563720703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  512316.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19958964.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  73997.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7240.91552734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.02754211425781    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  120541.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6906218.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  73997.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3673.05908203125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  976.9559326171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15908.693359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.530065542800003e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  8277873.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  15819114.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452755.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  803/6933] Loss: -866.9047 [iq: 8.3894,ans: 8.1377,interp: 9.3683,fusion: -892.8001]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2427367.75    \n",
      "module.ans_embedding.weight  dot:  869952.625    \n",
      "module.lstm.weight_ih_l0  dot:  37491980.0    \n",
      "module.lstm.weight_hh_l0  dot:  22101898.0    \n",
      "module.lstm.bias_ih_l0  dot:  2295809.5    \n",
      "module.lstm.bias_hh_l0  dot:  2295809.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18982732.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  54256.5    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1222128.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1222128.25    \n",
      "module.adapter.frcn_linear.weight  dot:  106891104.0    \n",
      "module.adapter.frcn_linear.bias  dot:  71981.53125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1507862.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  596.8359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1801245.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  26333232.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  89597.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  34799.5546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  262.80657958984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  302454.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.961758920922875e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5488141.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  89597.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3003.6396484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  597.1572875976562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5286.099609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6006765.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6405241.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452756.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  804/6933] Loss: -896.6926 [iq: 7.2886,ans: 7.1529,interp: 8.4083,fusion: -919.5424]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  165662.1875    \n",
      "module.ans_embedding.weight  dot:  858534.75    \n",
      "module.lstm.weight_ih_l0  dot:  1389973.0    \n",
      "module.lstm.weight_hh_l0  dot:  690349.375    \n",
      "module.lstm.bias_ih_l0  dot:  58978.25390625    \n",
      "module.lstm.bias_hh_l0  dot:  58978.25390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19367282.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11076.8681640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  933572.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  933572.125    \n",
      "module.adapter.frcn_linear.weight  dot:  45290456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26113.228515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  439048.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  229.35244750976562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  354355.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13309544.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42969.32421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5567.2939453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  72.19294738769531    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  68061.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.0587622152088443e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4391396.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42969.32421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  15.679620742797852    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.830143928527832    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  43.575889587402344    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5479656.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6956529.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452756.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  805/6933] Loss: -907.0078 [iq: 8.2959,ans: 7.3801,interp: 7.5907,fusion: -930.2745]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1515360.875    \n",
      "module.ans_embedding.weight  dot:  560138.125    \n",
      "module.lstm.weight_ih_l0  dot:  7270816.0    \n",
      "module.lstm.weight_hh_l0  dot:  3579064.25    \n",
      "module.lstm.bias_ih_l0  dot:  265685.53125    \n",
      "module.lstm.bias_hh_l0  dot:  265685.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12259299.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9843.484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1156425.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1156425.625    \n",
      "module.adapter.frcn_linear.weight  dot:  52689024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28710.86328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1021976.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  464.4704895019531    \n",
      "module.attflat_img.mlp.linear.weight  dot:  970992.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14218122.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45623.27734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14169.0712890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  146.35885620117188    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  229815.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.972111694063642e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4379133.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45623.27734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1370.010986328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  580.0570068359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1075.03466796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4182955.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4731266.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452757.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  806/6933] Loss: -909.4830 [iq: 10.1399,ans: 7.8617,interp: 8.3553,fusion: -935.8400]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  230377.609375    \n",
      "module.ans_embedding.weight  dot:  1033018.8125    \n",
      "module.lstm.weight_ih_l0  dot:  13495631.0    \n",
      "module.lstm.weight_hh_l0  dot:  11360120.0    \n",
      "module.lstm.bias_ih_l0  dot:  850265.875    \n",
      "module.lstm.bias_hh_l0  dot:  850265.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17603604.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16076.0595703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  927309.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  927309.75    \n",
      "module.adapter.frcn_linear.weight  dot:  41110576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28052.884765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  289357.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  139.68869018554688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  228494.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.914877642178908e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14018304.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48086.91796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7471.0966796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  56.45502471923828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  112364.90625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4360783.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48086.91796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  233.5684814453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  42.189613342285156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  416.47979736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5210715.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5806088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452757.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  807/6933] Loss: -890.3474 [iq: 12.2453,ans: 8.2259,interp: 8.2986,fusion: -919.1172]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  939960.0625    \n",
      "module.ans_embedding.weight  dot:  1035659.375    \n",
      "module.lstm.weight_ih_l0  dot:  7200113.0    \n",
      "module.lstm.weight_hh_l0  dot:  4252543.0    \n",
      "module.lstm.bias_ih_l0  dot:  356831.90625    \n",
      "module.lstm.bias_hh_l0  dot:  356831.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18114338.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20467.44140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  738881.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  738881.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  45578888.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25258.984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  563439.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  253.79788208007812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  482233.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.725290298461914e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  12482162.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  39704.91796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6094.015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  70.76197814941406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  67545.234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9878322038712213e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5320883.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  39704.91796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  256.4212646484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.05486297607422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1168.79736328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.566835632933362e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5123218.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4241621.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452757.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  808/6933] Loss: -859.7308 [iq: 12.4551,ans: 8.5918,interp: 9.9290,fusion: -890.7067]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  584823.375    \n",
      "module.ans_embedding.weight  dot:  1791263.25    \n",
      "module.lstm.weight_ih_l0  dot:  4865580.0    \n",
      "module.lstm.weight_hh_l0  dot:  3128810.25    \n",
      "module.lstm.bias_ih_l0  dot:  171250.546875    \n",
      "module.lstm.bias_hh_l0  dot:  171250.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34938616.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5019.857421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2170929.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2170929.75    \n",
      "module.adapter.frcn_linear.weight  dot:  59108032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38567.21875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  846851.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.52459716796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  575462.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15970458.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54098.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14740.5078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  236.3568115234375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  234262.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5929678.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54098.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  65.2818832397461    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.511944770812988    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  223.92015075683594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6176386.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7646959.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452758.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  809/6933] Loss: -874.8364 [iq: 11.0363,ans: 9.0727,interp: 8.9131,fusion: -903.8585]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  228004.671875    \n",
      "module.ans_embedding.weight  dot:  1499479.25    \n",
      "module.lstm.weight_ih_l0  dot:  3309457.0    \n",
      "module.lstm.weight_hh_l0  dot:  3000131.0    \n",
      "module.lstm.bias_ih_l0  dot:  84536.90625    \n",
      "module.lstm.bias_hh_l0  dot:  84536.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23212176.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  116643.2265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  594938.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  594938.375    \n",
      "module.adapter.frcn_linear.weight  dot:  46826608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30755.099609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  555257.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  227.05992126464844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  513770.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15111306.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50548.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16834.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  113.49539184570312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  208087.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5764421.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50548.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  50337.58984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10668.5625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  216005.875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5767621.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6742647.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452758.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  810/6933] Loss: -906.7665 [iq: 7.7062,ans: 7.5845,interp: 7.5628,fusion: -929.6200]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  378531.84375    \n",
      "module.ans_embedding.weight  dot:  1135455.75    \n",
      "module.lstm.weight_ih_l0  dot:  5075861.0    \n",
      "module.lstm.weight_hh_l0  dot:  4632791.0    \n",
      "module.lstm.bias_ih_l0  dot:  305717.5625    \n",
      "module.lstm.bias_hh_l0  dot:  305717.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16778642.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10109.119140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  613987.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  613987.0    \n",
      "module.adapter.frcn_linear.weight  dot:  57303392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39034.82421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  389817.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  197.66824340820312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  369373.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.822151484200731e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18338288.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62925.390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10436.177734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  95.49057006835938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  188681.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6342365.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62925.390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  160.02369689941406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.10769271850586    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  231.34982299804688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5340598.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6465885.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452758.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  811/6933] Loss: -908.4099 [iq: 7.4413,ans: 7.4079,interp: 8.0517,fusion: -931.3107]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  453483.71875    \n",
      "module.ans_embedding.weight  dot:  789424.8125    \n",
      "module.lstm.weight_ih_l0  dot:  16423895.0    \n",
      "module.lstm.weight_hh_l0  dot:  3227049.75    \n",
      "module.lstm.bias_ih_l0  dot:  1028593.125    \n",
      "module.lstm.bias_hh_l0  dot:  1028593.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15542629.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  87078.640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1146717.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1146717.75    \n",
      "module.adapter.frcn_linear.weight  dot:  68812048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45244.7734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  545306.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  274.05230712890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  469745.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18659236.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64152.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12230.4462890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  160.0072021484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  208074.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6100865.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64152.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2944.361083984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  432.5389404296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15343.6669921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.106403940160817e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5045766.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5349729.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452758.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  812/6933] Loss: -925.3470 [iq: 6.6488,ans: 6.6362,interp: 9.0504,fusion: -947.6823]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1182190.875    \n",
      "module.ans_embedding.weight  dot:  1119038.25    \n",
      "module.lstm.weight_ih_l0  dot:  15760084.0    \n",
      "module.lstm.weight_hh_l0  dot:  2236805.0    \n",
      "module.lstm.bias_ih_l0  dot:  871957.0625    \n",
      "module.lstm.bias_hh_l0  dot:  871957.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15899104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17797.630859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  459152.03125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  459152.03125    \n",
      "module.adapter.frcn_linear.weight  dot:  50893168.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34244.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  381379.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  183.39590454101562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  297164.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.412207322777249e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17095570.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58234.81640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12730.447265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  270.3879699707031    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  215426.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5363233.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58234.81640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  433.03912353515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  99.966796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1192.7530517578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.32149022227901e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5327336.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5930794.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452759.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  813/6933] Loss: -886.7535 [iq: 8.9649,ans: 8.4205,interp: 10.4457,fusion: -914.5845]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2057502.0    \n",
      "module.ans_embedding.weight  dot:  847086.25    \n",
      "module.lstm.weight_ih_l0  dot:  12814822.0    \n",
      "module.lstm.weight_hh_l0  dot:  3752754.0    \n",
      "module.lstm.bias_ih_l0  dot:  671519.0625    \n",
      "module.lstm.bias_hh_l0  dot:  671519.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22904726.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34866.2890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1833479.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1833479.5    \n",
      "module.adapter.frcn_linear.weight  dot:  43998476.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23217.0078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  443103.40625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  233.58346557617188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  320113.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13816824.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43717.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17472.240234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  157.34930419921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  217919.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5475620784854982e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3911342.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43717.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  402.8232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  112.03982543945312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  829.4833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5397777.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6520242.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452760.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  814/6933] Loss: -877.0791 [iq: 8.7656,ans: 8.3261,interp: 9.0853,fusion: -903.2560]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2329225.75    \n",
      "module.ans_embedding.weight  dot:  334249.3125    \n",
      "module.lstm.weight_ih_l0  dot:  7921683.5    \n",
      "module.lstm.weight_hh_l0  dot:  1353216.875    \n",
      "module.lstm.bias_ih_l0  dot:  355673.1875    \n",
      "module.lstm.bias_hh_l0  dot:  355673.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12225134.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10576.201171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  569801.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  569801.375    \n",
      "module.adapter.frcn_linear.weight  dot:  43012400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23964.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  704776.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  356.1416320800781    \n",
      "module.attflat_img.mlp.linear.weight  dot:  721122.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.265509000513703e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13306257.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43107.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12433.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  221.58651733398438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  266056.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.7853275241795927e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4835222.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43107.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  220.51345825195312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.31114387512207    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  701.137451171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5112772.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4604867.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452761.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  815/6933] Loss: -903.4204 [iq: 10.6292,ans: 8.1014,interp: 8.2741,fusion: -930.4251]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1055662.125    \n",
      "module.ans_embedding.weight  dot:  740882.875    \n",
      "module.lstm.weight_ih_l0  dot:  11490166.0    \n",
      "module.lstm.weight_hh_l0  dot:  3908454.25    \n",
      "module.lstm.bias_ih_l0  dot:  723354.0625    \n",
      "module.lstm.bias_hh_l0  dot:  723354.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10668924.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8807.7265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  371241.71875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  371241.71875    \n",
      "module.adapter.frcn_linear.weight  dot:  44983252.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26348.431640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  455672.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  261.4064636230469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  409907.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4917986845830455e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14833082.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47639.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11002.86328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  128.7147979736328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  185511.203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.980925728479633e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5136414.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47639.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  15.030287742614746    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.9306029081344604    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  70.17512512207031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4258046.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3938605.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452761.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  816/6933] Loss: -896.2711 [iq: 13.1168,ans: 8.7643,interp: 10.7264,fusion: -928.8786]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  472844.03125    \n",
      "module.ans_embedding.weight  dot:  1289843.5    \n",
      "module.lstm.weight_ih_l0  dot:  3081737.0    \n",
      "module.lstm.weight_hh_l0  dot:  1638816.875    \n",
      "module.lstm.bias_ih_l0  dot:  138169.515625    \n",
      "module.lstm.bias_hh_l0  dot:  138169.515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  65182360.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  123577.453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5200730.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5200730.5    \n",
      "module.adapter.frcn_linear.weight  dot:  47282312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28282.4921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  463287.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  226.79698181152344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  393051.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14022616.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44864.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6756.3857421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  99.48527526855469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  110775.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5326326.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44864.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3689.04296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  519.357666015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  22967.44140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.893241119432787e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  9086909.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  13272218.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452761.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  817/6933] Loss: -854.9387 [iq: 11.0845,ans: 7.9338,interp: 8.1284,fusion: -882.0855]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  250103.8125    \n",
      "module.ans_embedding.weight  dot:  630125.4375    \n",
      "module.lstm.weight_ih_l0  dot:  6727947.5    \n",
      "module.lstm.weight_hh_l0  dot:  5806870.0    \n",
      "module.lstm.bias_ih_l0  dot:  419064.6875    \n",
      "module.lstm.bias_hh_l0  dot:  419064.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15753639.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24947.6171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  464494.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  464494.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  36011144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19886.259765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  437791.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  221.92227172851562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  356162.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  10729760.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  33969.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3941.3994140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.168628692626953    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  42291.5703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.5278980564944504e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4173219.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  33969.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1277.67919921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  225.06895446777344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7180.3125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5119707.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4545498.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452762.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  818/6933] Loss: -888.4149 [iq: 11.0705,ans: 7.9645,interp: 7.8039,fusion: -915.2538]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1232328.25    \n",
      "module.ans_embedding.weight  dot:  1369700.0    \n",
      "module.lstm.weight_ih_l0  dot:  20926698.0    \n",
      "module.lstm.weight_hh_l0  dot:  3366135.75    \n",
      "module.lstm.bias_ih_l0  dot:  1194444.75    \n",
      "module.lstm.bias_hh_l0  dot:  1194444.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18791062.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20469.98046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  408843.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  408843.375    \n",
      "module.adapter.frcn_linear.weight  dot:  44851008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31573.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  230787.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  119.80812072753906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  162481.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15348280.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52021.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8464.8603515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  114.66455078125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  180392.65625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5867607.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52021.8203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  198.93605041503906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.269975662231445    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  589.4302978515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4856255.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4578787.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452762.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  819/6933] Loss: -884.8803 [iq: 10.5472,ans: 8.4608,interp: 8.3990,fusion: -912.2873]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  48173916.0    \n",
      "module.ans_embedding.weight  dot:  1065025.25    \n",
      "module.lstm.weight_ih_l0  dot:  333612096.0    \n",
      "module.lstm.weight_hh_l0  dot:  32480724.0    \n",
      "module.lstm.bias_ih_l0  dot:  23115914.0    \n",
      "module.lstm.bias_hh_l0  dot:  23115914.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17135482.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19214.7734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  809635.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  809635.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  61889568.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40981.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  744296.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  393.149169921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  604255.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18822396.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65935.9765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19079.794921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  292.42620849609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  208524.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5395575.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65935.9765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  675.4869384765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.42022705078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5437.78759765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4871708.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5732570.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452762.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  820/6933] Loss: -885.4894 [iq: 9.2441,ans: 8.7772,interp: 8.9086,fusion: -912.4193]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  445253.625    \n",
      "module.ans_embedding.weight  dot:  2118472.25    \n",
      "module.lstm.weight_ih_l0  dot:  1588608.375    \n",
      "module.lstm.weight_hh_l0  dot:  924744.875    \n",
      "module.lstm.bias_ih_l0  dot:  46237.34375    \n",
      "module.lstm.bias_hh_l0  dot:  46237.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32490228.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  43122.46875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1723481.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1723481.5    \n",
      "module.adapter.frcn_linear.weight  dot:  52586688.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32467.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  543669.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  248.48428344726562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  398921.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5547706172801554e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17381572.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56280.59375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5051.318359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  98.37865447998047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  101325.5546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1951328815484885e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6195787.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56280.59375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  250.4201202392578    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  55.035491943359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1404.6142578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.391018798566293e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7149273.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10207974.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452763.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  821/6933] Loss: -885.1281 [iq: 8.1352,ans: 8.3605,interp: 10.0466,fusion: -911.6703]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  511239.65625    \n",
      "module.ans_embedding.weight  dot:  692723.625    \n",
      "module.lstm.weight_ih_l0  dot:  7550088.0    \n",
      "module.lstm.weight_hh_l0  dot:  1058581.25    \n",
      "module.lstm.bias_ih_l0  dot:  464544.21875    \n",
      "module.lstm.bias_hh_l0  dot:  464544.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25610356.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31378.818359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2075862.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2075862.25    \n",
      "module.adapter.frcn_linear.weight  dot:  40890476.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25506.359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  320396.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  143.9146728515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  259843.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14091684.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45664.05859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7999.47509765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  39.56112289428711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  79703.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1690545420606213e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3852866.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45664.05859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9258.130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1867.153564453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  22826.3046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5457852.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6398881.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452763.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  822/6933] Loss: -893.8820 [iq: 7.0368,ans: 6.4406,interp: 7.2100,fusion: -914.5694]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  341711.625    \n",
      "module.ans_embedding.weight  dot:  746353.75    \n",
      "module.lstm.weight_ih_l0  dot:  3522876.75    \n",
      "module.lstm.weight_hh_l0  dot:  2236440.75    \n",
      "module.lstm.bias_ih_l0  dot:  188437.78125    \n",
      "module.lstm.bias_hh_l0  dot:  188437.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19732140.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13621.77734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1894106.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1894106.75    \n",
      "module.adapter.frcn_linear.weight  dot:  51217456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32501.38671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  375043.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  201.90951538085938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  274169.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0463630789890885e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18060440.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58144.6484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6732.3515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  96.75053405761719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  154770.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5534257.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58144.6484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  34066.84765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4031.133056640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35850.9296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5435733.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8414667.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452763.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  823/6933] Loss: -897.6642 [iq: 7.9001,ans: 7.8138,interp: 7.7589,fusion: -921.1370]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  339368.125    \n",
      "module.ans_embedding.weight  dot:  579162.0    \n",
      "module.lstm.weight_ih_l0  dot:  3211265.0    \n",
      "module.lstm.weight_hh_l0  dot:  1850549.5    \n",
      "module.lstm.bias_ih_l0  dot:  197044.03125    \n",
      "module.lstm.bias_hh_l0  dot:  197044.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24760468.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  185415.75    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1690928.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1690928.875    \n",
      "module.adapter.frcn_linear.weight  dot:  66744544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39650.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  402719.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  195.66639709472656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  311897.28125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20349814.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61981.7265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8793.841796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  169.70257568359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  229378.6875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2789769243681803e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5878970.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61981.7265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1941.973388671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  361.83837890625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3730.611328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.4141578453272814e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5527088.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5977862.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452764.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  824/6933] Loss: -893.2446 [iq: 8.9249,ans: 7.8995,interp: 8.0107,fusion: -918.0798]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  441467.28125    \n",
      "module.ans_embedding.weight  dot:  656680.0625    \n",
      "module.lstm.weight_ih_l0  dot:  10080684.0    \n",
      "module.lstm.weight_hh_l0  dot:  3635148.0    \n",
      "module.lstm.bias_ih_l0  dot:  642553.25    \n",
      "module.lstm.bias_hh_l0  dot:  642553.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17532420.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28424.96875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1079776.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1079776.75    \n",
      "module.adapter.frcn_linear.weight  dot:  73008872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44282.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1497722.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  619.506103515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1451732.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20299132.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63461.52734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8233.111328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  107.24748229980469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  202122.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.190056301922596e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6457308.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63461.52734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  441.27978515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  100.91371154785156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1768.31689453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6196698.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7161816.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452764.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  825/6933] Loss: -936.2097 [iq: 9.8207,ans: 7.7015,interp: 8.1433,fusion: -961.8752]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  469252.09375    \n",
      "module.ans_embedding.weight  dot:  445930.3125    \n",
      "module.lstm.weight_ih_l0  dot:  2257184.75    \n",
      "module.lstm.weight_hh_l0  dot:  798856.75    \n",
      "module.lstm.bias_ih_l0  dot:  82729.75    \n",
      "module.lstm.bias_hh_l0  dot:  82729.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14525738.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20291.1171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1204083.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1204083.375    \n",
      "module.adapter.frcn_linear.weight  dot:  54973928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33186.9140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  616494.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  252.74981689453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  430061.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17464274.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53938.7265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6325.2939453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  122.54891967773438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  171613.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4457374.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53938.7265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3252.546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  861.2291870117188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11218.8037109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6544662.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6960784.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452764.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  826/6933] Loss: -911.5315 [iq: 9.2363,ans: 7.2034,interp: 7.6757,fusion: -935.6469]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7151913.0    \n",
      "module.ans_embedding.weight  dot:  544158.6875    \n",
      "module.lstm.weight_ih_l0  dot:  40277136.0    \n",
      "module.lstm.weight_hh_l0  dot:  5264954.0    \n",
      "module.lstm.bias_ih_l0  dot:  1338406.0    \n",
      "module.lstm.bias_hh_l0  dot:  1338406.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9001131.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5529.28955078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  362455.21875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  362455.21875    \n",
      "module.adapter.frcn_linear.weight  dot:  73324264.0    \n",
      "module.adapter.frcn_linear.bias  dot:  43043.61328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  791106.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  430.8248291015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  599923.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22561662.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69882.40625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10783.2646484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  176.16297912597656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  175983.515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2284893930191174e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5915365.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69882.40625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  134.94703674316406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.36274528503418    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  261.12799072265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4621746.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4212330.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452765.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  827/6933] Loss: -853.4783 [iq: 11.5276,ans: 9.3464,interp: 9.4738,fusion: -883.8261]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1768862.0    \n",
      "module.ans_embedding.weight  dot:  682418.25    \n",
      "module.lstm.weight_ih_l0  dot:  17442196.0    \n",
      "module.lstm.weight_hh_l0  dot:  6814269.5    \n",
      "module.lstm.bias_ih_l0  dot:  1017424.875    \n",
      "module.lstm.bias_hh_l0  dot:  1017424.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10943400.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7515.431640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  263198.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  263198.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  35780040.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22513.37890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  357556.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  230.65423583984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  296060.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  12584075.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40659.47265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12202.0732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  160.6318359375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  189305.671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.326036666199798e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4199841.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40659.47265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  883.832275390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  170.88772583007812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4417.98876953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4154542.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4089334.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452765.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  828/6933] Loss: -906.9736 [iq: 9.3994,ans: 8.7032,interp: 9.3180,fusion: -934.3942]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  364677.28125    \n",
      "module.ans_embedding.weight  dot:  564018.375    \n",
      "module.lstm.weight_ih_l0  dot:  6234297.0    \n",
      "module.lstm.weight_hh_l0  dot:  3367382.5    \n",
      "module.lstm.bias_ih_l0  dot:  368240.28125    \n",
      "module.lstm.bias_hh_l0  dot:  368240.28125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12763356.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23756.630859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  497738.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  497738.25    \n",
      "module.adapter.frcn_linear.weight  dot:  90351560.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52473.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1631438.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  649.964599609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1623016.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24780268.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71208.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4936.4375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  53.581886291503906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  81946.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.123389877961017e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6429745.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71208.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12439.1865234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2183.84716796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  22096.095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4822190.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5995886.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452765.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  829/6933] Loss: -883.5026 [iq: 8.5519,ans: 8.7938,interp: 9.0350,fusion: -909.8832]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  268395.46875    \n",
      "module.ans_embedding.weight  dot:  964682.375    \n",
      "module.lstm.weight_ih_l0  dot:  1433172.625    \n",
      "module.lstm.weight_hh_l0  dot:  571823.5    \n",
      "module.lstm.bias_ih_l0  dot:  63652.3046875    \n",
      "module.lstm.bias_hh_l0  dot:  63652.3046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23044532.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1578.4703369140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1220660.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1220660.5    \n",
      "module.adapter.frcn_linear.weight  dot:  43116544.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25078.39453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  474377.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  180.53810119628906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  462595.15625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.482977379258955e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15579606.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47987.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6575.43212890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  95.1073226928711    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  157787.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.4489889811957255e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5379518.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47987.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4.9611945152282715    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.2493765354156494    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11.401812553405762    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.415007414162233e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5599324.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5823787.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452766.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  830/6933] Loss: -887.6447 [iq: 9.1633,ans: 8.1059,interp: 10.1498,fusion: -915.0637]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  616446.5    \n",
      "module.ans_embedding.weight  dot:  937640.625    \n",
      "module.lstm.weight_ih_l0  dot:  6800416.0    \n",
      "module.lstm.weight_hh_l0  dot:  3346374.25    \n",
      "module.lstm.bias_ih_l0  dot:  307146.125    \n",
      "module.lstm.bias_hh_l0  dot:  307146.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26976372.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31627.552734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2717599.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2717599.5    \n",
      "module.adapter.frcn_linear.weight  dot:  45203228.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28637.6171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  373972.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  167.18545532226562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  293338.28125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  16435066.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53093.37109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6964.9013671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  108.23467254638672    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  126854.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.571223482547794e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5038374.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53093.37109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  976.892578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  198.5476837158203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5549.53271484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6240822.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9586738.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452766.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  831/6933] Loss: -868.9250 [iq: 8.1090,ans: 7.8010,interp: 9.7892,fusion: -894.6243]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  505482.1875    \n",
      "module.ans_embedding.weight  dot:  1145407.75    \n",
      "module.lstm.weight_ih_l0  dot:  5608708.5    \n",
      "module.lstm.weight_hh_l0  dot:  868601.0    \n",
      "module.lstm.bias_ih_l0  dot:  361013.875    \n",
      "module.lstm.bias_hh_l0  dot:  361013.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19841856.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7847.00048828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  656273.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  656273.875    \n",
      "module.adapter.frcn_linear.weight  dot:  73643280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  47583.609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  847839.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  384.7190246582031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  783569.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.165951056871563e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22302696.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69889.2109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10398.37890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  115.84587860107422    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  186696.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.049529126059497e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5278560.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69889.2109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  109.06649780273438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  25.83208465576172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  527.9335327148438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.4016344468691386e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6609366.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7091751.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452767.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  832/6933] Loss: -917.1546 [iq: 8.4222,ans: 8.2680,interp: 9.0696,fusion: -942.9144]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  395962.65625    \n",
      "module.ans_embedding.weight  dot:  1457709.25    \n",
      "module.lstm.weight_ih_l0  dot:  5182517.0    \n",
      "module.lstm.weight_hh_l0  dot:  2455989.5    \n",
      "module.lstm.bias_ih_l0  dot:  243717.03125    \n",
      "module.lstm.bias_hh_l0  dot:  243717.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20227150.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13065.90234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  610201.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  610201.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  40111136.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23037.74609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  360508.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  167.56585693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  258951.234375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13664458.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40836.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5878.83203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  71.90056610107422    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  85566.0234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.293401272865594e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4765787.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40836.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  271.5107421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.74070358276367    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  622.8514404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.812950369474493e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5040406.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5498986.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452767.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  833/6933] Loss: -901.9053 [iq: 9.9271,ans: 8.3408,interp: 8.0706,fusion: -928.2438]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  330301.375    \n",
      "module.ans_embedding.weight  dot:  529220.125    \n",
      "module.lstm.weight_ih_l0  dot:  6639307.5    \n",
      "module.lstm.weight_hh_l0  dot:  2014378.875    \n",
      "module.lstm.bias_ih_l0  dot:  442097.5625    \n",
      "module.lstm.bias_hh_l0  dot:  442097.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10303362.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2815.0634765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  374494.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  374494.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  62704600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39195.40625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  768466.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  349.8970642089844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  581248.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7408297026122455e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17170980.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54428.16796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16441.73828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  112.54832458496094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  343309.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3655957193113863e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5496967.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54428.16796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  76.40260314941406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  10.859786987304688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  225.7142791748047    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4818349.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4316846.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452768.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  834/6933] Loss: -889.7587 [iq: 10.4917,ans: 8.6425,interp: 9.1982,fusion: -918.0912]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  119652.234375    \n",
      "module.ans_embedding.weight  dot:  961796.8125    \n",
      "module.lstm.weight_ih_l0  dot:  2865152.25    \n",
      "module.lstm.weight_hh_l0  dot:  1023202.9375    \n",
      "module.lstm.bias_ih_l0  dot:  176154.703125    \n",
      "module.lstm.bias_hh_l0  dot:  176154.703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20256860.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  71071.6328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  629449.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  629449.375    \n",
      "module.adapter.frcn_linear.weight  dot:  54240080.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35778.7890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  440813.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  241.89205932617188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  329689.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18413860.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57669.6015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6212.9326171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.48282241821289    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  117562.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.6384903978614602e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5178717.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57669.6015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5026.791015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  852.256591796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  16501.318359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6524820.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6470262.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452768.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  835/6933] Loss: -876.9203 [iq: 11.3727,ans: 9.1526,interp: 10.3746,fusion: -907.8202]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1051392.0    \n",
      "module.ans_embedding.weight  dot:  1116976.25    \n",
      "module.lstm.weight_ih_l0  dot:  9375734.0    \n",
      "module.lstm.weight_hh_l0  dot:  1840178.75    \n",
      "module.lstm.bias_ih_l0  dot:  449707.5625    \n",
      "module.lstm.bias_hh_l0  dot:  449707.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24402062.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35613.4453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1548309.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1548309.5    \n",
      "module.adapter.frcn_linear.weight  dot:  68093176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44162.8359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1295318.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  686.24951171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1166034.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.366907084360719e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20146312.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66115.84375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5751.7724609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  99.34754943847656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  83669.859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5434170.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66115.84375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  61153.484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7629.6328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  53079.8125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6508360.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7901257.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452769.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  836/6933] Loss: -902.4649 [iq: 10.5236,ans: 9.3142,interp: 9.2781,fusion: -931.5807]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  825396.375    \n",
      "module.ans_embedding.weight  dot:  926729.375    \n",
      "module.lstm.weight_ih_l0  dot:  20243306.0    \n",
      "module.lstm.weight_hh_l0  dot:  14352210.0    \n",
      "module.lstm.bias_ih_l0  dot:  1283225.125    \n",
      "module.lstm.bias_hh_l0  dot:  1283225.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20958118.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20319.869140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1541645.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1541645.375    \n",
      "module.adapter.frcn_linear.weight  dot:  55470184.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37976.1484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  350511.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  200.6915283203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  276210.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20979072.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68395.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11427.0068359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  185.6575469970703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  167476.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.7756819892820204e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7858956.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68395.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2645.676513671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  624.7010498046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8614.4677734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5392454.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6223336.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452769.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  837/6933] Loss: -906.8002 [iq: 8.0291,ans: 8.0643,interp: 8.3736,fusion: -931.2671]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  154968.359375    \n",
      "module.ans_embedding.weight  dot:  644236.9375    \n",
      "module.lstm.weight_ih_l0  dot:  1039113.3125    \n",
      "module.lstm.weight_hh_l0  dot:  685267.125    \n",
      "module.lstm.bias_ih_l0  dot:  64455.3203125    \n",
      "module.lstm.bias_hh_l0  dot:  64455.3203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19643826.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33179.328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1523067.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1523067.625    \n",
      "module.adapter.frcn_linear.weight  dot:  50031072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35525.87890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  446409.53125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  260.6441345214844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  290995.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.165951056871563e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16511943.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54262.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4077.281005859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  55.47599792480469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  56369.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1559323215569748e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6747902.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54262.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1473.3367919921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  375.5829162597656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6477.3017578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6687364.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8234034.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452769.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  838/6933] Loss: -927.6264 [iq: 6.1624,ans: 6.4894,interp: 7.7374,fusion: -948.0156]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  343249.96875    \n",
      "module.ans_embedding.weight  dot:  1361709.75    \n",
      "module.lstm.weight_ih_l0  dot:  2910141.5    \n",
      "module.lstm.weight_hh_l0  dot:  782141.875    \n",
      "module.lstm.bias_ih_l0  dot:  161519.390625    \n",
      "module.lstm.bias_hh_l0  dot:  161519.390625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20603638.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10069.5869140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1062324.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1062324.625    \n",
      "module.adapter.frcn_linear.weight  dot:  69810896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  47378.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  506415.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  164.40069580078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  504543.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  23465756.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70620.28125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9091.77734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  85.96527862548828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  143050.828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5934341.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70620.28125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  205.72039794921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  58.21530532836914    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  793.9651489257812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6585117.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7821451.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452769.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  839/6933] Loss: -882.1136 [iq: 9.8915,ans: 9.1307,interp: 9.3406,fusion: -910.4764]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1554577.875    \n",
      "module.ans_embedding.weight  dot:  642668.25    \n",
      "module.lstm.weight_ih_l0  dot:  25420674.0    \n",
      "module.lstm.weight_hh_l0  dot:  13087989.0    \n",
      "module.lstm.bias_ih_l0  dot:  1701146.25    \n",
      "module.lstm.bias_hh_l0  dot:  1701146.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16274080.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9852.48828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  676639.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  676639.75    \n",
      "module.adapter.frcn_linear.weight  dot:  63946672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42582.6875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  580480.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  242.38211059570312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  532100.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5067947717616335e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22365284.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68996.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14518.498046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  183.8910675048828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  281933.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2619807421287987e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7138491.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68996.0234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2842.1103515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  442.2131652832031    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13331.8232421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5981402.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5657504.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452770.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  840/6933] Loss: -913.6280 [iq: 9.5647,ans: 9.2762,interp: 9.6693,fusion: -942.1382]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1197059.75    \n",
      "module.ans_embedding.weight  dot:  723238.5    \n",
      "module.lstm.weight_ih_l0  dot:  25756840.0    \n",
      "module.lstm.weight_hh_l0  dot:  8012417.0    \n",
      "module.lstm.bias_ih_l0  dot:  1655083.25    \n",
      "module.lstm.bias_hh_l0  dot:  1655083.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13282228.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38371.01171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  664754.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  664754.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  90762472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57349.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1995948.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  715.0805053710938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2595400.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.381903171539307e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  25609388.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  76743.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12302.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  71.54325866699219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  202829.671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.130118552187923e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6652757.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  76743.1953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3121.9990234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  784.631103515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3442.85986328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4977715.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4374762.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452771.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  841/6933] Loss: -904.0088 [iq: 7.0057,ans: 7.1653,interp: 7.0725,fusion: -925.2523]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  550744.75    \n",
      "module.ans_embedding.weight  dot:  900764.625    \n",
      "module.lstm.weight_ih_l0  dot:  2915099.0    \n",
      "module.lstm.weight_hh_l0  dot:  1908330.75    \n",
      "module.lstm.bias_ih_l0  dot:  98028.9375    \n",
      "module.lstm.bias_hh_l0  dot:  98028.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16750877.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38760.6796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1145669.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1145669.125    \n",
      "module.adapter.frcn_linear.weight  dot:  54868248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31837.08203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  903918.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  394.8460693359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  699348.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18160644.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54441.4375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7287.328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  86.22321319580078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  102555.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2704646223937743e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7089364.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54441.4375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7329.1171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1462.3486328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  16012.1083984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.568967592102126e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5841971.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5906867.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452771.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  842/6933] Loss: -867.5192 [iq: 9.7693,ans: 8.9389,interp: 9.2224,fusion: -895.4498]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  30076736.0    \n",
      "module.ans_embedding.weight  dot:  1434893.5    \n",
      "module.lstm.weight_ih_l0  dot:  339559264.0    \n",
      "module.lstm.weight_hh_l0  dot:  26378480.0    \n",
      "module.lstm.bias_ih_l0  dot:  20867876.0    \n",
      "module.lstm.bias_hh_l0  dot:  20867876.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  41596236.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5354.1357421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4036444.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4036444.5    \n",
      "module.adapter.frcn_linear.weight  dot:  47111384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27368.568359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1075942.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  515.0733642578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  858197.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18020684.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54039.62109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  23718.37109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  174.78189086914062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  126788.421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.277755452785641e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6080462.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54039.62109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.2739949226379395    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.9525144100189209    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  21.008207321166992    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6928618.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  14260924.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452771.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  843/6933] Loss: -879.9064 [iq: 10.1154,ans: 9.1928,interp: 8.8305,fusion: -908.0451]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2439505.75    \n",
      "module.ans_embedding.weight  dot:  511397.75    \n",
      "module.lstm.weight_ih_l0  dot:  30118864.0    \n",
      "module.lstm.weight_hh_l0  dot:  7083127.0    \n",
      "module.lstm.bias_ih_l0  dot:  1564634.0    \n",
      "module.lstm.bias_hh_l0  dot:  1564634.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7627108.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11854.6640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  279006.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  279006.75    \n",
      "module.adapter.frcn_linear.weight  dot:  52763600.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31228.625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  763554.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  394.23370361328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  727762.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18040638.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55125.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22672.564453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  249.4573516845703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  324939.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6021783.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55125.7890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  179.70111083984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  22.80489730834961    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  308.145263671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.8689051962137455e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4439365.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3822055.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452772.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  844/6933] Loss: -940.0753 [iq: 8.8646,ans: 8.1773,interp: 7.6285,fusion: -964.7457]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1445502.375    \n",
      "module.ans_embedding.weight  dot:  617399.1875    \n",
      "module.lstm.weight_ih_l0  dot:  53744528.0    \n",
      "module.lstm.weight_hh_l0  dot:  41958220.0    \n",
      "module.lstm.bias_ih_l0  dot:  3624476.5    \n",
      "module.lstm.bias_hh_l0  dot:  3624476.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11556036.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31400.3203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  980734.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  980734.375    \n",
      "module.adapter.frcn_linear.weight  dot:  51057460.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35101.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  235925.65625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  132.3831787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  233068.15625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.552713678800501e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18784276.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59982.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8219.625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  79.1885986328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  132911.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5288605936802924e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5913865.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59982.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  377.6844482421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.49144744873047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2108.37451171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4688970.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5874818.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452772.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  845/6933] Loss: -910.5500 [iq: 7.2611,ans: 7.0875,interp: 6.8768,fusion: -931.7755]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  896362.625    \n",
      "module.ans_embedding.weight  dot:  1495329.25    \n",
      "module.lstm.weight_ih_l0  dot:  8446566.0    \n",
      "module.lstm.weight_hh_l0  dot:  2434629.75    \n",
      "module.lstm.bias_ih_l0  dot:  470372.9375    \n",
      "module.lstm.bias_hh_l0  dot:  470372.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26435248.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22810.392578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3270136.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3270136.0    \n",
      "module.adapter.frcn_linear.weight  dot:  60420984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34978.7890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  655904.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.42486572265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  608436.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19217876.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56655.8828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6535.201171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  42.33790588378906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  111005.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.90333446173463e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5303065.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56655.8828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  991.4258422851562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  256.58349609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3650.87158203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6026658.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9473994.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452772.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  846/6933] Loss: -879.5284 [iq: 7.8153,ans: 7.5592,interp: 7.6654,fusion: -902.5684]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  160801.953125    \n",
      "module.ans_embedding.weight  dot:  376718.0    \n",
      "module.lstm.weight_ih_l0  dot:  3003152.0    \n",
      "module.lstm.weight_hh_l0  dot:  2398965.0    \n",
      "module.lstm.bias_ih_l0  dot:  229493.15625    \n",
      "module.lstm.bias_hh_l0  dot:  229493.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7485311.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  962.2611083984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  364862.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  364862.5    \n",
      "module.adapter.frcn_linear.weight  dot:  62132440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38555.4140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  493250.03125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  217.88235473632812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  501207.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  21494272.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65475.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6074.35205078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  86.91227722167969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  113821.1015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.207443448147387e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6495595.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65475.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1.30495023727417    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.268582820892334    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4.916026592254639    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4240103.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4548234.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452773.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  847/6933] Loss: -928.2073 [iq: 7.4343,ans: 7.5141,interp: 8.7527,fusion: -951.9084]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  287311.78125    \n",
      "module.ans_embedding.weight  dot:  1370479.75    \n",
      "module.lstm.weight_ih_l0  dot:  5907480.5    \n",
      "module.lstm.weight_hh_l0  dot:  4622112.0    \n",
      "module.lstm.bias_ih_l0  dot:  314114.4375    \n",
      "module.lstm.bias_hh_l0  dot:  314114.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19461740.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24327.640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  616704.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  616704.25    \n",
      "module.adapter.frcn_linear.weight  dot:  62119000.0    \n",
      "module.adapter.frcn_linear.bias  dot:  43282.57421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  559548.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  257.706787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  474671.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.913989298278466e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19123540.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61276.75390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6293.88427734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  65.61871337890625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  68222.3203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.004086117172847e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6718519.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61276.75390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  522.9537963867188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  119.75595092773438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3152.06787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5311520.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5575055.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452773.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  848/6933] Loss: -881.8984 [iq: 8.0073,ans: 7.8866,interp: 8.4387,fusion: -906.2310]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1078502.75    \n",
      "module.ans_embedding.weight  dot:  872695.25    \n",
      "module.lstm.weight_ih_l0  dot:  44581056.0    \n",
      "module.lstm.weight_hh_l0  dot:  37968288.0    \n",
      "module.lstm.bias_ih_l0  dot:  3015437.5    \n",
      "module.lstm.bias_hh_l0  dot:  3015437.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10824716.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28935.66796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  247523.28125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  247523.28125    \n",
      "module.adapter.frcn_linear.weight  dot:  41006916.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25765.236328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  533753.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  249.52926635742188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  551076.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4190391084412113e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15717703.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  49108.55078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17729.4921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  100.44804382324219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  204895.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.991829089500243e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5407067.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  49108.55078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3224.47021484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  494.83837890625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  23588.51953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4301714.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3277008.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452774.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  849/6933] Loss: -912.9248 [iq: 8.4407,ans: 7.7954,interp: 8.3510,fusion: -937.5120]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2404064.75    \n",
      "module.ans_embedding.weight  dot:  748722.125    \n",
      "module.lstm.weight_ih_l0  dot:  25104912.0    \n",
      "module.lstm.weight_hh_l0  dot:  4165858.5    \n",
      "module.lstm.bias_ih_l0  dot:  1354961.75    \n",
      "module.lstm.bias_hh_l0  dot:  1354961.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14453203.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31688.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  464517.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  464517.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  47557220.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25054.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  460312.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  265.84619140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  374186.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  16219020.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48074.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10240.59375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  91.04811096191406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172130.765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5283962.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48074.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  945.2283325195312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  179.57334899902344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2820.706787109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.345346139520643e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5613748.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5659006.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452774.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  850/6933] Loss: -926.6420 [iq: 9.1476,ans: 8.5093,interp: 8.1454,fusion: -952.4443]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2310318.75    \n",
      "module.ans_embedding.weight  dot:  667514.5    \n",
      "module.lstm.weight_ih_l0  dot:  21303026.0    \n",
      "module.lstm.weight_hh_l0  dot:  3219364.5    \n",
      "module.lstm.bias_ih_l0  dot:  1336470.75    \n",
      "module.lstm.bias_hh_l0  dot:  1336470.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9817884.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34577.875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  241766.59375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  241766.59375    \n",
      "module.adapter.frcn_linear.weight  dot:  43647396.0    \n",
      "module.adapter.frcn_linear.bias  dot:  24110.943359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  475717.09375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  241.58628845214844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  341608.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14824796.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43599.5703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9081.48828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  132.03746032714844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  150399.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2535252835732535e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4906453.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43599.5703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  127.96448516845703    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.3437442779541    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  910.8656616210938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.3598012433012627e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4222433.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3613803.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452775.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  851/6933] Loss: -887.6667 [iq: 9.5669,ans: 8.1645,interp: 8.4690,fusion: -913.8672]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  182292.59375    \n",
      "module.ans_embedding.weight  dot:  863296.1875    \n",
      "module.lstm.weight_ih_l0  dot:  2528977.5    \n",
      "module.lstm.weight_hh_l0  dot:  2427308.5    \n",
      "module.lstm.bias_ih_l0  dot:  147830.6875    \n",
      "module.lstm.bias_hh_l0  dot:  147830.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14135817.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  64489.2890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  580637.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  580637.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  60133272.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36685.5    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  949964.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  426.3597106933594    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1247890.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17209784.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52526.04296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7748.2255859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.744361877441406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  88043.84375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.412026217120001e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6778874.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52526.04296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9016.4599609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1524.9627685546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11744.3134765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4371523.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4497615.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452775.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  852/6933] Loss: -852.8799 [iq: 9.9549,ans: 9.0368,interp: 9.0836,fusion: -880.9553]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  420363.53125    \n",
      "module.ans_embedding.weight  dot:  1021341.4375    \n",
      "module.lstm.weight_ih_l0  dot:  5740492.0    \n",
      "module.lstm.weight_hh_l0  dot:  2837724.5    \n",
      "module.lstm.bias_ih_l0  dot:  301242.5625    \n",
      "module.lstm.bias_hh_l0  dot:  301242.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19574060.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  47043.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  775823.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  775823.5    \n",
      "module.adapter.frcn_linear.weight  dot:  42088532.0    \n",
      "module.adapter.frcn_linear.bias  dot:  24699.328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  596660.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.24005126953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  456120.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.725290298461914e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13746807.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41323.13671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7719.27197265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  57.01032257080078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  108062.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.502567207964603e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5063121.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41323.13671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  5976.7861328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1190.470947265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  29102.296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5398938.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5631088.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452775.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  853/6933] Loss: -892.4485 [iq: 8.6546,ans: 7.9093,interp: 8.4204,fusion: -917.4327]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  216853.390625    \n",
      "module.ans_embedding.weight  dot:  1121133.5    \n",
      "module.lstm.weight_ih_l0  dot:  7010561.5    \n",
      "module.lstm.weight_hh_l0  dot:  8530946.0    \n",
      "module.lstm.bias_ih_l0  dot:  450476.375    \n",
      "module.lstm.bias_hh_l0  dot:  450476.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21879192.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18267.662109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1090907.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1090907.75    \n",
      "module.adapter.frcn_linear.weight  dot:  57454112.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32864.390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  743807.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  341.1910400390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  692530.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20183108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59852.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7766.75732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  38.45233154296875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74396.46875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7567703.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59852.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  134.50381469726562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  28.066890716552734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  357.741943359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.530065542800003e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6264212.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6952430.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452776.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  854/6933] Loss: -829.5977 [iq: 10.3812,ans: 9.8145,interp: 10.1720,fusion: -859.9654]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  616775.75    \n",
      "module.ans_embedding.weight  dot:  438849.125    \n",
      "module.lstm.weight_ih_l0  dot:  16564875.0    \n",
      "module.lstm.weight_hh_l0  dot:  4016520.5    \n",
      "module.lstm.bias_ih_l0  dot:  875146.9375    \n",
      "module.lstm.bias_hh_l0  dot:  875146.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18597860.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14377.91796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1908542.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1908542.25    \n",
      "module.adapter.frcn_linear.weight  dot:  68110632.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46828.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  503112.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  256.41729736328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  442263.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21514340.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  68813.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7873.171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  101.84913635253906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144146.234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.502567207964603e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7874423.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  68813.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  70.44660949707031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.206151962280273    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  214.87767028808594    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5993934.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8257009.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452776.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  855/6933] Loss: -939.6443 [iq: 8.1407,ans: 7.8237,interp: 9.3807,fusion: -964.9893]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  46216.515625    \n",
      "module.ans_embedding.weight  dot:  1027100.8125    \n",
      "module.lstm.weight_ih_l0  dot:  541077.375    \n",
      "module.lstm.weight_hh_l0  dot:  425796.3125    \n",
      "module.lstm.bias_ih_l0  dot:  30690.37890625    \n",
      "module.lstm.bias_hh_l0  dot:  30690.37890625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26099584.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15245.01953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1382744.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1382744.0    \n",
      "module.adapter.frcn_linear.weight  dot:  41699044.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25774.35546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  327176.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  186.2411651611328    \n",
      "module.attflat_img.mlp.linear.weight  dot:  295059.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  14216778.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42509.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2080.51220703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  16.65066909790039    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  34892.94921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4280345794759342e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4561338.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42509.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  96.8266830444336    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  36.39687728881836    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  306.3565673828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3219647598816664e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6249771.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8589818.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452776.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  856/6933] Loss: -887.6413 [iq: 7.8074,ans: 7.7755,interp: 7.9340,fusion: -911.1581]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  62186740.0    \n",
      "module.ans_embedding.weight  dot:  1079291.25    \n",
      "module.lstm.weight_ih_l0  dot:  901970368.0    \n",
      "module.lstm.weight_hh_l0  dot:  75495064.0    \n",
      "module.lstm.bias_ih_l0  dot:  52889912.0    \n",
      "module.lstm.bias_hh_l0  dot:  52889912.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17785106.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3861.137939453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1586630.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1586630.5    \n",
      "module.adapter.frcn_linear.weight  dot:  69133312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45007.3984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  645813.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  327.4154357910156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  758569.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22084536.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69316.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9868.05078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  129.8858184814453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  161081.484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9454660105111543e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6288384.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69316.1953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13.668204307556152    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.003079891204834    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15.69295883178711    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.698463840213662e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4931655.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7093495.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452777.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  857/6933] Loss: -889.7507 [iq: 8.2889,ans: 8.4858,interp: 10.4388,fusion: -916.9641]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  309333.96875    \n",
      "module.ans_embedding.weight  dot:  414577.0625    \n",
      "module.lstm.weight_ih_l0  dot:  5328948.5    \n",
      "module.lstm.weight_hh_l0  dot:  3250402.25    \n",
      "module.lstm.bias_ih_l0  dot:  421539.1875    \n",
      "module.lstm.bias_hh_l0  dot:  421539.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9627208.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7774.03857421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  379134.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  379134.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  49448488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31350.810546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  566071.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  273.58966064453125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  456469.15625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16694282.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52553.359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3303.318359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  40.147422790527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  62173.7109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5913849.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52553.359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  99.89425659179688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.54513168334961    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  471.4404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4112234.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3807774.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452777.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  858/6933] Loss: -896.3505 [iq: 8.9862,ans: 8.1300,interp: 8.5312,fusion: -921.9978]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  623044.75    \n",
      "module.ans_embedding.weight  dot:  678784.125    \n",
      "module.lstm.weight_ih_l0  dot:  16395993.0    \n",
      "module.lstm.weight_hh_l0  dot:  16325090.0    \n",
      "module.lstm.bias_ih_l0  dot:  1346262.0    \n",
      "module.lstm.bias_hh_l0  dot:  1346262.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14029787.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30386.953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  827990.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  827990.0    \n",
      "module.adapter.frcn_linear.weight  dot:  88173656.0    \n",
      "module.adapter.frcn_linear.bias  dot:  56909.45703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1218284.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  497.6011657714844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1128603.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24499692.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  75446.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8429.359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  140.17974853515625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  161351.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8543907.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  75446.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2757.856689453125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  348.81048583984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  12220.833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5390208.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5588946.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452778.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  859/6933] Loss: -873.7374 [iq: 12.4009,ans: 10.0836,interp: 11.3549,fusion: -907.5767]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  831180.9375    \n",
      "module.ans_embedding.weight  dot:  1265150.875    \n",
      "module.lstm.weight_ih_l0  dot:  13148321.0    \n",
      "module.lstm.weight_hh_l0  dot:  6820406.5    \n",
      "module.lstm.bias_ih_l0  dot:  798966.25    \n",
      "module.lstm.bias_hh_l0  dot:  798966.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24029136.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  34642.75    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1181752.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1181752.75    \n",
      "module.adapter.frcn_linear.weight  dot:  55216984.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36200.37890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  389678.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  178.71963500976562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  362916.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.186123861582018e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18862360.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  57869.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11345.2626953125    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.attflat_lang.mlp.fc.linear.bias  dot:  97.52473449707031    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  169218.5625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5067947717616335e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7147521.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  57869.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8148.0390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1675.3182373046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  40323.8515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6036047.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7032064.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452778.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  860/6933] Loss: -874.3063 [iq: 11.9053,ans: 9.3922,interp: 10.3219,fusion: -905.9257]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1017526.625    \n",
      "module.ans_embedding.weight  dot:  511292.21875    \n",
      "module.lstm.weight_ih_l0  dot:  13306070.0    \n",
      "module.lstm.weight_hh_l0  dot:  2516898.0    \n",
      "module.lstm.bias_ih_l0  dot:  753083.25    \n",
      "module.lstm.bias_hh_l0  dot:  753083.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7895009.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14182.884765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  311866.59375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  311866.59375    \n",
      "module.adapter.frcn_linear.weight  dot:  45819368.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27336.38671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  505062.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  270.4881591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  371493.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14903924.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44344.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7842.76953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  63.81000900268555    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  117731.6484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4976304.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44344.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  697.6961669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  123.30534362792969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3430.2265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.220446049250313e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4277430.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3436318.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452778.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  861/6933] Loss: -913.2510 [iq: 8.5970,ans: 7.8932,interp: 8.2699,fusion: -938.0112]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  744004.0    \n",
      "module.ans_embedding.weight  dot:  904148.625    \n",
      "module.lstm.weight_ih_l0  dot:  38887024.0    \n",
      "module.lstm.weight_hh_l0  dot:  27328140.0    \n",
      "module.lstm.bias_ih_l0  dot:  2607238.75    \n",
      "module.lstm.bias_hh_l0  dot:  2607238.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16593106.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9826.8583984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1233769.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1233769.125    \n",
      "module.adapter.frcn_linear.weight  dot:  52190428.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32444.771484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  598063.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  290.69671630859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  405452.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8828814063454047e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19999656.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60027.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5574.89990234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.85478973388672    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  82473.6640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.916387297271285e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6095912.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60027.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  123.8200912475586    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.22213363647461    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  987.606689453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5247098.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6884403.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452779.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  862/6933] Loss: -908.1505 [iq: 9.6336,ans: 9.1555,interp: 8.9586,fusion: -935.8981]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  720448.75    \n",
      "module.ans_embedding.weight  dot:  430151.0625    \n",
      "module.lstm.weight_ih_l0  dot:  11978894.0    \n",
      "module.lstm.weight_hh_l0  dot:  6045314.0    \n",
      "module.lstm.bias_ih_l0  dot:  724867.0    \n",
      "module.lstm.bias_hh_l0  dot:  724867.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10138629.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6695.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  439959.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  439959.375    \n",
      "module.adapter.frcn_linear.weight  dot:  61526056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37867.91015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  910233.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  354.7906799316406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  790223.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19391432.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56832.2421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8597.5859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.99574279785156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  158459.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4210854715202004e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7075251.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56832.2421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  67.80398559570312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.115687370300293    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  182.67477416992188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.298783551348606e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5151740.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4476151.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452779.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  863/6933] Loss: -866.5767 [iq: 8.4613,ans: 8.5263,interp: 9.8799,fusion: -893.4443]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  276355.40625    \n",
      "module.ans_embedding.weight  dot:  568622.9375    \n",
      "module.lstm.weight_ih_l0  dot:  7432136.5    \n",
      "module.lstm.weight_hh_l0  dot:  9323954.0    \n",
      "module.lstm.bias_ih_l0  dot:  733691.1875    \n",
      "module.lstm.bias_hh_l0  dot:  733691.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7830403.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1065.8994140625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  187026.515625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  187026.515625    \n",
      "module.adapter.frcn_linear.weight  dot:  44693208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26809.880859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  438260.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  210.33702087402344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  472606.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16661462.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47727.5    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9150.830078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.84942626953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  170730.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7909229654833325e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5447255.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47727.5    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1.0157194137573242    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.045714594423770905    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7.732004165649414    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4374182.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3949714.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452779.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  864/6933] Loss: -888.8154 [iq: 9.4990,ans: 9.3052,interp: 9.8221,fusion: -917.4417]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  96638.9453125    \n",
      "module.ans_embedding.weight  dot:  372197.46875    \n",
      "module.lstm.weight_ih_l0  dot:  1965200.375    \n",
      "module.lstm.weight_hh_l0  dot:  1853627.0    \n",
      "module.lstm.bias_ih_l0  dot:  151611.703125    \n",
      "module.lstm.bias_hh_l0  dot:  151611.703125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9179284.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6835.0078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  858325.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  858325.625    \n",
      "module.adapter.frcn_linear.weight  dot:  49533976.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31353.388671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  735982.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  302.8678894042969    \n",
      "module.attflat_img.mlp.linear.weight  dot:  544477.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17203754.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  51667.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5920.1796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  32.08324432373047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  80753.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3648104868480004e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5365148.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  51667.1640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  99.35905456542969    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  39.453392028808594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  67.69320678710938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4155626.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5217549.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452780.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  865/6933] Loss: -934.9653 [iq: 7.7252,ans: 7.9042,interp: 7.5422,fusion: -958.1369]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  594919.375    \n",
      "module.ans_embedding.weight  dot:  626288.875    \n",
      "module.lstm.weight_ih_l0  dot:  2713037.0    \n",
      "module.lstm.weight_hh_l0  dot:  647380.0625    \n",
      "module.lstm.bias_ih_l0  dot:  66581.203125    \n",
      "module.lstm.bias_hh_l0  dot:  66581.203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18858680.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15838.154296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1326546.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1326546.75    \n",
      "module.adapter.frcn_linear.weight  dot:  46586876.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25986.06640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  689136.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  324.77618408203125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  764781.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17248682.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  46875.44140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9935.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  81.44952392578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  212902.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4940942.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  46875.44140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  248.25857543945312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.250728607177734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1579.2655029296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.004086117172847e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6103746.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5729831.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452780.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  866/6933] Loss: -946.0249 [iq: 8.1223,ans: 7.7876,interp: 7.8516,fusion: -969.7864]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1162252.125    \n",
      "module.ans_embedding.weight  dot:  955307.0    \n",
      "module.lstm.weight_ih_l0  dot:  19207420.0    \n",
      "module.lstm.weight_hh_l0  dot:  8517253.0    \n",
      "module.lstm.bias_ih_l0  dot:  1155363.125    \n",
      "module.lstm.bias_hh_l0  dot:  1155363.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13005747.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14852.9560546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  368735.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  368735.75    \n",
      "module.adapter.frcn_linear.weight  dot:  73876048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  47115.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  832115.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  395.44256591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  675799.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22276620.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63450.24609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11335.166015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  58.08140563964844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  162113.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5475620784854982e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6204991.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63450.24609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  276.951171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  38.676849365234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  824.011962890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4943303.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4378609.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452781.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  867/6933] Loss: -889.1000 [iq: 9.0040,ans: 8.1700,interp: 8.3270,fusion: -914.6010]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  399653.0    \n",
      "module.ans_embedding.weight  dot:  315927.0625    \n",
      "module.lstm.weight_ih_l0  dot:  3302468.75    \n",
      "module.lstm.weight_hh_l0  dot:  2581930.0    \n",
      "module.lstm.bias_ih_l0  dot:  175408.28125    \n",
      "module.lstm.bias_hh_l0  dot:  175408.28125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11344757.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  67280.1171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  714106.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  714106.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  61570760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36402.48828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  908800.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  331.406494140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  980219.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19521008.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53464.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6167.1376953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.70075225830078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  109961.3671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.474820679613913e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5346970.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53464.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29675.0859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5756.1787109375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  103845.109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4525421.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4415711.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452781.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  868/6933] Loss: -925.7821 [iq: 7.3626,ans: 7.3820,interp: 7.1095,fusion: -947.6362]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  480565.25    \n",
      "module.ans_embedding.weight  dot:  801194.75    \n",
      "module.lstm.weight_ih_l0  dot:  6949005.5    \n",
      "module.lstm.weight_hh_l0  dot:  2861967.0    \n",
      "module.lstm.bias_ih_l0  dot:  392989.375    \n",
      "module.lstm.bias_hh_l0  dot:  392989.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  8035318.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  46105.65625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  641567.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  641567.5    \n",
      "module.adapter.frcn_linear.weight  dot:  69537848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  44482.0546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  735729.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  248.69659423828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  605397.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  24746044.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  70216.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6059.953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  55.251678466796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  81113.6796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7439255.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  70216.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  335.39013671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.69877815246582    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2135.10302734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4354779.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4542258.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452781.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  869/6933] Loss: -896.7617 [iq: 7.7810,ans: 7.9053,interp: 8.2400,fusion: -920.6880]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  888405.0    \n",
      "module.ans_embedding.weight  dot:  752163.75    \n",
      "module.lstm.weight_ih_l0  dot:  50446400.0    \n",
      "module.lstm.weight_hh_l0  dot:  34976824.0    \n",
      "module.lstm.bias_ih_l0  dot:  2886481.0    \n",
      "module.lstm.bias_hh_l0  dot:  2886481.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18307138.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29281.568359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  786251.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  786251.375    \n",
      "module.adapter.frcn_linear.weight  dot:  67954608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  50433.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  794314.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  341.81842041015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  816963.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  26503980.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  80651.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11520.484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  65.80816650390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  135596.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.488413196668262e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  11356738.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  80651.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29277.748046875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2844.08984375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  28308.560546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.417089082333405e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7476807.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6707398.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452781.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  870/6933] Loss: -890.9438 [iq: 8.2058,ans: 8.4467,interp: 9.4892,fusion: -917.0856]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  444861.75    \n",
      "module.ans_embedding.weight  dot:  1336738.0    \n",
      "module.lstm.weight_ih_l0  dot:  9402273.0    \n",
      "module.lstm.weight_hh_l0  dot:  2949922.0    \n",
      "module.lstm.bias_ih_l0  dot:  354300.0    \n",
      "module.lstm.bias_hh_l0  dot:  354300.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21012640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  211439.578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1642212.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1642212.0    \n",
      "module.adapter.frcn_linear.weight  dot:  46369144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27162.3984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  500741.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  238.83551025390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  538506.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17272672.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  46368.5625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6277.97509765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  51.53550720214844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  84440.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3101520696400257e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4340685.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  46368.5625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10673.7880859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2366.81396484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  64745.69921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5039426.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6321090.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452782.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  871/6933] Loss: -850.3685 [iq: 8.4723,ans: 7.7445,interp: 9.1977,fusion: -875.7830]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  274212.6875    \n",
      "module.ans_embedding.weight  dot:  648020.375    \n",
      "module.lstm.weight_ih_l0  dot:  9396217.0    \n",
      "module.lstm.weight_hh_l0  dot:  4623977.0    \n",
      "module.lstm.bias_ih_l0  dot:  654909.125    \n",
      "module.lstm.bias_hh_l0  dot:  654909.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15817244.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6079.4462890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  861244.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  861244.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  68966992.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42077.109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  952977.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  426.609130859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  850724.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22396328.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63118.73828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9159.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  47.676612854003906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  91202.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.1746375245566014e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7829934.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63118.73828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  60.3631591796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.314983367919922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  186.50132751464844    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5176361.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5940976.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452783.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  872/6933] Loss: -882.0652 [iq: 9.9611,ans: 8.9158,interp: 8.9102,fusion: -909.8522]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1592248.5    \n",
      "module.ans_embedding.weight  dot:  1106219.5    \n",
      "module.lstm.weight_ih_l0  dot:  40313912.0    \n",
      "module.lstm.weight_hh_l0  dot:  38818360.0    \n",
      "module.lstm.bias_ih_l0  dot:  3454585.75    \n",
      "module.lstm.bias_hh_l0  dot:  3454585.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17584808.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  122518.78125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  809740.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  809740.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  45916848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28848.4609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  391802.09375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  216.772705078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  270497.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19610204.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56789.26171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5094.03515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  58.49102020263672    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  98079.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0520474208751693e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5874998.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56789.26171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  27953.17578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4675.138671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  111908.484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5243920.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5408580.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452783.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  873/6933] Loss: -901.6506 [iq: 10.8868,ans: 8.8025,interp: 8.9917,fusion: -930.3315]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1841258.0    \n",
      "module.ans_embedding.weight  dot:  349167.25    \n",
      "module.lstm.weight_ih_l0  dot:  80248904.0    \n",
      "module.lstm.weight_hh_l0  dot:  59766732.0    \n",
      "module.lstm.bias_ih_l0  dot:  5004917.0    \n",
      "module.lstm.bias_hh_l0  dot:  5004917.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7964692.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  36979.9765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  286515.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  286515.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  59947440.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40548.1640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  849226.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  364.3994140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  919489.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-08    \n",
      "module.attflat_img.linear_merge.weight  dot:  17578492.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50028.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  25593.94140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.66484069824219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  191699.78125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.1391778065881226e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5604729.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50028.55859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  179.24655151367188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.46400451660156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  730.7511596679688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4229815.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3022492.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452783.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  874/6933] Loss: -840.5464 [iq: 8.4132,ans: 6.8030,interp: 6.7962,fusion: -862.5588]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  307620.0    \n",
      "module.ans_embedding.weight  dot:  1026341.625    \n",
      "module.lstm.weight_ih_l0  dot:  5079346.0    \n",
      "module.lstm.weight_hh_l0  dot:  2186280.0    \n",
      "module.lstm.bias_ih_l0  dot:  349168.34375    \n",
      "module.lstm.bias_hh_l0  dot:  349168.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26724852.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3799.4248046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1661451.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1661451.875    \n",
      "module.adapter.frcn_linear.weight  dot:  40426896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23972.61328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  442164.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  211.21554565429688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  359463.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6043486539274454e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14761589.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40288.8359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5264.404296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  61.74929428100586    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  102583.890625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.73985675550648e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5728982.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40288.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  26.31993865966797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.918893575668335    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  96.3606185913086    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.792167077193881e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7311802.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5672337.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452784.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  875/6933] Loss: -883.9574 [iq: 9.3021,ans: 8.3784,interp: 8.8571,fusion: -910.4949]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1541217.5    \n",
      "module.ans_embedding.weight  dot:  609758.125    \n",
      "module.lstm.weight_ih_l0  dot:  32061548.0    \n",
      "module.lstm.weight_hh_l0  dot:  25925492.0    \n",
      "module.lstm.bias_ih_l0  dot:  2176257.75    \n",
      "module.lstm.bias_hh_l0  dot:  2176257.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11680114.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2885.161376953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  387781.46875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  387781.46875    \n",
      "module.adapter.frcn_linear.weight  dot:  61202072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37827.1875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  338991.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  156.4069061279297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  265109.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.967194738332182e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19842794.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54896.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14147.5546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  85.42436218261719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  170034.640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7391514.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54896.1484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  29.800128936767578    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.7022866010665894    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  207.93923950195312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5065230.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4608912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452784.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  876/6933] Loss: -941.8721 [iq: 8.2199,ans: 8.3445,interp: 7.6482,fusion: -966.0848]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  645603.25    \n",
      "module.ans_embedding.weight  dot:  443513.625    \n",
      "module.lstm.weight_ih_l0  dot:  4880390.0    \n",
      "module.lstm.weight_hh_l0  dot:  3913647.0    \n",
      "module.lstm.bias_ih_l0  dot:  256036.5625    \n",
      "module.lstm.bias_hh_l0  dot:  256036.5625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12001860.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28339.62890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  645497.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  645497.25    \n",
      "module.adapter.frcn_linear.weight  dot:  54298816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31909.68359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  686424.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.17218017578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  463064.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0520474208751693e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18521182.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50963.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16242.8681640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  110.4730224609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  268928.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1459690085757757e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5846359.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50963.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10751.875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1701.0557861328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  53184.6484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5287149.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4097772.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452784.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  877/6933] Loss: -913.0778 [iq: 8.4360,ans: 8.5528,interp: 9.1817,fusion: -939.2482]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  236993.984375    \n",
      "module.ans_embedding.weight  dot:  972271.4375    \n",
      "module.lstm.weight_ih_l0  dot:  2213831.0    \n",
      "module.lstm.weight_hh_l0  dot:  1464259.875    \n",
      "module.lstm.bias_ih_l0  dot:  110442.40625    \n",
      "module.lstm.bias_hh_l0  dot:  110442.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20500814.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  754.591796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1294700.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1294700.25    \n",
      "module.adapter.frcn_linear.weight  dot:  49950804.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29227.0859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  416631.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  202.621826171875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  313855.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20453100.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55791.0234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6921.8994140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  53.02266311645508    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  87431.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.098868882669194e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5884873.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55791.0234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11.014904022216797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.0253043174743652    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  54.7834358215332    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5248527.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6432437.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452785.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  878/6933] Loss: -876.2139 [iq: 8.2323,ans: 8.4853,interp: 8.1665,fusion: -901.0980]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6717303.5    \n",
      "module.ans_embedding.weight  dot:  1351643.0    \n",
      "module.lstm.weight_ih_l0  dot:  109426720.0    \n",
      "module.lstm.weight_hh_l0  dot:  32259138.0    \n",
      "module.lstm.bias_ih_l0  dot:  7149609.5    \n",
      "module.lstm.bias_hh_l0  dot:  7149609.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18755416.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  43459.625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  655243.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  655243.75    \n",
      "module.adapter.frcn_linear.weight  dot:  51115960.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32546.296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  565036.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  275.7066650390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  507416.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18463108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52409.03515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5814.29052734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  47.820823669433594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  72919.703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.014101134293014e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5437373.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52409.03515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  230.98074340820312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  46.12565612792969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1169.7371826171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4610679.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4543377.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452785.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  879/6933] Loss: -902.5435 [iq: 7.7585,ans: 7.5485,interp: 9.5083,fusion: -927.3588]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1103476.25    \n",
      "module.ans_embedding.weight  dot:  527741.625    \n",
      "module.lstm.weight_ih_l0  dot:  37085116.0    \n",
      "module.lstm.weight_hh_l0  dot:  20276352.0    \n",
      "module.lstm.bias_ih_l0  dot:  2220823.25    \n",
      "module.lstm.bias_hh_l0  dot:  2220823.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14406664.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  30233.46875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  866408.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  866408.375    \n",
      "module.adapter.frcn_linear.weight  dot:  50835400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28264.63671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  595001.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  251.11236572265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  508601.21875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.456524038687348e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17200062.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  46646.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4607.30908203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.10395812988281    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  62574.03125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.622396770377236e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5119883.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  46646.1640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1772.6590576171875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  349.43536376953125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11009.63671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6380473.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4918812.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452786.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  880/6933] Loss: -901.3682 [iq: 7.9693,ans: 7.4142,interp: 7.4837,fusion: -924.2355]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  986785.5    \n",
      "module.ans_embedding.weight  dot:  1203325.75    \n",
      "module.lstm.weight_ih_l0  dot:  10703260.0    \n",
      "module.lstm.weight_hh_l0  dot:  3881380.5    \n",
      "module.lstm.bias_ih_l0  dot:  592265.25    \n",
      "module.lstm.bias_hh_l0  dot:  592265.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12356610.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19966.35546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  335534.96875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  335534.96875    \n",
      "module.adapter.frcn_linear.weight  dot:  36205048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20834.0703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  483488.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  252.93023681640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  343162.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5547706172801554e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13755702.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38610.5703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5679.68359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  51.73065948486328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  57860.7578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.726267211983213e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4821315.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38610.5703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  390.7952575683594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  57.652549743652344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1212.5909423828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4088857.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3923972.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452786.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  881/6933] Loss: -851.4245 [iq: 11.2290,ans: 9.4804,interp: 8.8997,fusion: -881.0336]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2271747.75    \n",
      "module.ans_embedding.weight  dot:  625673.5    \n",
      "module.lstm.weight_ih_l0  dot:  42585416.0    \n",
      "module.lstm.weight_hh_l0  dot:  7769911.5    \n",
      "module.lstm.bias_ih_l0  dot:  2673204.0    \n",
      "module.lstm.bias_hh_l0  dot:  2673204.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  21546828.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  37810.23046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1870195.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1870195.0    \n",
      "module.adapter.frcn_linear.weight  dot:  74950824.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46135.4140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1627710.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  580.630859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1522035.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.2532413974404335e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22893848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65206.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8661.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  61.37215042114258    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  125835.2734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3263026232834818e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6101633.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65206.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  272.751953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  64.62042999267578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  956.4674072265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6075976.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6604906.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452786.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  882/6933] Loss: -880.7209 [iq: 6.0983,ans: 6.1108,interp: 5.2609,fusion: -898.1909]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  105801.15625    \n",
      "module.ans_embedding.weight  dot:  744793.1875    \n",
      "module.lstm.weight_ih_l0  dot:  1678971.75    \n",
      "module.lstm.weight_hh_l0  dot:  640006.4375    \n",
      "module.lstm.bias_ih_l0  dot:  107095.5    \n",
      "module.lstm.bias_hh_l0  dot:  107095.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11881520.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8038.09033203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  420950.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  420950.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  35446660.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21194.99609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  536902.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  227.40054321289062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  551634.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.043126970529556e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  12734663.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36101.97265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5715.85986328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  45.3743896484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  80723.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5967316358000971e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4515232.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36101.97265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13.930451393127441    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.9943732023239136    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  119.60743713378906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4204130.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3582999.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452787.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  883/6933] Loss: -925.0648 [iq: 8.9362,ans: 7.6222,interp: 7.5380,fusion: -949.1613]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  10102034.0    \n",
      "module.ans_embedding.weight  dot:  757561.1875    \n",
      "module.lstm.weight_ih_l0  dot:  84651600.0    \n",
      "module.lstm.weight_hh_l0  dot:  12694646.0    \n",
      "module.lstm.bias_ih_l0  dot:  4981342.0    \n",
      "module.lstm.bias_hh_l0  dot:  4981342.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16530691.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6886.9560546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1453375.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1453375.75    \n",
      "module.adapter.frcn_linear.weight  dot:  99153312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  74532.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1193005.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  488.4494323730469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1281435.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  29914150.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  92438.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20378.5703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  95.51866149902344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  241257.75    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.3289779821643606e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8987394.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  92438.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  380.77362060546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  91.33395385742188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1360.4581298828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4750609.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6144665.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452787.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  884/6933] Loss: -874.6016 [iq: 8.6069,ans: 7.5868,interp: 7.9465,fusion: -898.7417]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  248708.1875    \n",
      "module.ans_embedding.weight  dot:  517382.90625    \n",
      "module.lstm.weight_ih_l0  dot:  3010849.0    \n",
      "module.lstm.weight_hh_l0  dot:  3323821.5    \n",
      "module.lstm.bias_ih_l0  dot:  186769.546875    \n",
      "module.lstm.bias_hh_l0  dot:  186769.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7789897.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15173.521484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  238641.484375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  238641.484375    \n",
      "module.adapter.frcn_linear.weight  dot:  61536144.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38374.2109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  597782.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  280.5539855957031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  443562.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23445124.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66865.2265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6503.197265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  72.45323944091797    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  81612.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.083201580826426e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6027737.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66865.2265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  129.84722900390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.226726531982422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  445.0423583984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3507703.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3525303.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452787.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  885/6933] Loss: -875.0795 [iq: 10.2948,ans: 8.9664,interp: 9.3317,fusion: -903.6724]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2223752.75    \n",
      "module.ans_embedding.weight  dot:  625513.5    \n",
      "module.lstm.weight_ih_l0  dot:  66773544.0    \n",
      "module.lstm.weight_hh_l0  dot:  55181164.0    \n",
      "module.lstm.bias_ih_l0  dot:  5455128.5    \n",
      "module.lstm.bias_hh_l0  dot:  5455128.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13657896.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13835.39453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  742910.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  742910.75    \n",
      "module.adapter.frcn_linear.weight  dot:  97259152.0    \n",
      "module.adapter.frcn_linear.bias  dot:  66281.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  683990.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  326.735595703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  810802.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.852175384759903e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  35157368.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  106757.578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10395.36328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  136.58602905273438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  73691.828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.447322675739997e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  11032470.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  106757.578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4318.39111328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  766.6950073242188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  20719.9765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5326244.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5420980.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452788.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  886/6933] Loss: -844.8780 [iq: 8.4175,ans: 8.4035,interp: 8.9708,fusion: -870.6697]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  247249.5    \n",
      "module.ans_embedding.weight  dot:  1655836.875    \n",
      "module.lstm.weight_ih_l0  dot:  15612874.0    \n",
      "module.lstm.weight_hh_l0  dot:  12960811.0    \n",
      "module.lstm.bias_ih_l0  dot:  981075.375    \n",
      "module.lstm.bias_hh_l0  dot:  981075.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24715892.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  38152.43359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  802448.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  802448.375    \n",
      "module.adapter.frcn_linear.weight  dot:  50259040.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33565.24609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  524352.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  224.41378784179688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  445511.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7209913494298235e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17160866.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52937.265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4694.4140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.027332305908203    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  62839.9296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.424748567544157e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6217807.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52937.265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9400.1357421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1667.153564453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15082.8525390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5153123.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6106642.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452788.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  887/6933] Loss: -931.0491 [iq: 7.8920,ans: 8.3118,interp: 9.4765,fusion: -956.7294]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  7410792.0    \n",
      "module.ans_embedding.weight  dot:  1164622.0    \n",
      "module.lstm.weight_ih_l0  dot:  72982016.0    \n",
      "module.lstm.weight_hh_l0  dot:  10137812.0    \n",
      "module.lstm.bias_ih_l0  dot:  4116413.5    \n",
      "module.lstm.bias_hh_l0  dot:  4116413.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20612390.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14567.1474609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  699231.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  699231.875    \n",
      "module.adapter.frcn_linear.weight  dot:  85580416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  57176.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  987658.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  422.42669677734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1111213.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.7063151719630696e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  31131530.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  90107.3984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15976.119140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  87.86630249023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  205982.15625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.494871864655579e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7513718.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  90107.3984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2652.7470703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  278.12445068359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2541.7353515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.220446049250313e-16    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5307182.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5890354.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452789.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  888/6933] Loss: -792.2299 [iq: 8.2604,ans: 7.6158,interp: 8.5130,fusion: -816.6191]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  390498.875    \n",
      "module.ans_embedding.weight  dot:  393130.84375    \n",
      "module.lstm.weight_ih_l0  dot:  9437066.0    \n",
      "module.lstm.weight_hh_l0  dot:  7248436.0    \n",
      "module.lstm.bias_ih_l0  dot:  503078.875    \n",
      "module.lstm.bias_hh_l0  dot:  503078.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11636774.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13508.603515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  713233.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  713233.0    \n",
      "module.adapter.frcn_linear.weight  dot:  57445848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39163.046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  343057.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  183.53707885742188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  261104.390625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18108648.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55875.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9628.00390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  119.48561096191406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  108021.2109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.403677505455562e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6067353.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55875.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  21.57341194152832    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3.3970141410827637    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  62.412818908691406    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5230135.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5280532.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452789.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  889/6933] Loss: -918.8203 [iq: 8.8512,ans: 8.5070,interp: 8.2102,fusion: -944.3887]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  897956.75    \n",
      "module.ans_embedding.weight  dot:  898949.5625    \n",
      "module.lstm.weight_ih_l0  dot:  12609442.0    \n",
      "module.lstm.weight_hh_l0  dot:  5544607.0    \n",
      "module.lstm.bias_ih_l0  dot:  798236.375    \n",
      "module.lstm.bias_hh_l0  dot:  798236.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16036492.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5152.658203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  703542.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  703542.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  55661032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37572.02734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  260270.734375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  136.053466796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  242422.609375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18066512.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55582.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6743.0458984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  48.84730911254883    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  85468.296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.424748567544157e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5029140.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55582.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1.2941392660140991    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.2860366702079773    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4.494233131408691    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4912213.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4772266.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452790.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  890/6933] Loss: -898.4117 [iq: 7.3302,ans: 7.1394,interp: 7.3674,fusion: -920.2488]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  162516.5625    \n",
      "module.ans_embedding.weight  dot:  1284590.375    \n",
      "module.lstm.weight_ih_l0  dot:  9018889.0    \n",
      "module.lstm.weight_hh_l0  dot:  7334260.0    \n",
      "module.lstm.bias_ih_l0  dot:  638229.6875    \n",
      "module.lstm.bias_hh_l0  dot:  638229.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22461404.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8870.2314453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  675160.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  675160.0    \n",
      "module.adapter.frcn_linear.weight  dot:  55227512.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35747.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  536148.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  271.521240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  443168.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21654424.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  65811.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2818.6162109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  37.567955017089844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50039.203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.204139258945361e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7793583.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  65811.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  887.2044067382812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  84.77407836914062    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3990.12939453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.403677505455562e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5252658.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4618912.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452791.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  891/6933] Loss: -923.2286 [iq: 9.8319,ans: 8.7237,interp: 8.6454,fusion: -950.4297]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  590270.5    \n",
      "module.ans_embedding.weight  dot:  1290934.0    \n",
      "module.lstm.weight_ih_l0  dot:  8918660.0    \n",
      "module.lstm.weight_hh_l0  dot:  5934746.0    \n",
      "module.lstm.bias_ih_l0  dot:  557120.625    \n",
      "module.lstm.bias_hh_l0  dot:  557120.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19341546.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13808.5361328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  676144.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  676144.5    \n",
      "module.adapter.frcn_linear.weight  dot:  42908912.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26239.484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  346739.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  166.0498504638672    \n",
      "module.attflat_img.mlp.linear.weight  dot:  270204.28125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0954757928848267e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16095286.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48646.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6337.31884765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  87.64351654052734    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  77476.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6155152.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48646.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  505.7672119140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.4581069946289    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2079.314453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5636432.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5154742.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452791.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  892/6933] Loss: -898.2731 [iq: 10.1254,ans: 9.1440,interp: 8.8208,fusion: -926.3633]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  138884.296875    \n",
      "module.ans_embedding.weight  dot:  843552.5    \n",
      "module.lstm.weight_ih_l0  dot:  769206.25    \n",
      "module.lstm.weight_hh_l0  dot:  948965.75    \n",
      "module.lstm.bias_ih_l0  dot:  32585.08203125    \n",
      "module.lstm.bias_hh_l0  dot:  32585.08203125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13933809.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4288.0009765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  620356.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  620356.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  59504488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41370.1953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  523972.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  342.4757385253906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  359802.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.8417267710901797e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20490676.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64595.0625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5778.4453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.03775024414062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  98762.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.589928271845565e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7009682.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64595.0625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  130.18167114257812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.95797348022461    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  807.29443359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5524695.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5704495.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452792.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  893/6933] Loss: -937.8989 [iq: 7.7323,ans: 7.7612,interp: 7.8173,fusion: -961.2096]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  194891.015625    \n",
      "module.ans_embedding.weight  dot:  946295.125    \n",
      "module.lstm.weight_ih_l0  dot:  3025094.0    \n",
      "module.lstm.weight_hh_l0  dot:  3210617.5    \n",
      "module.lstm.bias_ih_l0  dot:  189575.71875    \n",
      "module.lstm.bias_hh_l0  dot:  189575.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12642762.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20537.4453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  390690.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  390690.25    \n",
      "module.adapter.frcn_linear.weight  dot:  72843808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51380.88671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  534569.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  247.4148406982422    \n",
      "module.attflat_img.mlp.linear.weight  dot:  483605.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22594790.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  71109.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6912.97021484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  61.41306686401367    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  98718.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.184261908652843e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8427736.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  71109.9296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1386.394287109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  342.309814453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5351.9248046875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5141322.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5509145.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452792.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  894/6933] Loss: -916.4863 [iq: 7.4067,ans: 7.4325,interp: 7.1515,fusion: -938.4770]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  311623.875    \n",
      "module.ans_embedding.weight  dot:  707191.75    \n",
      "module.lstm.weight_ih_l0  dot:  1637086.0    \n",
      "module.lstm.weight_hh_l0  dot:  1732183.0    \n",
      "module.lstm.bias_ih_l0  dot:  68602.328125    \n",
      "module.lstm.bias_hh_l0  dot:  68602.328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23527356.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17568.62890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2970285.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2970285.25    \n",
      "module.adapter.frcn_linear.weight  dot:  35729176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20585.58984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  590507.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  296.2032775878906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  638289.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  13868442.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42341.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6479.1455078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  50.98698425292969    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  58126.8671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.255671122635249e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5160361.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42341.1484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  405.622802734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  97.12631225585938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2923.9501953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.810507465118462e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5587397.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9436766.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452792.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  895/6933] Loss: -912.7903 [iq: 7.0300,ans: 7.3110,interp: 8.1279,fusion: -935.2592]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1511726.625    \n",
      "module.ans_embedding.weight  dot:  758435.75    \n",
      "module.lstm.weight_ih_l0  dot:  23989134.0    \n",
      "module.lstm.weight_hh_l0  dot:  2904596.25    \n",
      "module.lstm.bias_ih_l0  dot:  1439562.125    \n",
      "module.lstm.bias_hh_l0  dot:  1439562.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16183720.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35900.4375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  600335.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  600335.875    \n",
      "module.adapter.frcn_linear.weight  dot:  65600148.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41007.4921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  990123.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  333.8937683105469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1069447.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3194388631964102e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  21009616.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62657.9609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8109.05224609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  105.97368621826172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  122008.8203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5812449.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62657.9609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1494.43798828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  241.2015380859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5431.93798828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5366074.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5052448.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452793.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  896/6933] Loss: -917.4810 [iq: 7.5788,ans: 6.9544,interp: 7.7919,fusion: -939.8060]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  218587.734375    \n",
      "module.ans_embedding.weight  dot:  2120417.5    \n",
      "module.lstm.weight_ih_l0  dot:  1797214.5    \n",
      "module.lstm.weight_hh_l0  dot:  1614017.0    \n",
      "module.lstm.bias_ih_l0  dot:  105860.8515625    \n",
      "module.lstm.bias_hh_l0  dot:  105860.8515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36430544.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  66110.5234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2255290.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2255290.75    \n",
      "module.adapter.frcn_linear.weight  dot:  36269660.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23591.8203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  235207.921875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  118.43165588378906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  192755.234375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.255671122635249e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13972736.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43122.765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4754.15185546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.721559524536133    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  52733.73046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3833414413966238e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5143058.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43122.765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  279.23211669921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.72227478027344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  655.28662109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7172724.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  10711374.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452793.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  897/6933] Loss: -892.4114 [iq: 9.1386,ans: 7.7895,interp: 7.6673,fusion: -917.0068]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  515263.625    \n",
      "module.ans_embedding.weight  dot:  626638.875    \n",
      "module.lstm.weight_ih_l0  dot:  12648418.0    \n",
      "module.lstm.weight_hh_l0  dot:  7592613.5    \n",
      "module.lstm.bias_ih_l0  dot:  826609.9375    \n",
      "module.lstm.bias_hh_l0  dot:  826609.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9232744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13854.57421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  210933.921875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  210933.921875    \n",
      "module.adapter.frcn_linear.weight  dot:  33743348.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19675.943359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  328823.90625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  164.79623413085938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  296446.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14851208.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44800.0703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4130.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  22.5567626953125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45649.421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0746958878371515e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5226313.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44800.0703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2276.11572265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  387.7870788574219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9775.19921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4663887.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3660701.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452793.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  898/6933] Loss: -911.1094 [iq: 9.0505,ans: 7.7916,interp: 7.7130,fusion: -935.6645]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1233394.375    \n",
      "module.ans_embedding.weight  dot:  809021.5625    \n",
      "module.lstm.weight_ih_l0  dot:  14874956.0    \n",
      "module.lstm.weight_hh_l0  dot:  1529976.5    \n",
      "module.lstm.bias_ih_l0  dot:  908487.9375    \n",
      "module.lstm.bias_hh_l0  dot:  908487.9375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11090541.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2743.26708984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  409249.84375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  409249.84375    \n",
      "module.adapter.frcn_linear.weight  dot:  31955696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19354.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  495302.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  196.95079040527344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  456266.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12697016.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40157.1328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4168.7158203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.63307189941406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45595.95703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.798597157991026e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4000363.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40157.1328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.2965264320373535    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.8868556618690491    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  47.99311065673828    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4715688.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4021895.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452794.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  899/6933] Loss: -911.3806 [iq: 7.6960,ans: 7.4092,interp: 7.4828,fusion: -933.9688]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  267575.84375    \n",
      "module.ans_embedding.weight  dot:  534142.8125    \n",
      "module.lstm.weight_ih_l0  dot:  2292919.0    \n",
      "module.lstm.weight_hh_l0  dot:  1422983.375    \n",
      "module.lstm.bias_ih_l0  dot:  106005.0    \n",
      "module.lstm.bias_hh_l0  dot:  106005.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12166754.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28681.71484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  528661.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  528661.75    \n",
      "module.adapter.frcn_linear.weight  dot:  51564968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33342.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  485052.28125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  291.1177978515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  427071.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2284893930191174e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  16931680.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  51172.7890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4681.6044921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.7929801940918    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  86079.9609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.070823595408001e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6136219.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  51172.7890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  620.02880859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  187.81707763671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2972.076904296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4875906.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4723580.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452794.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  900/6933] Loss: -822.4484 [iq: 10.3225,ans: 9.3769,interp: 9.6768,fusion: -851.8246]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  105203.9609375    \n",
      "module.ans_embedding.weight  dot:  465436.5625    \n",
      "module.lstm.weight_ih_l0  dot:  2555435.75    \n",
      "module.lstm.weight_hh_l0  dot:  1802533.25    \n",
      "module.lstm.bias_ih_l0  dot:  153121.625    \n",
      "module.lstm.bias_hh_l0  dot:  153121.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  20589576.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  55862.50390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1685282.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1685282.875    \n",
      "module.adapter.frcn_linear.weight  dot:  43649400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29417.44140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  406620.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  192.9638214111328    \n",
      "module.attflat_img.mlp.linear.weight  dot:  350750.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15296844.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48249.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6207.2041015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.308624267578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  57591.51953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.829506097958074e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5521769.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48249.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  23089.57421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5346.5400390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  127439.59375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5481127.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6153219.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452794.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  901/6933] Loss: -941.5053 [iq: 8.8981,ans: 8.7175,interp: 8.2397,fusion: -967.3606]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  638859.8125    \n",
      "module.ans_embedding.weight  dot:  1576846.25    \n",
      "module.lstm.weight_ih_l0  dot:  9068651.0    \n",
      "module.lstm.weight_hh_l0  dot:  4865674.0    \n",
      "module.lstm.bias_ih_l0  dot:  534742.6875    \n",
      "module.lstm.bias_hh_l0  dot:  534742.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  54522368.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21226.630859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5722749.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5722749.0    \n",
      "module.adapter.frcn_linear.weight  dot:  45641032.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29170.529296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  347379.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  177.1031036376953    \n",
      "module.attflat_img.mlp.linear.weight  dot:  300500.90625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.0520474208751693e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15866002.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48145.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3018.060546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  29.141990661621094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  62658.8828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3424604478350375e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5952150.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48145.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  433.5987854003906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  69.7471923828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2970.742431640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7375725.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  12513726.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452795.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  902/6933] Loss: -920.9085 [iq: 6.5214,ans: 6.7155,interp: 6.7897,fusion: -940.9351]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2379674.5    \n",
      "module.ans_embedding.weight  dot:  414751.96875    \n",
      "module.lstm.weight_ih_l0  dot:  24850328.0    \n",
      "module.lstm.weight_hh_l0  dot:  4696378.0    \n",
      "module.lstm.bias_ih_l0  dot:  1142534.125    \n",
      "module.lstm.bias_hh_l0  dot:  1142534.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10310478.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22089.703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  505212.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  505212.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  37532328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  23544.55859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  349201.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  211.237060546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  257360.59375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6284396881237626e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12476611.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37813.78125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5486.9560546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.9578857421875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  79068.5    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4698489.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37813.78125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  972.7279052734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  190.842529296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5654.39013671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4147131.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3756148.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452795.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  903/6933] Loss: -924.0324 [iq: 7.6664,ans: 7.6599,interp: 8.3123,fusion: -947.6710]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2229143.0    \n",
      "module.ans_embedding.weight  dot:  1257211.125    \n",
      "module.lstm.weight_ih_l0  dot:  37378808.0    \n",
      "module.lstm.weight_hh_l0  dot:  5634260.5    \n",
      "module.lstm.bias_ih_l0  dot:  2199982.0    \n",
      "module.lstm.bias_hh_l0  dot:  2199982.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35012896.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5813.82958984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2940503.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2940503.25    \n",
      "module.adapter.frcn_linear.weight  dot:  61204208.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37317.8671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  573534.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.03302001953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  472119.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1393589122453704e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19960764.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59054.6015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6060.45849609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.97869873046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  104301.9453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.901959644281305e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5519093.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59054.6015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  134.69619750976562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  27.225971221923828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  893.255126953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6756562.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9154442.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452795.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  904/6933] Loss: -918.0911 [iq: 7.2689,ans: 7.5577,interp: 7.6499,fusion: -940.5676]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  248438.78125    \n",
      "module.ans_embedding.weight  dot:  1837611.0    \n",
      "module.lstm.weight_ih_l0  dot:  4810173.0    \n",
      "module.lstm.weight_hh_l0  dot:  1900369.75    \n",
      "module.lstm.bias_ih_l0  dot:  246778.5    \n",
      "module.lstm.bias_hh_l0  dot:  246778.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  25882334.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11654.4775390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  714886.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  714886.625    \n",
      "module.adapter.frcn_linear.weight  dot:  48276456.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31608.53515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  735030.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  309.6310119628906    \n",
      "module.attflat_img.mlp.linear.weight  dot:  852972.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15325959.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47354.01953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4414.1376953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.370330810546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  56316.1328125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.296865251442796e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5666218.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47354.01953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  245.42190551757812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.32731819152832    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2297.141357421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5562107.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6305525.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452796.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  905/6933] Loss: -895.0558 [iq: 11.2065,ans: 9.8590,interp: 10.8437,fusion: -926.9650]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  3437515.75    \n",
      "module.ans_embedding.weight  dot:  789375.125    \n",
      "module.lstm.weight_ih_l0  dot:  65814672.0    \n",
      "module.lstm.weight_hh_l0  dot:  9164218.0    \n",
      "module.lstm.bias_ih_l0  dot:  4372916.0    \n",
      "module.lstm.bias_hh_l0  dot:  4372916.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10698946.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28560.67578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  407612.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  407612.0    \n",
      "module.adapter.frcn_linear.weight  dot:  80269008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46639.65625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  2038175.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  917.2857055664062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2955713.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.684341886080801e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22908362.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  69286.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  20520.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  71.46461486816406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  202738.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8417267710901797e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8007612.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  69286.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  75.76502990722656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.621618270874023    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  160.23239135742188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4750602.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5065178.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452796.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  906/6933] Loss: -841.5282 [iq: 9.5844,ans: 9.0940,interp: 9.8256,fusion: -870.0322]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  203878.53125    \n",
      "module.ans_embedding.weight  dot:  1048360.625    \n",
      "module.lstm.weight_ih_l0  dot:  3921960.0    \n",
      "module.lstm.weight_hh_l0  dot:  2314490.25    \n",
      "module.lstm.bias_ih_l0  dot:  264826.53125    \n",
      "module.lstm.bias_hh_l0  dot:  264826.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  29080726.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32612.1328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2142735.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2142735.0    \n",
      "module.adapter.frcn_linear.weight  dot:  43801008.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25786.9921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  858282.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  405.3232116699219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  674084.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.165951056871563e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17030750.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48546.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5431.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  46.06135940551758    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  95999.8515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.386926543200389e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6076138.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48546.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1350.4954833984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  256.0367736816406    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  7386.021484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5610669.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6886334.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452797.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  907/6933] Loss: -956.5018 [iq: 7.3334,ans: 7.7965,interp: 7.0581,fusion: -978.6898]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  273956.625    \n",
      "module.ans_embedding.weight  dot:  521154.0    \n",
      "module.lstm.weight_ih_l0  dot:  4417493.0    \n",
      "module.lstm.weight_hh_l0  dot:  1614654.125    \n",
      "module.lstm.bias_ih_l0  dot:  293762.375    \n",
      "module.lstm.bias_hh_l0  dot:  293762.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14403630.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13659.943359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  933103.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  933103.375    \n",
      "module.adapter.frcn_linear.weight  dot:  55522176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36335.59375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  610982.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  329.6392822265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  529842.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  16975662.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50886.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7964.583984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  43.39533996582031    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  129265.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.407891189359361e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6010061.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50886.8203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  404.9784240722656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  100.65775299072266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  832.27197265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4223479.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4664696.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452797.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  908/6933] Loss: -899.0107 [iq: 9.1493,ans: 8.6459,interp: 9.4763,fusion: -926.2822]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  89771.4140625    \n",
      "module.ans_embedding.weight  dot:  830763.4375    \n",
      "module.lstm.weight_ih_l0  dot:  1481820.0    \n",
      "module.lstm.weight_hh_l0  dot:  680825.6875    \n",
      "module.lstm.bias_ih_l0  dot:  82586.9296875    \n",
      "module.lstm.bias_hh_l0  dot:  82586.9296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27625760.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  176604.0625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1607196.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1607196.25    \n",
      "module.adapter.frcn_linear.weight  dot:  46842588.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25542.16796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1273658.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  576.713134765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1235958.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15724451.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44633.53125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3480.500732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.408416748046875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  33076.51171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.780531526193954e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5577439.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44633.53125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1882.601806640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  560.5957641601562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9451.791015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.403677505455562e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5454418.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5331320.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452797.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  909/6933] Loss: -845.7621 [iq: 8.6659,ans: 8.2571,interp: 9.3787,fusion: -872.0638]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  242637.0    \n",
      "module.ans_embedding.weight  dot:  870951.25    \n",
      "module.lstm.weight_ih_l0  dot:  1645777.0    \n",
      "module.lstm.weight_hh_l0  dot:  612509.4375    \n",
      "module.lstm.bias_ih_l0  dot:  61034.96875    \n",
      "module.lstm.bias_hh_l0  dot:  61034.96875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22730696.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11244.2861328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1733845.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1733845.0    \n",
      "module.adapter.frcn_linear.weight  dot:  36814120.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22134.1640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  408929.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  247.66876220703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  285713.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13208710.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37602.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4113.66943359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  30.865894317626953    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  76437.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.538517034940014e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4097349.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37602.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  42.506412506103516    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7.468308448791504    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  159.84710693359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5166909.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5785632.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452798.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  910/6933] Loss: -932.1922 [iq: 6.2304,ans: 6.9254,interp: 6.3707,fusion: -951.7188]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  369204.375    \n",
      "module.ans_embedding.weight  dot:  388768.75    \n",
      "module.lstm.weight_ih_l0  dot:  2565595.0    \n",
      "module.lstm.weight_hh_l0  dot:  956841.1875    \n",
      "module.lstm.bias_ih_l0  dot:  144339.859375    \n",
      "module.lstm.bias_hh_l0  dot:  144339.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9160771.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7816.8291015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  407499.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  407499.5    \n",
      "module.adapter.frcn_linear.weight  dot:  52717928.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30222.923828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  614315.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  297.03485107421875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  460331.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17860688.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50645.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6679.955078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  90.85668182373047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  106889.6171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5047338.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50645.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  102.02051544189453    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.724529266357422    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  89.2360610961914    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4066807.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3505575.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452798.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  911/6933] Loss: -921.3958 [iq: 7.7686,ans: 7.5032,interp: 7.4435,fusion: -944.1111]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1531056.5    \n",
      "module.ans_embedding.weight  dot:  419795.25    \n",
      "module.lstm.weight_ih_l0  dot:  30276836.0    \n",
      "module.lstm.weight_hh_l0  dot:  3059958.75    \n",
      "module.lstm.bias_ih_l0  dot:  2470878.5    \n",
      "module.lstm.bias_hh_l0  dot:  2470878.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15020210.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41589.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  454722.34375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  454722.34375    \n",
      "module.adapter.frcn_linear.weight  dot:  49043024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29696.83203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  522785.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  322.17138671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  493945.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18147912.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53326.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8103.1591796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  207.50277709960938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  136722.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5288605936802924e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7104835.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53326.4765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3900.624755859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  664.8056640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13752.7724609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5490839.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3847423.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452799.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  912/6933] Loss: -928.6895 [iq: 8.1779,ans: 7.9210,interp: 7.8262,fusion: -952.6144]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  289743.9375    \n",
      "module.ans_embedding.weight  dot:  768314.8125    \n",
      "module.lstm.weight_ih_l0  dot:  1985289.875    \n",
      "module.lstm.weight_hh_l0  dot:  1873429.125    \n",
      "module.lstm.bias_ih_l0  dot:  101495.3125    \n",
      "module.lstm.bias_hh_l0  dot:  101495.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19763280.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3958.525390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2458042.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2458042.0    \n",
      "module.adapter.frcn_linear.weight  dot:  33623256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  18753.4921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  305534.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  150.71951293945312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  217324.734375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  12550005.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34783.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6167.4970703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.704010009765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  80107.375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.555378710501827e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4647520.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34783.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  75.11459350585938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.983665466308594    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  525.24755859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4692748.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7880176.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452799.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  913/6933] Loss: -925.3281 [iq: 9.9628,ans: 8.9630,interp: 8.5374,fusion: -952.7913]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  195392.25    \n",
      "module.ans_embedding.weight  dot:  1018965.25    \n",
      "module.lstm.weight_ih_l0  dot:  1188740.5    \n",
      "module.lstm.weight_hh_l0  dot:  566334.0    \n",
      "module.lstm.bias_ih_l0  dot:  41471.5859375    \n",
      "module.lstm.bias_hh_l0  dot:  41471.5859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24221222.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12113.8525390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1214560.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1214560.0    \n",
      "module.adapter.frcn_linear.weight  dot:  39741220.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22310.51953125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  508525.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  213.33056640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  337019.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.502567207964603e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13650451.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37458.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4903.00390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  23.44315528869629    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  70113.453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.8267663765291218e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5068743.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37458.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  229.7195587158203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  48.39936828613281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  902.7076416015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2366996315904544e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6804100.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6630983.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452800.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  914/6933] Loss: -949.8351 [iq: 8.7856,ans: 8.5646,interp: 7.8437,fusion: -975.0290]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  425649.875    \n",
      "module.ans_embedding.weight  dot:  739495.5    \n",
      "module.lstm.weight_ih_l0  dot:  8907703.0    \n",
      "module.lstm.weight_hh_l0  dot:  1543707.0    \n",
      "module.lstm.bias_ih_l0  dot:  567247.3125    \n",
      "module.lstm.bias_hh_l0  dot:  567247.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13385567.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  118547.25    \n",
      "module.ans_lstm.bias_ih_l0  dot:  689727.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  689727.125    \n",
      "module.adapter.frcn_linear.weight  dot:  45438152.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26349.76171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  662268.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  269.3536071777344    \n",
      "module.attflat_img.mlp.linear.weight  dot:  730818.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16402527.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44226.96875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2984.74951171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  21.424713134765625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  43872.12890625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1118786719398486e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5478764.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44226.96875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  13962.822265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2834.974365234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  79444.28125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0510348147363402e-10    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4953242.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4599310.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452800.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  915/6933] Loss: -899.2224 [iq: 7.8713,ans: 7.7456,interp: 7.7998,fusion: -922.6391]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  426018.1875    \n",
      "module.ans_embedding.weight  dot:  791142.75    \n",
      "module.lstm.weight_ih_l0  dot:  8567947.0    \n",
      "module.lstm.weight_hh_l0  dot:  14582509.0    \n",
      "module.lstm.bias_ih_l0  dot:  791961.1875    \n",
      "module.lstm.bias_hh_l0  dot:  791961.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18055140.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4228.42333984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  785436.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  785436.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  55753776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40034.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  403427.53125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  220.90716552734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  340029.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20003720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59368.71875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8269.9326171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.45143508911133    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  114533.546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.293401272865594e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7587576.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59368.71875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  31.94330596923828    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.422043323516846    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  242.34181213378906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4974982.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4619886.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452800.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  916/6933] Loss: -941.8728 [iq: 7.9831,ans: 7.9533,interp: 9.7664,fusion: -967.5756]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  514592.46875    \n",
      "module.ans_embedding.weight  dot:  279667.4375    \n",
      "module.lstm.weight_ih_l0  dot:  13025900.0    \n",
      "module.lstm.weight_hh_l0  dot:  9634856.0    \n",
      "module.lstm.bias_ih_l0  dot:  955953.4375    \n",
      "module.lstm.bias_hh_l0  dot:  955953.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  8892346.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12834.083984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  377170.34375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  377170.34375    \n",
      "module.adapter.frcn_linear.weight  dot:  40463852.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21732.796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  581439.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  264.81829833984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  500606.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4019543565809727e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16732303.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45167.16796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10398.6123046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.17042922973633    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  115621.8046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.253806527136476e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6184327.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45167.16796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  451.36114501953125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  66.68856048583984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3498.991943359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4966237.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3708853.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452800.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  917/6933] Loss: -932.3959 [iq: 8.5688,ans: 8.4782,interp: 8.7360,fusion: -958.1788]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  606951.5625    \n",
      "module.ans_embedding.weight  dot:  988029.125    \n",
      "module.lstm.weight_ih_l0  dot:  23351584.0    \n",
      "module.lstm.weight_hh_l0  dot:  14625723.0    \n",
      "module.lstm.bias_ih_l0  dot:  1550092.5    \n",
      "module.lstm.bias_hh_l0  dot:  1550092.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11903352.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3924.33642578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  617050.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  617050.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  61904640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38727.6640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  746587.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  299.89215087890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  941893.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.007016857736744e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24054112.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  66181.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5185.462890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.637657165527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  101945.515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.53819712522818e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8035084.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  66181.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  404.7219543457031    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  100.37162780761719    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2158.41845703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.124100812432971e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4768716.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5431150.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452801.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  918/6933] Loss: -960.2542 [iq: 7.9002,ans: 7.4856,interp: 7.0582,fusion: -982.6981]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  459292.21875    \n",
      "module.ans_embedding.weight  dot:  564076.875    \n",
      "module.lstm.weight_ih_l0  dot:  13143024.0    \n",
      "module.lstm.weight_hh_l0  dot:  5675434.0    \n",
      "module.lstm.bias_ih_l0  dot:  954738.625    \n",
      "module.lstm.bias_hh_l0  dot:  954738.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10269822.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18574.80078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  328334.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  328334.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  47038832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31203.857421875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  498972.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  263.75164794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  660816.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  18012796.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52161.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8258.8154296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.735870361328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  88066.734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.798597157991026e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6516051.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52161.7421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  159.3435821533203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  24.543075561523438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1556.732666015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4109200.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3682285.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452801.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  919/6933] Loss: -882.3901 [iq: 10.9550,ans: 9.0855,interp: 9.0253,fusion: -911.4559]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4651702.0    \n",
      "module.ans_embedding.weight  dot:  1190406.625    \n",
      "module.lstm.weight_ih_l0  dot:  87771792.0    \n",
      "module.lstm.weight_hh_l0  dot:  13552189.0    \n",
      "module.lstm.bias_ih_l0  dot:  5448201.0    \n",
      "module.lstm.bias_hh_l0  dot:  5448201.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18772712.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10388.4228515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  435619.96875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  435619.96875    \n",
      "module.adapter.frcn_linear.weight  dot:  82437488.0    \n",
      "module.adapter.frcn_linear.bias  dot:  52831.84375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  751173.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  320.5282897949219    \n",
      "module.attflat_img.mlp.linear.weight  dot:  636000.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.476099325576797e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23941724.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64121.96484375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  26613.279296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  173.05853271484375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  244164.625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.143885234952904e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7635074.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64121.96484375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  152.620361328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.066495895385742    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  776.1328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.214229214014267e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4894346.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3521452.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452801.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  920/6933] Loss: -874.3728 [iq: 9.4611,ans: 7.9132,interp: 8.1809,fusion: -899.9280]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  373250.40625    \n",
      "module.ans_embedding.weight  dot:  584465.25    \n",
      "module.lstm.weight_ih_l0  dot:  8138437.5    \n",
      "module.lstm.weight_hh_l0  dot:  6386406.0    \n",
      "module.lstm.bias_ih_l0  dot:  482914.96875    \n",
      "module.lstm.bias_hh_l0  dot:  482914.96875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14072184.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19470.39453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1767889.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1767889.625    \n",
      "module.adapter.frcn_linear.weight  dot:  54847816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34272.1796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  305189.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  149.17910766601562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  208626.859375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_img.linear_merge.weight  dot:  20226030.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55495.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3005.283203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  19.484066009521484    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  46298.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2832437334964197e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6310774.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55495.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  681.924560546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  87.99505615234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4624.646484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4689764.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6287992.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452802.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  921/6933] Loss: -943.2298 [iq: 7.5841,ans: 7.5247,interp: 7.7054,fusion: -966.0441]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11356648.0    \n",
      "module.ans_embedding.weight  dot:  1288896.0    \n",
      "module.lstm.weight_ih_l0  dot:  112817800.0    \n",
      "module.lstm.weight_hh_l0  dot:  8410116.0    \n",
      "module.lstm.bias_ih_l0  dot:  6647956.5    \n",
      "module.lstm.bias_hh_l0  dot:  6647956.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22682340.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  62026.52734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1384246.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1384246.25    \n",
      "module.adapter.frcn_linear.weight  dot:  48702472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31600.10546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  653045.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  277.84283447265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  677970.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  17418556.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47449.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  19021.833984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  195.95460510253906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  109576.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.446110786673671e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4665016.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47449.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  15301.74609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2862.22216796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  64683.62890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5263177.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6123319.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  18  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452802.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  922/6933] Loss: -889.8506 [iq: 9.1994,ans: 9.1154,interp: 8.7143,fusion: -916.8798]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  340391.15625    \n",
      "module.ans_embedding.weight  dot:  604140.5625    \n",
      "module.lstm.weight_ih_l0  dot:  5141496.0    \n",
      "module.lstm.weight_hh_l0  dot:  3106930.0    \n",
      "module.lstm.bias_ih_l0  dot:  295076.3125    \n",
      "module.lstm.bias_hh_l0  dot:  295076.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17647724.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4486.212890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1535411.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1535411.25    \n",
      "module.adapter.frcn_linear.weight  dot:  62443160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41233.8828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  443711.65625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  174.15121459960938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  352524.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.38737071515061e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23032108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64176.8984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7177.169921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  35.89825439453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  112279.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.640288236463675e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6766277.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64176.8984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  605.0084228515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  148.7638397216797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2330.0810546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4806986.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7178960.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452802.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  923/6933] Loss: -907.6395 [iq: 7.8561,ans: 8.0141,interp: 7.8718,fusion: -931.3815]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1219640.375    \n",
      "module.ans_embedding.weight  dot:  1367194.75    \n",
      "module.lstm.weight_ih_l0  dot:  7899987.0    \n",
      "module.lstm.weight_hh_l0  dot:  1066329.875    \n",
      "module.lstm.bias_ih_l0  dot:  441604.40625    \n",
      "module.lstm.bias_hh_l0  dot:  441604.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19681220.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11892.4765625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  602726.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  602726.75    \n",
      "module.adapter.frcn_linear.weight  dot:  53178240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30619.3828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  747418.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  341.74261474609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  587018.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17995992.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48337.734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6031.134765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  40.198204040527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  84408.0703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.190248313941993e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5350842.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48337.734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  230.58709716796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  44.06916046142578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  260.12640380859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5470179.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5721084.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452803.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  924/6933] Loss: -898.3331 [iq: 9.0792,ans: 8.1050,interp: 9.4383,fusion: -924.9556]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  239575.71875    \n",
      "module.ans_embedding.weight  dot:  802898.625    \n",
      "module.lstm.weight_ih_l0  dot:  5858164.0    \n",
      "module.lstm.weight_hh_l0  dot:  8422550.0    \n",
      "module.lstm.bias_ih_l0  dot:  543417.0625    \n",
      "module.lstm.bias_hh_l0  dot:  543417.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17918348.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8669.3642578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  886446.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  886446.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  45851592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27482.298828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  554833.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  270.1011962890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  443442.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.70142663794104e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15237486.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41708.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15964.00390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.27250671386719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  201683.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6095373.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41708.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  423.7972412109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  84.21154022216797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2153.57763671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.194245199571014e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5267585.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4492606.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452803.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  925/6933] Loss: -921.3343 [iq: 8.4212,ans: 8.3113,interp: 8.4423,fusion: -946.5091]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  254176.046875    \n",
      "module.ans_embedding.weight  dot:  575188.5625    \n",
      "module.lstm.weight_ih_l0  dot:  8868698.0    \n",
      "module.lstm.weight_hh_l0  dot:  6514979.5    \n",
      "module.lstm.bias_ih_l0  dot:  556583.25    \n",
      "module.lstm.bias_hh_l0  dot:  556583.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10576788.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11816.9453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  269196.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  269196.5    \n",
      "module.adapter.frcn_linear.weight  dot:  45310096.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26841.697265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  427147.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  243.09811401367188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  306747.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.7826096154749393e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15873745.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43808.953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5674.7451171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  41.96571350097656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  124657.4140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.277755452785641e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5026243.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43808.953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  191.6722412109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.63016891479492    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1132.129638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.967582315084655e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4422363.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3675336.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452803.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  926/6933] Loss: -937.9484 [iq: 7.5661,ans: 7.5065,interp: 7.3668,fusion: -960.3878]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  505058.4375    \n",
      "module.ans_embedding.weight  dot:  688177.5625    \n",
      "module.lstm.weight_ih_l0  dot:  30937300.0    \n",
      "module.lstm.weight_hh_l0  dot:  20712462.0    \n",
      "module.lstm.bias_ih_l0  dot:  2121869.25    \n",
      "module.lstm.bias_hh_l0  dot:  2121869.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18388784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  143256.9375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  912193.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  912193.75    \n",
      "module.adapter.frcn_linear.weight  dot:  51040048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31116.75390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  341411.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  159.58169555664062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  209305.078125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22473924.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61691.51171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10189.904296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.37718963623047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  91351.1640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5307258511020336e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8756904.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61691.51171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  24877.466796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6149.08544921875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  90312.28125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5096625.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3833916.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452804.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  927/6933] Loss: -909.4396 [iq: 11.9581,ans: 9.6074,interp: 9.6202,fusion: -940.6252]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  697315.9375    \n",
      "module.ans_embedding.weight  dot:  1984323.25    \n",
      "module.lstm.weight_ih_l0  dot:  19733316.0    \n",
      "module.lstm.weight_hh_l0  dot:  16856684.0    \n",
      "module.lstm.bias_ih_l0  dot:  1294075.5    \n",
      "module.lstm.bias_hh_l0  dot:  1294075.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36495640.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  56806.6328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2781029.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2781029.5    \n",
      "module.adapter.frcn_linear.weight  dot:  80061536.0    \n",
      "module.adapter.frcn_linear.bias  dot:  55032.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1044016.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  468.40087890625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  981121.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.604316927725449e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  27676150.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  79819.09375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7734.7919921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  74.87548065185547    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  76543.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.2852799348765984e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  12363620.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  79819.09375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  786.1998291015625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  162.33270263671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1107.396728515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6711862.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6330240.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452805.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  928/6933] Loss: -905.7586 [iq: 10.6173,ans: 8.3617,interp: 8.9099,fusion: -933.6476]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  220274.9375    \n",
      "module.ans_embedding.weight  dot:  1157328.625    \n",
      "module.lstm.weight_ih_l0  dot:  4082187.25    \n",
      "module.lstm.weight_hh_l0  dot:  4241588.0    \n",
      "module.lstm.bias_ih_l0  dot:  294610.375    \n",
      "module.lstm.bias_hh_l0  dot:  294610.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  32504314.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40362.02734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  4178546.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  4178546.0    \n",
      "module.adapter.frcn_linear.weight  dot:  30792884.0    \n",
      "module.adapter.frcn_linear.bias  dot:  18392.63671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  430432.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  203.49819946289062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  504256.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2028067430946976e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13004760.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  35083.79296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10121.2138671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  34.76178741455078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  117123.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.722622458710248e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3640290.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  35083.79296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  11762.380859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2054.6025390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  27233.1171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5746725.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8860311.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452805.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  929/6933] Loss: -920.8301 [iq: 7.2920,ans: 6.6576,interp: 6.0581,fusion: -940.8378]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  350638.25    \n",
      "module.ans_embedding.weight  dot:  530946.5    \n",
      "module.lstm.weight_ih_l0  dot:  12336484.0    \n",
      "module.lstm.weight_hh_l0  dot:  9383440.0    \n",
      "module.lstm.bias_ih_l0  dot:  843451.6875    \n",
      "module.lstm.bias_hh_l0  dot:  843451.6875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12902484.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14570.9921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  644246.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  644246.625    \n",
      "module.adapter.frcn_linear.weight  dot:  34276788.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20733.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  243300.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  116.46080780029297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  204854.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14307606.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38673.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6158.96044921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  62.48308181762695    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  86312.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6838148.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38673.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10.458698272705078    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.985797107219696    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  86.72531127929688    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4591825.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4970759.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452805.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  930/6933] Loss: -921.8042 [iq: 8.3840,ans: 7.8787,interp: 7.6769,fusion: -945.7438]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  149422.1875    \n",
      "module.ans_embedding.weight  dot:  909598.5625    \n",
      "module.lstm.weight_ih_l0  dot:  2112727.5    \n",
      "module.lstm.weight_hh_l0  dot:  2300822.25    \n",
      "module.lstm.bias_ih_l0  dot:  100663.859375    \n",
      "module.lstm.bias_hh_l0  dot:  100663.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15732219.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12191.365234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  845356.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  845356.125    \n",
      "module.adapter.frcn_linear.weight  dot:  30676036.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19509.75    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  380497.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  209.11424255371094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  336443.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  12160006.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  35456.1171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7173.6533203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  57.75402069091797    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  62413.66796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3932603.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  35456.1171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  170.51571655273438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.723037719726562    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  836.9468994140625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4508967.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4311869.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452805.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  931/6933] Loss: -915.6960 [iq: 7.8506,ans: 7.5005,interp: 7.2422,fusion: -938.2894]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  565425.125    \n",
      "module.ans_embedding.weight  dot:  1264557.75    \n",
      "module.lstm.weight_ih_l0  dot:  32233072.0    \n",
      "module.lstm.weight_hh_l0  dot:  21478492.0    \n",
      "module.lstm.bias_ih_l0  dot:  2056797.75    \n",
      "module.lstm.bias_hh_l0  dot:  2056797.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19563812.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  68185.7578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  904512.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  904512.0    \n",
      "module.adapter.frcn_linear.weight  dot:  48087408.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29577.33984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  332922.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  176.21182250976562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  304441.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22768928.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  61062.2265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10030.775390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.092479705810547    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  100927.703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7521933.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  61062.2265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7189.66796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1919.6109619140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37457.703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.842615114990622e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5676212.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5992159.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452806.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  932/6933] Loss: -930.9527 [iq: 8.1148,ans: 7.9977,interp: 8.6402,fusion: -955.7054]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  451552.5    \n",
      "module.ans_embedding.weight  dot:  1424621.25    \n",
      "module.lstm.weight_ih_l0  dot:  19453916.0    \n",
      "module.lstm.weight_hh_l0  dot:  14203026.0    \n",
      "module.lstm.bias_ih_l0  dot:  1170283.75    \n",
      "module.lstm.bias_hh_l0  dot:  1170283.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17923502.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  25712.1328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  265371.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  265371.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  37898260.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22099.779296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  334537.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  144.48428344726562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  255295.953125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16453004.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43685.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2671.883056640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.356698989868164    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  32882.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.030503765126923e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6043868.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43685.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  290.3763732910156    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  59.25180435180664    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1284.6690673828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5249499.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4359456.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452806.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  933/6933] Loss: -905.3918 [iq: 9.5289,ans: 9.4154,interp: 9.5174,fusion: -933.8535]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  121505.875    \n",
      "module.ans_embedding.weight  dot:  348494.34375    \n",
      "module.lstm.weight_ih_l0  dot:  3701654.0    \n",
      "module.lstm.weight_hh_l0  dot:  3656190.0    \n",
      "module.lstm.bias_ih_l0  dot:  339297.625    \n",
      "module.lstm.bias_hh_l0  dot:  339297.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12395580.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6803.14453125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1319612.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1319612.625    \n",
      "module.adapter.frcn_linear.weight  dot:  54841432.0    \n",
      "module.adapter.frcn_linear.bias  dot:  38036.39453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  591088.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  248.7936248779297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  470294.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19876928.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56994.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15982.181640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  84.03772735595703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172124.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.66731001122389e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8130821.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56994.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  791.2155151367188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  129.99908447265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4432.7958984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4930279235159105e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5178347.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6881581.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452806.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  934/6933] Loss: -970.8132 [iq: 6.7935,ans: 7.2816,interp: 8.1836,fusion: -993.0719]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  273426.875    \n",
      "module.ans_embedding.weight  dot:  558569.0    \n",
      "module.lstm.weight_ih_l0  dot:  1185197.5    \n",
      "module.lstm.weight_hh_l0  dot:  634663.0    \n",
      "module.lstm.bias_ih_l0  dot:  45576.4375    \n",
      "module.lstm.bias_hh_l0  dot:  45576.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18265452.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  32099.283203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1131311.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1131311.875    \n",
      "module.adapter.frcn_linear.weight  dot:  44373808.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27361.736328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  437940.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  216.46664428710938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  307107.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15916206.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43819.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6043.8623046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  50.63362503051758    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  56074.64453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.645884008728899e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4805275.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43819.4765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  81576.5546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  9684.810546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  74428.59375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5121553.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6014904.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452807.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  935/6933] Loss: -954.7144 [iq: 8.2564,ans: 7.7073,interp: 7.6131,fusion: -978.2913]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  356034.25    \n",
      "module.ans_embedding.weight  dot:  405332.03125    \n",
      "module.lstm.weight_ih_l0  dot:  2752619.75    \n",
      "module.lstm.weight_hh_l0  dot:  1556972.25    \n",
      "module.lstm.bias_ih_l0  dot:  77480.1796875    \n",
      "module.lstm.bias_hh_l0  dot:  77480.1796875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13265318.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12522.240234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1080483.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1080483.0    \n",
      "module.adapter.frcn_linear.weight  dot:  39443024.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22538.63671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  494899.46875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  254.530029296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  415508.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.552713678800501e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14999246.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40420.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10499.736328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.203704833984375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  137913.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0591207910692901e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5043250.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40420.6328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  430.95001220703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  71.38706970214844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3005.84912109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.72937269744034e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4571099.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5287656.5    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452807.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  936/6933] Loss: -935.7607 [iq: 8.2470,ans: 8.0674,interp: 7.7804,fusion: -959.8555]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  23221508.0    \n",
      "module.ans_embedding.weight  dot:  1137121.0    \n",
      "module.lstm.weight_ih_l0  dot:  207737376.0    \n",
      "module.lstm.weight_hh_l0  dot:  19821796.0    \n",
      "module.lstm.bias_ih_l0  dot:  11822585.0    \n",
      "module.lstm.bias_hh_l0  dot:  11822585.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19969682.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17247.232421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1476470.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1476470.875    \n",
      "module.adapter.frcn_linear.weight  dot:  41319256.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22721.86328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  723548.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  301.0789794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  774328.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.266986929404084e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16310810.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43149.6328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15196.5419921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  116.13893127441406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  184114.09375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.240216983613209e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5055075.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43149.6328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2521.4560546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  429.600341796875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4516.666015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4301588.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4921478.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452807.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  937/6933] Loss: -938.8299 [iq: 8.3356,ans: 7.9178,interp: 7.6463,fusion: -962.7296]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2564948.0    \n",
      "module.ans_embedding.weight  dot:  828542.125    \n",
      "module.lstm.weight_ih_l0  dot:  46009880.0    \n",
      "module.lstm.weight_hh_l0  dot:  16323428.0    \n",
      "module.lstm.bias_ih_l0  dot:  2455273.0    \n",
      "module.lstm.bias_hh_l0  dot:  2455273.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  27784290.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18415.75390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2501517.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2501517.5    \n",
      "module.adapter.frcn_linear.weight  dot:  34334400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22232.7890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  242984.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  145.52548217773438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  170019.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14089278.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  39838.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7410.486328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  46.347904205322266    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  102695.9609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.176800878965878e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5081572.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  39838.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  515.761474609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  89.2856674194336    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  716.4107666015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5607523.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5839506.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452808.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  938/6933] Loss: -933.1034 [iq: 7.7770,ans: 7.2752,interp: 7.2208,fusion: -955.3763]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  14210441.0    \n",
      "module.ans_embedding.weight  dot:  624265.5625    \n",
      "module.lstm.weight_ih_l0  dot:  142252688.0    \n",
      "module.lstm.weight_hh_l0  dot:  18084024.0    \n",
      "module.lstm.bias_ih_l0  dot:  8370140.0    \n",
      "module.lstm.bias_hh_l0  dot:  8370140.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11672358.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12019.720703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  330005.65625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  330005.65625    \n",
      "module.adapter.frcn_linear.weight  dot:  58888696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34983.91796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  662795.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  286.4794921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  780976.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.165951056871563e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20194188.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54660.63671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8274.1953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  80.97221374511719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  108348.0546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.206324095117452e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6625643.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54660.63671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  595.8609619140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  134.9069061279297    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4262.41455078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.377298440909726e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4523508.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3722586.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452808.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  939/6933] Loss: -930.5153 [iq: 8.2257,ans: 7.9186,interp: 7.6000,fusion: -954.2596]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2237930.75    \n",
      "module.ans_embedding.weight  dot:  805993.875    \n",
      "module.lstm.weight_ih_l0  dot:  19683622.0    \n",
      "module.lstm.weight_hh_l0  dot:  3098259.5    \n",
      "module.lstm.bias_ih_l0  dot:  1175639.5    \n",
      "module.lstm.bias_hh_l0  dot:  1175639.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19850164.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  177099.640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1456626.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1456626.75    \n",
      "module.adapter.frcn_linear.weight  dot:  54311384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35884.00390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  936007.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  365.4896240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1287060.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18975818.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  52203.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6857.072265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.33998107910156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  86480.296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.665835717787559e-09    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.attflat_lang.linear_merge.weight  dot:  6298489.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  52203.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  91477.359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19949.3515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  386388.15625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4739765.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4440823.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452808.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  940/6933] Loss: -900.2118 [iq: 7.3246,ans: 7.2658,interp: 8.8744,fusion: -923.6766]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  535515.4375    \n",
      "module.ans_embedding.weight  dot:  281670.4375    \n",
      "module.lstm.weight_ih_l0  dot:  22245738.0    \n",
      "module.lstm.weight_hh_l0  dot:  14972754.0    \n",
      "module.lstm.bias_ih_l0  dot:  1380813.75    \n",
      "module.lstm.bias_hh_l0  dot:  1380813.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  8352536.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10953.9072265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  424687.21875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  424687.21875    \n",
      "module.adapter.frcn_linear.weight  dot:  63941760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41252.7109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  324747.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  180.40182495117188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  233841.015625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.022684490540996e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22702068.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62320.0546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11700.859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  56.86820983886719    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  127149.3046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.942855014178349e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8537776.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62320.0546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  928.82763671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  242.3799591064453    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5579.017578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4479877.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3940492.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452809.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  941/6933] Loss: -910.4710 [iq: 8.3259,ans: 8.5976,interp: 9.2191,fusion: -936.6136]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  8245903.5    \n",
      "module.ans_embedding.weight  dot:  782033.75    \n",
      "module.lstm.weight_ih_l0  dot:  84003088.0    \n",
      "module.lstm.weight_hh_l0  dot:  11715962.0    \n",
      "module.lstm.bias_ih_l0  dot:  4798975.0    \n",
      "module.lstm.bias_hh_l0  dot:  4798975.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17444010.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17737.158203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1739659.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1739659.25    \n",
      "module.adapter.frcn_linear.weight  dot:  102607552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  76185.1171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1057417.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  409.7643127441406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1172818.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.980392986908555e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  30805668.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  86386.3671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7260.2802734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  29.373775482177734    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  125538.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5548323517577955e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9656354.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  86386.3671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8334.515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2344.156494140625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11411.001953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4863659.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6585263.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452809.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  942/6933] Loss: -877.1429 [iq: 7.4368,ans: 7.4946,interp: 7.5037,fusion: -899.5781]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  385996.84375    \n",
      "module.ans_embedding.weight  dot:  439014.125    \n",
      "module.lstm.weight_ih_l0  dot:  9338986.0    \n",
      "module.lstm.weight_hh_l0  dot:  7327372.0    \n",
      "module.lstm.bias_ih_l0  dot:  599343.5    \n",
      "module.lstm.bias_hh_l0  dot:  599343.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9362392.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  52516.828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  401096.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  401096.5    \n",
      "module.adapter.frcn_linear.weight  dot:  47755864.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25996.546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  717129.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  283.3761901855469    \n",
      "module.attflat_img.mlp.linear.weight  dot:  586496.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17381812.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45050.66796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  35418.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  96.50120544433594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  281588.3125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.4016344468691386e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6042471.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45050.66796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  365.7673645019531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  64.26229858398438    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  868.0667724609375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4176227.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3385379.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452810.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  943/6933] Loss: -920.3422 [iq: 8.7411,ans: 8.2839,interp: 7.6825,fusion: -945.0497]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  907327.375    \n",
      "module.ans_embedding.weight  dot:  1087865.0    \n",
      "module.lstm.weight_ih_l0  dot:  21386662.0    \n",
      "module.lstm.weight_hh_l0  dot:  17760012.0    \n",
      "module.lstm.bias_ih_l0  dot:  1742797.25    \n",
      "module.lstm.bias_hh_l0  dot:  1742797.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13703113.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  39830.24609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  527316.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  527316.125    \n",
      "module.adapter.frcn_linear.weight  dot:  39842760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  24686.87109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  334154.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  153.9005126953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  301670.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.2737367544323206e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17558984.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48031.796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8417.1318359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  39.84436798095703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  61999.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.963581545780471e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6057234.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48031.796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  606.6058349609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  142.15252685546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2167.0830078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.1325163857000007e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4065988.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4029705.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452810.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  944/6933] Loss: -901.5093 [iq: 8.3548,ans: 7.6004,interp: 7.4991,fusion: -924.9637]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  848183.0    \n",
      "module.ans_embedding.weight  dot:  853622.375    \n",
      "module.lstm.weight_ih_l0  dot:  20673318.0    \n",
      "module.lstm.weight_hh_l0  dot:  9184339.0    \n",
      "module.lstm.bias_ih_l0  dot:  1096038.0    \n",
      "module.lstm.bias_hh_l0  dot:  1096038.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10847927.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20174.908203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  413238.84375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  413238.84375    \n",
      "module.adapter.frcn_linear.weight  dot:  33199140.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21293.453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  260487.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  134.74916076660156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  207623.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.801471055136062e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14458380.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41221.375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2652.02880859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.730045318603516    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  35230.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.386926543200389e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4760924.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41221.375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  103.8773193359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.657245635986328    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  346.6834716796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4406621.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3972403.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452811.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  945/6933] Loss: -896.1620 [iq: 9.6304,ans: 8.3067,interp: 8.1095,fusion: -922.2086]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  4320697.0    \n",
      "module.ans_embedding.weight  dot:  541510.375    \n",
      "module.lstm.weight_ih_l0  dot:  58668232.0    \n",
      "module.lstm.weight_hh_l0  dot:  5485759.0    \n",
      "module.lstm.bias_ih_l0  dot:  2905784.5    \n",
      "module.lstm.bias_hh_l0  dot:  2905784.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10006885.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15441.3359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  156713.640625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  156713.640625    \n",
      "module.adapter.frcn_linear.weight  dot:  55676896.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32399.884765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  484701.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  245.9145965576172    \n",
      "module.attflat_img.mlp.linear.weight  dot:  364748.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18307448.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47360.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6187.64111328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  45.43483352661133    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  38402.828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.480558063984063e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6356150.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47360.0078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  288.2060546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  54.65177536010742    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2241.651611328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4860340.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3213636.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452812.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  946/6933] Loss: -934.2997 [iq: 6.7302,ans: 7.0302,interp: 7.3662,fusion: -955.4263]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  874048.0    \n",
      "module.ans_embedding.weight  dot:  1355195.375    \n",
      "module.lstm.weight_ih_l0  dot:  9837536.0    \n",
      "module.lstm.weight_hh_l0  dot:  6956134.5    \n",
      "module.lstm.bias_ih_l0  dot:  619071.5    \n",
      "module.lstm.bias_hh_l0  dot:  619071.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26142710.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6297.1806640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1021683.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1021683.75    \n",
      "module.adapter.frcn_linear.weight  dot:  56626640.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36325.0    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  667748.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  308.65081787109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  496102.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.904397302307189e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22704020.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62158.6953125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7921.1484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  50.227333068847656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  99175.953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.412207322777249e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7680054.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62158.6953125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2.7068517208099365    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.6760764718055725    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.99481201171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.1325163857000007e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4671291.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4580779.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452812.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  947/6933] Loss: -885.0557 [iq: 10.6033,ans: 9.2925,interp: 9.9013,fusion: -914.8528]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  302705.8125    \n",
      "module.ans_embedding.weight  dot:  688444.25    \n",
      "module.lstm.weight_ih_l0  dot:  6658882.0    \n",
      "module.lstm.weight_hh_l0  dot:  3171004.0    \n",
      "module.lstm.bias_ih_l0  dot:  481669.03125    \n",
      "module.lstm.bias_hh_l0  dot:  481669.03125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12393695.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7033.58203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  333051.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  333051.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  53944776.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33525.37890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  868385.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  384.6424560546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  653584.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.459241947392002e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17458376.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48985.25    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7834.8134765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.99267578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  88155.1015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1151436158106662e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7193152.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48985.25    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  516.6610107421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  129.55593872070312    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1537.254638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4267913.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3519365.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452812.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  948/6933] Loss: -920.0955 [iq: 7.4422,ans: 7.5799,interp: 7.7879,fusion: -942.9055]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1707723.75    \n",
      "module.ans_embedding.weight  dot:  1023143.25    \n",
      "module.lstm.weight_ih_l0  dot:  37394036.0    \n",
      "module.lstm.weight_hh_l0  dot:  37267808.0    \n",
      "module.lstm.bias_ih_l0  dot:  1922751.25    \n",
      "module.lstm.bias_hh_l0  dot:  1922751.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19628128.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20045.5078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1295416.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1295416.25    \n",
      "module.adapter.frcn_linear.weight  dot:  42705616.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22931.916015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  338677.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  207.09881591796875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  208457.921875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14140720.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36492.55078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  31481.611328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  88.87310791015625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  237943.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.1391778065881226e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4875736.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36492.55078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  80.46980285644531    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  17.312541961669922    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  174.64955139160156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4986507.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5302078.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452813.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  949/6933] Loss: -902.2493 [iq: 8.9430,ans: 8.4260,interp: 8.1246,fusion: -927.7429]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  281403.03125    \n",
      "module.ans_embedding.weight  dot:  1005039.25    \n",
      "module.lstm.weight_ih_l0  dot:  4130514.75    \n",
      "module.lstm.weight_hh_l0  dot:  5140396.0    \n",
      "module.lstm.bias_ih_l0  dot:  345739.0    \n",
      "module.lstm.bias_hh_l0  dot:  345739.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24567254.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  470.97222900390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1894763.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1894763.625    \n",
      "module.adapter.frcn_linear.weight  dot:  47009364.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31122.611328125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  253796.390625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  123.19709777832031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  257186.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20108818.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54237.3515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6381.67431640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  24.71255111694336    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  69197.6796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.805066732340492e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6432830.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54237.3515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  6.274263381958008    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.0695204734802246    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  24.095684051513672    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6872951.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6467199.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452813.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  950/6933] Loss: -884.8135 [iq: 6.7175,ans: 6.3770,interp: 6.0580,fusion: -903.9661]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  511612.875    \n",
      "module.ans_embedding.weight  dot:  497512.90625    \n",
      "module.lstm.weight_ih_l0  dot:  19861594.0    \n",
      "module.lstm.weight_hh_l0  dot:  9473088.0    \n",
      "module.lstm.bias_ih_l0  dot:  1292686.5    \n",
      "module.lstm.bias_hh_l0  dot:  1292686.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9501832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12595.3330078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  337515.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  337515.375    \n",
      "module.adapter.frcn_linear.weight  dot:  47842240.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28301.703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  670282.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  297.8098449707031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  707503.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17656116.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47805.6640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4838.2724609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.117950439453125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  75714.4609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2325109821631486e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6124234.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47805.6640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  229.62005615234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.72956085205078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  732.32421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4317257.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2983337.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452813.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  951/6933] Loss: -947.0237 [iq: 8.4636,ans: 7.8228,interp: 7.8657,fusion: -971.1758]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  98572.03125    \n",
      "module.ans_embedding.weight  dot:  446373.4375    \n",
      "module.lstm.weight_ih_l0  dot:  3588391.75    \n",
      "module.lstm.weight_hh_l0  dot:  1218259.625    \n",
      "module.lstm.bias_ih_l0  dot:  239936.15625    \n",
      "module.lstm.bias_hh_l0  dot:  239936.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7400471.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5966.9677734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  245459.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  245459.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  37102108.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21936.69921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  804323.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  327.02978515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  933182.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.384229785297066e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13110906.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36328.33203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5016.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.210533142089844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74607.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.1173605091462377e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4447236.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36328.33203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1704.343994140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  307.610595703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5997.2431640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  3443215.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2945628.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452814.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  952/6933] Loss: -949.1213 [iq: 8.2916,ans: 7.6248,interp: 7.0023,fusion: -972.0400]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  249503.9375    \n",
      "module.ans_embedding.weight  dot:  352607.5    \n",
      "module.lstm.weight_ih_l0  dot:  4311518.5    \n",
      "module.lstm.weight_hh_l0  dot:  3906028.25    \n",
      "module.lstm.bias_ih_l0  dot:  178849.875    \n",
      "module.lstm.bias_hh_l0  dot:  178849.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7308575.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11503.603515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  486660.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  486660.125    \n",
      "module.adapter.frcn_linear.weight  dot:  77217472.0    \n",
      "module.adapter.frcn_linear.bias  dot:  49545.72265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  937745.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  426.00018310546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  918055.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.980925728479633e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  24392248.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67074.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7885.10302734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  34.71195602416992    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  93565.171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.90333446173463e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9988337.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67074.0390625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  116.15992736816406    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.520442962646484    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  344.429443359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4389394.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4396840.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452814.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  953/6933] Loss: -918.2228 [iq: 8.4849,ans: 8.2317,interp: 8.8286,fusion: -943.7679]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  94950.3515625    \n",
      "module.ans_embedding.weight  dot:  622208.125    \n",
      "module.lstm.weight_ih_l0  dot:  506846.75    \n",
      "module.lstm.weight_hh_l0  dot:  693628.8125    \n",
      "module.lstm.bias_ih_l0  dot:  22438.021484375    \n",
      "module.lstm.bias_hh_l0  dot:  22438.021484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17132154.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  77417.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  773839.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  773839.375    \n",
      "module.adapter.frcn_linear.weight  dot:  30353662.0    \n",
      "module.adapter.frcn_linear.bias  dot:  17912.9296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  248271.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  141.1171112060547    \n",
      "module.attflat_img.mlp.linear.weight  dot:  200055.53125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.029185791092459e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12663478.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34954.23046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3669.84326171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  24.730239868164062    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50717.70703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7195134205394424e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5483326.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34954.23046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  30203.125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6596.83203125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  148564.15625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4967160.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3880808.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452814.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  954/6933] Loss: -909.7925 [iq: 9.5618,ans: 8.9046,interp: 8.9057,fusion: -937.1647]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  700626.6875    \n",
      "module.ans_embedding.weight  dot:  976342.875    \n",
      "module.lstm.weight_ih_l0  dot:  8484470.0    \n",
      "module.lstm.weight_hh_l0  dot:  4623161.5    \n",
      "module.lstm.bias_ih_l0  dot:  274634.3125    \n",
      "module.lstm.bias_hh_l0  dot:  274634.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16738170.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  11961.17578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  906287.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  906287.625    \n",
      "module.adapter.frcn_linear.weight  dot:  34213344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19754.72265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  424553.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  216.7026824951172    \n",
      "module.attflat_img.mlp.linear.weight  dot:  279795.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.6816557035781443e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13340793.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  35805.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14949.3486328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  43.02737045288086    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  96143.8046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.924490788951516e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4789345.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  35805.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  221.65127563476562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.493255615234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  892.6246337890625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4866350.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4553454.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452815.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  955/6933] Loss: -894.5573 [iq: 8.6598,ans: 7.8834,interp: 8.4653,fusion: -919.5658]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  202666.53125    \n",
      "module.ans_embedding.weight  dot:  601719.25    \n",
      "module.lstm.weight_ih_l0  dot:  5992318.0    \n",
      "module.lstm.weight_hh_l0  dot:  2973876.5    \n",
      "module.lstm.bias_ih_l0  dot:  371224.21875    \n",
      "module.lstm.bias_hh_l0  dot:  371224.21875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10244854.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12455.7763671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  224522.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  224522.5    \n",
      "module.adapter.frcn_linear.weight  dot:  54209584.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37896.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  573538.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  289.0414123535156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  578208.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  18971372.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55722.15625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14019.490234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.85331726074219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  126844.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7240836.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55722.15625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  25.631858825683594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.409013748168945    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  98.64309692382812    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3990436.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3803693.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452815.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  956/6933] Loss: -920.4875 [iq: 7.5629,ans: 7.8831,interp: 8.2110,fusion: -944.1445]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  863059.875    \n",
      "module.ans_embedding.weight  dot:  467750.71875    \n",
      "module.lstm.weight_ih_l0  dot:  35065140.0    \n",
      "module.lstm.weight_hh_l0  dot:  19327768.0    \n",
      "module.lstm.bias_ih_l0  dot:  2261687.5    \n",
      "module.lstm.bias_hh_l0  dot:  2261687.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11078109.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  54568.2421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  504386.96875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  504386.96875    \n",
      "module.adapter.frcn_linear.weight  dot:  50459784.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35125.8125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  503978.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  253.8258056640625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  402475.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  22024946.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  63173.33984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14779.7275390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  76.51305389404297    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  133411.546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4551915228366852e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9393116.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  63173.33984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16569.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  3435.042724609375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  79028.8515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3861423.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3569738.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452816.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  957/6933] Loss: -934.6515 [iq: 8.3317,ans: 8.5046,interp: 8.3653,fusion: -959.8531]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  282100.34375    \n",
      "module.ans_embedding.weight  dot:  785939.625    \n",
      "module.lstm.weight_ih_l0  dot:  5246077.0    \n",
      "module.lstm.weight_hh_l0  dot:  7420806.0    \n",
      "module.lstm.bias_ih_l0  dot:  445526.25    \n",
      "module.lstm.bias_hh_l0  dot:  445526.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18390288.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  41269.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1363368.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1363368.5    \n",
      "module.adapter.frcn_linear.weight  dot:  44914192.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27177.08203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  384960.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  177.90005493164062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  423441.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16882308.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47601.1875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11292.83984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.04877471923828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  84478.265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3133103493601084e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6049825.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47601.1875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1188.5263671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  356.257080078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3152.172607421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5049820.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6647011.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452816.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  958/6933] Loss: -860.7675 [iq: 10.1125,ans: 8.8347,interp: 8.9586,fusion: -888.6733]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  5234952.5    \n",
      "module.ans_embedding.weight  dot:  1450036.375    \n",
      "module.lstm.weight_ih_l0  dot:  63213376.0    \n",
      "module.lstm.weight_hh_l0  dot:  18103832.0    \n",
      "module.lstm.bias_ih_l0  dot:  3512778.5    \n",
      "module.lstm.bias_hh_l0  dot:  3512778.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  24270916.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6433.728515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1946136.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1946136.75    \n",
      "module.adapter.frcn_linear.weight  dot:  63893892.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39209.0625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  593346.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  236.747802734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  506875.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.456524038687348e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22718168.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60526.5859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17243.44921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  239.68606567382812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  162497.390625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5327876.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60526.5859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3.8210361003875732    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.405131995677948    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  13.277727127075195    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-16    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4802500.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5214489.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452816.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  959/6933] Loss: -925.2532 [iq: 7.0450,ans: 7.0244,interp: 6.7255,fusion: -946.0482]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  181605.8125    \n",
      "module.ans_embedding.weight  dot:  507966.125    \n",
      "module.lstm.weight_ih_l0  dot:  2881472.25    \n",
      "module.lstm.weight_hh_l0  dot:  2723285.5    \n",
      "module.lstm.bias_ih_l0  dot:  171677.15625    \n",
      "module.lstm.bias_hh_l0  dot:  171677.15625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12596206.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7373.1904296875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1275043.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1275043.5    \n",
      "module.adapter.frcn_linear.weight  dot:  60892352.0    \n",
      "module.adapter.frcn_linear.bias  dot:  41535.2578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  750320.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  353.1253662109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  740006.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19483798.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53663.2734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12430.1279296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  50.391334533691406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  123810.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.1154423747211695e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6689986.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53663.2734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  12.150396347045898    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.1106712818145752    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  32.87302780151367    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4089118.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5478797.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452816.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  960/6933] Loss: -937.1515 [iq: 8.7767,ans: 8.8409,interp: 7.9314,fusion: -962.7005]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  315545.375    \n",
      "module.ans_embedding.weight  dot:  231268.234375    \n",
      "module.lstm.weight_ih_l0  dot:  7387869.0    \n",
      "module.lstm.weight_hh_l0  dot:  4791720.0    \n",
      "module.lstm.bias_ih_l0  dot:  426921.375    \n",
      "module.lstm.bias_hh_l0  dot:  426921.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  6364785.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6686.7109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  325203.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  325203.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  41632336.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27907.845703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  306802.53125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  166.9210205078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  285949.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  16129482.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43754.6015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7700.4833984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  40.95922088623047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  113746.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.091749078976136e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5544545.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43754.6015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  82.53047180175781    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  20.044689178466797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  262.8507385253906    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.6187051699034782e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4028007.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3627133.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452817.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  961/6933] Loss: -955.9911 [iq: 7.8683,ans: 7.7397,interp: 7.3946,fusion: -978.9938]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  695444.1875    \n",
      "module.ans_embedding.weight  dot:  412089.78125    \n",
      "module.lstm.weight_ih_l0  dot:  9122452.0    \n",
      "module.lstm.weight_hh_l0  dot:  3998507.5    \n",
      "module.lstm.bias_ih_l0  dot:  436060.84375    \n",
      "module.lstm.bias_hh_l0  dot:  436060.84375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14380762.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8294.72265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1130054.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1130054.0    \n",
      "module.adapter.frcn_linear.weight  dot:  60226820.0    \n",
      "module.adapter.frcn_linear.bias  dot:  34066.125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  525497.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  216.4642333984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  392731.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.3096723705530167e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22304280.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54712.69140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4612.134765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  47.587318420410156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  86016.109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.655955703787185e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7080376.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54712.69140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  73.54292297363281    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  8.82425308227539    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  276.462158203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4951540.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6605803.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452817.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  962/6933] Loss: -923.3544 [iq: 7.4520,ans: 7.7455,interp: 8.8191,fusion: -947.3710]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  203586.53125    \n",
      "module.ans_embedding.weight  dot:  676900.8125    \n",
      "module.lstm.weight_ih_l0  dot:  8765405.0    \n",
      "module.lstm.weight_hh_l0  dot:  6355708.5    \n",
      "module.lstm.bias_ih_l0  dot:  538392.125    \n",
      "module.lstm.bias_hh_l0  dot:  538392.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15183144.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  43492.3125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  992455.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  992455.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  34888392.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22720.5859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  226562.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  131.60171508789062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  185098.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.5547706172801554e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14630108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  39448.62109375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6995.1865234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.077932357788086    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  75936.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.997232849746069e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5322220.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  39448.62109375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1102.6011962890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  242.76644897460938    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8257.853515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4511096.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4412624.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452817.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  963/6933] Loss: -931.4587 [iq: 7.5906,ans: 7.8042,interp: 7.9293,fusion: -954.7828]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  78373.625    \n",
      "module.ans_embedding.weight  dot:  698210.625    \n",
      "module.lstm.weight_ih_l0  dot:  1499430.75    \n",
      "module.lstm.weight_hh_l0  dot:  1367862.75    \n",
      "module.lstm.bias_ih_l0  dot:  62170.5859375    \n",
      "module.lstm.bias_hh_l0  dot:  62170.5859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11971600.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6595.5673828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  623047.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  623047.0    \n",
      "module.adapter.frcn_linear.weight  dot:  37631224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22887.904296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  593847.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  278.6968994140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  486803.84375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.781864042044617e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  13705414.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36961.8203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5503.5732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.86499786376953    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74038.3671875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.441570501332535e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5137514.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36961.8203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  146.91302490234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  49.29508972167969    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  655.5704345703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4545315.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3556214.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452818.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  964/6933] Loss: -926.0489 [iq: 4.9033,ans: 5.4389,interp: 6.1311,fusion: -942.5222]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  11157674.0    \n",
      "module.ans_embedding.weight  dot:  963872.5625    \n",
      "module.lstm.weight_ih_l0  dot:  114704944.0    \n",
      "module.lstm.weight_hh_l0  dot:  10693654.0    \n",
      "module.lstm.bias_ih_l0  dot:  6281522.0    \n",
      "module.lstm.bias_hh_l0  dot:  6281522.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19064848.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1136.864501953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1115368.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1115368.75    \n",
      "module.adapter.frcn_linear.weight  dot:  41783816.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25693.259765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  834168.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  301.46343994140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1082543.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13983106.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37731.125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9095.421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  38.32537841796875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  89714.046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4596709.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37731.125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1.9306225776672363    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.26681822538375854    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  8.131707191467285    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6635735.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4766515.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452818.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  965/6933] Loss: -883.5547 [iq: 7.1997,ans: 6.3033,interp: 6.8593,fusion: -903.9169]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  151924.078125    \n",
      "module.ans_embedding.weight  dot:  701121.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1226067.125    \n",
      "module.lstm.weight_hh_l0  dot:  540621.1875    \n",
      "module.lstm.bias_ih_l0  dot:  39722.1328125    \n",
      "module.lstm.bias_hh_l0  dot:  39722.1328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12914866.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  31436.890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  822574.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  822574.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  57306384.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37289.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  413396.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  203.33084106445312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  311701.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17199184.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45288.8828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3477.998779296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  32.85121154785156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  58028.046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.089209136916907e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6473412.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45288.8828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  475.2215270996094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  85.68330383300781    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2254.1865234375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4641162.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4346019.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452818.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  966/6933] Loss: -933.0060 [iq: 7.1669,ans: 6.9190,interp: 6.6921,fusion: -953.7841]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2397310.5    \n",
      "module.ans_embedding.weight  dot:  649514.1875    \n",
      "module.lstm.weight_ih_l0  dot:  46607864.0    \n",
      "module.lstm.weight_hh_l0  dot:  6876336.0    \n",
      "module.lstm.bias_ih_l0  dot:  3136660.0    \n",
      "module.lstm.bias_hh_l0  dot:  3136660.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22379896.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15516.6484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1658688.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1658688.125    \n",
      "module.adapter.frcn_linear.weight  dot:  41373736.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25380.94140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  572677.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  257.8282470703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  673057.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2789769243681803e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  15114278.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40062.203125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6464.259765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  25.438995361328125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  63436.82421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0756195933936397e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5510404.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40062.203125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  55.16314697265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.919028282165527    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  308.079833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.535394613318203e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4651510.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5510100.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452819.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  967/6933] Loss: -896.6846 [iq: 8.7918,ans: 7.9653,interp: 7.6254,fusion: -921.0671]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1938584.375    \n",
      "module.ans_embedding.weight  dot:  569111.3125    \n",
      "module.lstm.weight_ih_l0  dot:  28708540.0    \n",
      "module.lstm.weight_hh_l0  dot:  5658045.5    \n",
      "module.lstm.bias_ih_l0  dot:  1707219.5    \n",
      "module.lstm.bias_hh_l0  dot:  1707219.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15739888.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3239.854248046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  984672.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  984672.625    \n",
      "module.adapter.frcn_linear.weight  dot:  47925080.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30056.55078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  461996.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  214.27577209472656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  321205.46875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.42746647624881e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22455380.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59687.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2535.0380859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  19.446083068847656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  27853.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.3565470403118525e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7622005.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59687.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  33.79786682128906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.643614768981934    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  251.9847412109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4650876.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4324877.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452819.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  968/6933] Loss: -861.6207 [iq: 10.1254,ans: 9.7369,interp: 9.9635,fusion: -891.4465]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1468153.875    \n",
      "module.ans_embedding.weight  dot:  487578.8125    \n",
      "module.lstm.weight_ih_l0  dot:  23349052.0    \n",
      "module.lstm.weight_hh_l0  dot:  6697336.0    \n",
      "module.lstm.bias_ih_l0  dot:  1501864.625    \n",
      "module.lstm.bias_hh_l0  dot:  1501864.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10866395.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  0.0    NOT UPDATING\n",
      "module.ans_lstm.bias_ih_l0  dot:  572810.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  572810.5    \n",
      "module.adapter.frcn_linear.weight  dot:  100347832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  67791.34375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1675219.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  545.0521240234375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  2151581.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  32517302.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  87734.9921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5002.98291015625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  40.897422790527344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  82414.6015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0510348147363402e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9750684.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  87734.9921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5510837.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5219818.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  21  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452820.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  969/6933] Loss: -962.0776 [iq: 5.6727,ans: 6.0418,interp: 5.6134,fusion: -979.4055]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1059028.5    \n",
      "module.ans_embedding.weight  dot:  423217.625    \n",
      "module.lstm.weight_ih_l0  dot:  12755910.0    \n",
      "module.lstm.weight_hh_l0  dot:  4743837.0    \n",
      "module.lstm.bias_ih_l0  dot:  774427.3125    \n",
      "module.lstm.bias_hh_l0  dot:  774427.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7471317.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20018.97265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  244717.359375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  244717.359375    \n",
      "module.adapter.frcn_linear.weight  dot:  65230952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31521.19921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  1235781.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  504.18609619140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1221185.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  20670332.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50815.328125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8632.76953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  70.69609069824219    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  107642.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.2397586008082726e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6453182.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50815.328125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  965.06884765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  85.56002044677734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5575.9638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.879385536085465e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3984485.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3455524.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452820.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  970/6933] Loss: -938.5223 [iq: 7.9176,ans: 7.9672,interp: 7.7732,fusion: -962.1804]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  76682.453125    \n",
      "module.ans_embedding.weight  dot:  797779.5625    \n",
      "module.lstm.weight_ih_l0  dot:  1491239.25    \n",
      "module.lstm.weight_hh_l0  dot:  2142197.75    \n",
      "module.lstm.bias_ih_l0  dot:  149047.375    \n",
      "module.lstm.bias_hh_l0  dot:  149047.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11970490.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  16130.87109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  360164.71875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  360164.71875    \n",
      "module.adapter.frcn_linear.weight  dot:  45545664.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27140.46875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  717655.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  227.66192626953125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  730055.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.1159076974727213e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15974581.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41968.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5104.46533203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  31.33924674987793    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  48953.69140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.90940968486575e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5738152.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41968.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  390.0625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.81588745117188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1158.8599853515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.176259172870232e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4052478.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3774205.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452820.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  971/6933] Loss: -947.7964 [iq: 7.8054,ans: 8.2743,interp: 7.9484,fusion: -971.8245]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6614384.0    \n",
      "module.ans_embedding.weight  dot:  384969.25    \n",
      "module.lstm.weight_ih_l0  dot:  68314136.0    \n",
      "module.lstm.weight_hh_l0  dot:  6576852.0    \n",
      "module.lstm.bias_ih_l0  dot:  3901182.75    \n",
      "module.lstm.bias_hh_l0  dot:  3901182.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9553862.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15290.138671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  391567.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  391567.375    \n",
      "module.adapter.frcn_linear.weight  dot:  51027904.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32878.5234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  352445.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  203.1027374267578    \n",
      "module.attflat_img.mlp.linear.weight  dot:  247904.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19136410.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50996.5234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4312.06640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.56005859375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  44178.3984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0267342531733448e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6639015.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50996.5234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  84.00323486328125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.349815368652344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  746.0933227539062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.606537787476555e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4032372.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3678049.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452821.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  972/6933] Loss: -894.3885 [iq: 8.8203,ans: 8.5622,interp: 9.6724,fusion: -921.4434]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  178685.328125    \n",
      "module.ans_embedding.weight  dot:  757207.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3106358.75    \n",
      "module.lstm.weight_hh_l0  dot:  4057411.25    \n",
      "module.lstm.bias_ih_l0  dot:  200063.40625    \n",
      "module.lstm.bias_hh_l0  dot:  200063.40625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23431380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15598.83203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2992170.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2992170.75    \n",
      "module.adapter.frcn_linear.weight  dot:  45288576.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28367.693359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  545917.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  286.0531311035156    \n",
      "module.attflat_img.mlp.linear.weight  dot:  446356.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2789769243681803e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15833560.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42296.8125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  18909.0390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.09286499023438    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  189899.28125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5376524.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42296.8125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  141.25677490234375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  26.619165420532227    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  471.9716796875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0264011862659572e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5829523.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8755434.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452821.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  973/6933] Loss: -951.2128 [iq: 6.8319,ans: 6.5496,interp: 6.4777,fusion: -971.0721]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  6192058.0    \n",
      "module.ans_embedding.weight  dot:  456530.90625    \n",
      "module.lstm.weight_ih_l0  dot:  16474164.0    \n",
      "module.lstm.weight_hh_l0  dot:  3856922.75    \n",
      "module.lstm.bias_ih_l0  dot:  530935.3125    \n",
      "module.lstm.bias_hh_l0  dot:  530935.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10803978.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3625.56787109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  389729.3125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  389729.3125    \n",
      "module.adapter.frcn_linear.weight  dot:  69694696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  45008.8671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  999803.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  502.82855224609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  773614.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.2364831642771605e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22215446.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60771.15234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  13125.70703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  82.92385864257812    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  172753.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6903175.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60771.15234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3.967240333557129    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.6125055551528931    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  33.310707092285156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.552713678800501e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3913268.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3359862.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452822.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  974/6933] Loss: -930.6315 [iq: 10.7797,ans: 8.9796,interp: 8.7841,fusion: -959.1749]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  142659.5    \n",
      "module.ans_embedding.weight  dot:  358232.09375    \n",
      "module.lstm.weight_ih_l0  dot:  3491076.0    \n",
      "module.lstm.weight_hh_l0  dot:  2071578.0    \n",
      "module.lstm.bias_ih_l0  dot:  220950.5    \n",
      "module.lstm.bias_hh_l0  dot:  220950.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9379107.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  40145.46875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  774623.1875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  774623.1875    \n",
      "module.adapter.frcn_linear.weight  dot:  52362936.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32303.8984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  467564.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  229.91848754882812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  373293.09375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16152298.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44724.890625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9368.908203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  50.51878356933594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  91666.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.0520474208751693e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6212108.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44724.890625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  117306.765625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13705.279296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  104802.75    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  3898775.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4909648.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452822.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  975/6933] Loss: -953.8959 [iq: 8.4681,ans: 7.5859,interp: 7.8167,fusion: -977.7667]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  80511400.0    \n",
      "module.ans_embedding.weight  dot:  411384.625    \n",
      "module.lstm.weight_ih_l0  dot:  503844096.0    \n",
      "module.lstm.weight_hh_l0  dot:  47553328.0    \n",
      "module.lstm.bias_ih_l0  dot:  28628564.0    \n",
      "module.lstm.bias_hh_l0  dot:  28628564.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9123548.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5073.93359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  435900.90625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  435900.90625    \n",
      "module.adapter.frcn_linear.weight  dot:  55346224.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32275.021484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  732113.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  345.56182861328125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  584685.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16383541.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45828.234375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  28230.302734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  193.07061767578125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  135372.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9454660105111543e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5697658.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45828.234375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  100.30252075195312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  15.174076080322266    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  600.1221313476562    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.0267342531733448e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3643301.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3364894.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452822.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  976/6933] Loss: -905.6023 [iq: 8.3820,ans: 8.4507,interp: 8.5984,fusion: -931.0334]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1573505.0    \n",
      "module.ans_embedding.weight  dot:  388009.375    \n",
      "module.lstm.weight_ih_l0  dot:  19791836.0    \n",
      "module.lstm.weight_hh_l0  dot:  3118148.5    \n",
      "module.lstm.bias_ih_l0  dot:  960373.5    \n",
      "module.lstm.bias_hh_l0  dot:  960373.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10337233.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4804.03271484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  539260.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  539260.875    \n",
      "module.adapter.frcn_linear.weight  dot:  39385312.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19911.306640625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  699008.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  283.9638671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  590198.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.648633643635549e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14460132.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37395.0    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10230.6806640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  40.59701156616211    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  89880.765625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.456524038687348e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4731610.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37395.0    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  162.93511962890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  31.98175048828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  972.54443359375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4314308.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4222232.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452822.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  977/6933] Loss: -907.0036 [iq: 9.8891,ans: 9.7470,interp: 9.9146,fusion: -936.5544]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  880784.5    \n",
      "module.ans_embedding.weight  dot:  423411.25    \n",
      "module.lstm.weight_ih_l0  dot:  15038336.0    \n",
      "module.lstm.weight_hh_l0  dot:  2336715.0    \n",
      "module.lstm.bias_ih_l0  dot:  943432.5    \n",
      "module.lstm.bias_hh_l0  dot:  943432.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9063792.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1999.7130126953125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  416931.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  416931.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  68169728.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42248.87890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  834954.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  274.707763671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  935290.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.178705133497715e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  19805904.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53663.1015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5364.2939453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  37.18694305419922    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  90411.796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.624123223744391e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6395174.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53663.1015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  30.518054962158203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.418268203735352    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  136.06326293945312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4014645.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3972060.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452823.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  978/6933] Loss: -905.4844 [iq: 8.2525,ans: 7.6196,interp: 8.9590,fusion: -930.3156]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  216708.75    \n",
      "module.ans_embedding.weight  dot:  524836.4375    \n",
      "module.lstm.weight_ih_l0  dot:  5652541.5    \n",
      "module.lstm.weight_hh_l0  dot:  2964223.0    \n",
      "module.lstm.bias_ih_l0  dot:  373623.46875    \n",
      "module.lstm.bias_hh_l0  dot:  373623.46875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15781022.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  22290.4375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1937207.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1937207.75    \n",
      "module.adapter.frcn_linear.weight  dot:  50512836.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32960.23046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  436354.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  217.44314575195312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  469912.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20763806.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  59705.171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4237.13134765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  17.624250411987305    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  47657.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.20550350099802e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5591125.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  59705.171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  284.28985595703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  50.95075225830078    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1208.217041015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4250493.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6355630.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452823.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  979/6933] Loss: -936.7797 [iq: 6.8282,ans: 7.2605,interp: 7.1148,fusion: -957.9833]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  252926.0625    \n",
      "module.ans_embedding.weight  dot:  660957.75    \n",
      "module.lstm.weight_ih_l0  dot:  3137417.5    \n",
      "module.lstm.weight_hh_l0  dot:  762616.75    \n",
      "module.lstm.bias_ih_l0  dot:  188664.25    \n",
      "module.lstm.bias_hh_l0  dot:  188664.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12342104.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4129.2705078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  887710.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  887710.375    \n",
      "module.adapter.frcn_linear.weight  dot:  28021278.0    \n",
      "module.adapter.frcn_linear.bias  dot:  16113.8916015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  338962.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  167.91058349609375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  320480.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  10586294.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  28930.4765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2630.12646484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  14.449252128601074    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  37783.8203125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3463772.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  28930.4765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  97.77734375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  19.82286834716797    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  553.680908203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.194245199571014e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4187384.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4622316.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452823.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  980/6933] Loss: -909.5347 [iq: 9.3463,ans: 8.3577,interp: 8.3460,fusion: -935.5847]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  289150.9375    \n",
      "module.ans_embedding.weight  dot:  576551.4375    \n",
      "module.lstm.weight_ih_l0  dot:  14582586.0    \n",
      "module.lstm.weight_hh_l0  dot:  9183670.0    \n",
      "module.lstm.bias_ih_l0  dot:  899930.125    \n",
      "module.lstm.bias_hh_l0  dot:  899930.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13374165.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17167.16015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  976930.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  976930.75    \n",
      "module.adapter.frcn_linear.weight  dot:  38040048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22779.72265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  483844.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  219.85830688476562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  407475.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.185452315956354e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12662968.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  33877.63671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2937.8935546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  21.8952579498291    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45834.7265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.6427748050773516e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4059898.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  33877.63671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  122.25848388671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.579898834228516    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  529.9447021484375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5107247.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3979410.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452824.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  981/6933] Loss: -933.2457 [iq: 7.3770,ans: 7.0076,interp: 6.7472,fusion: -954.3774]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1404238.75    \n",
      "module.ans_embedding.weight  dot:  680572.0    \n",
      "module.lstm.weight_ih_l0  dot:  48303152.0    \n",
      "module.lstm.weight_hh_l0  dot:  44956512.0    \n",
      "module.lstm.bias_ih_l0  dot:  2951308.5    \n",
      "module.lstm.bias_hh_l0  dot:  2951308.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13405124.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1753.14794921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  711743.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  711743.375    \n",
      "module.adapter.frcn_linear.weight  dot:  34829280.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19561.078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  460277.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  209.89511108398438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  363803.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  10725692.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  29124.349609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3710.51708984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  37.81073760986328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  72727.4453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.865476744773332e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4108819.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  29124.349609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.7435892820358276    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.17234474420547485    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4.860764980316162    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.800116025829084e-16    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3755981.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3722940.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452824.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  982/6933] Loss: -941.9497 [iq: 9.4719,ans: 8.1426,interp: 7.5820,fusion: -967.1462]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  434740.25    \n",
      "module.ans_embedding.weight  dot:  664972.0    \n",
      "module.lstm.weight_ih_l0  dot:  6321812.5    \n",
      "module.lstm.weight_hh_l0  dot:  7489750.0    \n",
      "module.lstm.bias_ih_l0  dot:  306217.90625    \n",
      "module.lstm.bias_hh_l0  dot:  306217.90625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14344457.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  113549.734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  576970.6875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  576970.6875    \n",
      "module.adapter.frcn_linear.weight  dot:  33188248.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19677.98046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  384688.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  159.45425415039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  284151.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.87805368215777e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  11330333.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  30774.66015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3566.688232421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  20.443883895874023    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  52863.7734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.460574463242665e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3732162.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  30774.66015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  27133.99609375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4857.9404296875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  79843.0625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4371538.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3775694.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452825.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  983/6933] Loss: -936.9731 [iq: 8.3042,ans: 7.6245,interp: 7.2525,fusion: -960.1544]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1409347.625    \n",
      "module.ans_embedding.weight  dot:  1680027.25    \n",
      "module.lstm.weight_ih_l0  dot:  35563400.0    \n",
      "module.lstm.weight_hh_l0  dot:  35639888.0    \n",
      "module.lstm.bias_ih_l0  dot:  2106449.75    \n",
      "module.lstm.bias_hh_l0  dot:  2106449.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23582784.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33748.83984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  937931.9375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  937931.9375    \n",
      "module.adapter.frcn_linear.weight  dot:  41085592.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22152.580078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  386797.71875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  188.68765258789062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  270919.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.8267663765291218e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14271407.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38155.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5429.40283203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.83885955810547    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  37773.8125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.6984638402136625e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4036394.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38155.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9394.19140625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1827.738525390625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  69358.8125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5592130.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5886952.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452825.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  984/6933] Loss: -891.7097 [iq: 8.2795,ans: 8.0925,interp: 8.0849,fusion: -916.1666]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  274931.9375    \n",
      "module.ans_embedding.weight  dot:  919397.25    \n",
      "module.lstm.weight_ih_l0  dot:  4315119.0    \n",
      "module.lstm.weight_hh_l0  dot:  6768836.5    \n",
      "module.lstm.bias_ih_l0  dot:  243525.640625    \n",
      "module.lstm.bias_hh_l0  dot:  243525.640625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23212694.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24639.484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1726467.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1726467.25    \n",
      "module.adapter.frcn_linear.weight  dot:  60386504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  39865.8515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  588421.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  337.01416015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  549364.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  20771732.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  58934.1796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  22491.228515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  81.16706848144531    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  217765.703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6156226.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  58934.1796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3374.5341796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  627.9482421875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  15832.4150390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5657073.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6042084.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452825.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  985/6933] Loss: -935.2731 [iq: 8.3663,ans: 8.3021,interp: 9.1140,fusion: -961.0554]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  168669.0625    \n",
      "module.ans_embedding.weight  dot:  1193282.625    \n",
      "module.lstm.weight_ih_l0  dot:  3055058.0    \n",
      "module.lstm.weight_hh_l0  dot:  3356712.0    \n",
      "module.lstm.bias_ih_l0  dot:  265208.875    \n",
      "module.lstm.bias_hh_l0  dot:  265208.875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  28005908.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6391.36669921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2494316.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2494316.0    \n",
      "module.adapter.frcn_linear.weight  dot:  46829832.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29542.064453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  581998.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  282.732666015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  747743.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16598398.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47293.0859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7988.86279296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.758377075195312    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  88215.984375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.915943125321064e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5057987.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47293.0859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  252.5796661376953    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  40.951541900634766    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  909.8516845703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.1510792319313623e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6040195.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7148743.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452825.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  986/6933] Loss: -922.1830 [iq: 8.1056,ans: 8.5682,interp: 8.1834,fusion: -947.0402]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  251061.25    \n",
      "module.ans_embedding.weight  dot:  785027.5    \n",
      "module.lstm.weight_ih_l0  dot:  837967.75    \n",
      "module.lstm.weight_hh_l0  dot:  450134.40625    \n",
      "module.lstm.bias_ih_l0  dot:  22734.29296875    \n",
      "module.lstm.bias_hh_l0  dot:  22734.29296875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11826960.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  15541.1640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  558682.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  558682.75    \n",
      "module.adapter.frcn_linear.weight  dot:  38590268.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25195.93359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  314464.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  150.99859619140625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  228656.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12397910.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34696.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2908.45556640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.360054969787598    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  37384.1484375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7408297026122455e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5232799.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34696.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  149.48690795898438    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.96055030822754    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  528.7525024414062    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3864419.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3677979.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452826.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  987/6933] Loss: -904.5303 [iq: 8.3478,ans: 7.7991,interp: 8.8952,fusion: -929.5724]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  237135.203125    \n",
      "module.ans_embedding.weight  dot:  359312.1875    \n",
      "module.lstm.weight_ih_l0  dot:  1038477.3125    \n",
      "module.lstm.weight_hh_l0  dot:  725481.875    \n",
      "module.lstm.bias_ih_l0  dot:  27728.521484375    \n",
      "module.lstm.bias_hh_l0  dot:  27728.521484375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10523188.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  45742.0625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  697622.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  697622.625    \n",
      "module.adapter.frcn_linear.weight  dot:  44758328.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30885.08984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  365044.15625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  220.72674560546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  252452.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.330104275140911e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14711860.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42129.875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6531.9501953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.476879119873047    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  71202.0703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.037147505187022e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5546090.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42129.875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  100458.25    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12086.8740234375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  102457.96875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2789769243681803e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3917153.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4119027.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452826.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  988/6933] Loss: -933.5080 [iq: 10.0032,ans: 8.5054,interp: 8.5147,fusion: -960.5314]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  289075.875    \n",
      "module.ans_embedding.weight  dot:  1240790.375    \n",
      "module.lstm.weight_ih_l0  dot:  2924205.75    \n",
      "module.lstm.weight_hh_l0  dot:  1743412.875    \n",
      "module.lstm.bias_ih_l0  dot:  106914.3671875    \n",
      "module.lstm.bias_hh_l0  dot:  106914.3671875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19158422.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2111.85791015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  418387.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  418387.375    \n",
      "module.adapter.frcn_linear.weight  dot:  27333558.0    \n",
      "module.adapter.frcn_linear.bias  dot:  14528.478515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  398169.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  176.13632202148438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  320055.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.083201580826426e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  10347542.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  28540.27734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3701.837890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  17.087074279785156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  39329.12890625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.143885234952904e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3451523.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  28540.27734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.4301121234893799    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.0641963928937912    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1.415340542793274    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  4898852.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4034162.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452827.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  989/6933] Loss: -888.2845 [iq: 9.6587,ans: 7.5334,interp: 8.0114,fusion: -913.4880]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  182254.90625    \n",
      "module.ans_embedding.weight  dot:  552179.75    \n",
      "module.lstm.weight_ih_l0  dot:  12313473.0    \n",
      "module.lstm.weight_hh_l0  dot:  8730330.0    \n",
      "module.lstm.bias_ih_l0  dot:  784941.25    \n",
      "module.lstm.bias_hh_l0  dot:  784941.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14133294.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14024.01171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  450064.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  450064.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  51611052.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31718.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  761188.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  358.29052734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  595352.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16167108.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43480.35546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10950.732421875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  28.125717163085938    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  89147.2421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.572964477731148e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7112506.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43480.35546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  929.2242431640625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  60.03306579589844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4615.0517578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.2825296380469808e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4200789.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3385351.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452827.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  990/6933] Loss: -931.2280 [iq: 11.2880,ans: 8.8781,interp: 9.6561,fusion: -961.0502]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  19142.912109375    \n",
      "module.ans_embedding.weight  dot:  1032427.5625    \n",
      "module.lstm.weight_ih_l0  dot:  432127.125    \n",
      "module.lstm.weight_hh_l0  dot:  583978.4375    \n",
      "module.lstm.bias_ih_l0  dot:  29951.53515625    \n",
      "module.lstm.bias_hh_l0  dot:  29951.53515625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22222490.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10320.6767578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1274888.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1274888.625    \n",
      "module.adapter.frcn_linear.weight  dot:  28681416.0    \n",
      "module.adapter.frcn_linear.bias  dot:  17776.14453125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  359121.96875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  159.7044677734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  407333.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  10884454.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  30091.447265625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3493.0517578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.925620079040527    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  39418.59375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.162391072284663e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3848250.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  30091.447265625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  603.5348510742188    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  142.24012756347656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  5696.57958984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5138252.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4424243.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452827.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  991/6933] Loss: -953.1252 [iq: 7.8317,ans: 7.3698,interp: 7.5430,fusion: -975.8697]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  587047.75    \n",
      "module.ans_embedding.weight  dot:  304321.4375    \n",
      "module.lstm.weight_ih_l0  dot:  5506796.0    \n",
      "module.lstm.weight_hh_l0  dot:  6384751.5    \n",
      "module.lstm.bias_ih_l0  dot:  384409.34375    \n",
      "module.lstm.bias_hh_l0  dot:  384409.34375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12283372.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20524.390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  890184.8125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  890184.8125    \n",
      "module.adapter.frcn_linear.weight  dot:  59190608.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42718.2578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  545245.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  295.58331298828125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  522459.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  18695650.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53673.453125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6511.150390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  23.83881950378418    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  70888.921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.073172270111172e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7347049.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53673.453125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  297.03668212890625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  52.6690673828125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1066.773681640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.6843418860808015e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5608400.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4675935.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452827.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  992/6933] Loss: -930.3712 [iq: 6.4289,ans: 6.8649,interp: 7.0514,fusion: -950.7164]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2286858.5    \n",
      "module.ans_embedding.weight  dot:  459790.21875    \n",
      "module.lstm.weight_ih_l0  dot:  50726384.0    \n",
      "module.lstm.weight_hh_l0  dot:  40253304.0    \n",
      "module.lstm.bias_ih_l0  dot:  2951904.5    \n",
      "module.lstm.bias_hh_l0  dot:  2951904.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  8627904.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  29547.97265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  193688.234375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  193688.234375    \n",
      "module.adapter.frcn_linear.weight  dot:  53696288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33692.828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  753676.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  341.54681396484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  861507.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  17180100.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45905.5859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8743.384765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.90061569213867    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  97416.1171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4100720591159188e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5724766.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45905.5859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  545.4561157226562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  120.62290954589844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2070.559326171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4471121.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3396985.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452828.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  993/6933] Loss: -899.5380 [iq: 7.0402,ans: 7.4107,interp: 7.4702,fusion: -921.4591]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  300900.375    \n",
      "module.ans_embedding.weight  dot:  1505672.375    \n",
      "module.lstm.weight_ih_l0  dot:  7361866.5    \n",
      "module.lstm.weight_hh_l0  dot:  3692210.25    \n",
      "module.lstm.bias_ih_l0  dot:  455468.0625    \n",
      "module.lstm.bias_hh_l0  dot:  455468.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18610284.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  35568.27734375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  547347.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  547347.0    \n",
      "module.adapter.frcn_linear.weight  dot:  40345052.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25086.34765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  304049.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  173.95425415039062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  200715.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.114131009671837e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14328535.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41685.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2404.2919921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.865509033203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  33239.37109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.571099220309407e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5260035.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41685.9296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  9921.537109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1803.342041015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  60104.125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3901967.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3674774.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452828.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  994/6933] Loss: -876.7331 [iq: 7.6235,ans: 7.6183,interp: 10.9131,fusion: -902.8880]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  252276.859375    \n",
      "module.ans_embedding.weight  dot:  285690.625    \n",
      "module.lstm.weight_ih_l0  dot:  3275537.25    \n",
      "module.lstm.weight_hh_l0  dot:  4296149.0    \n",
      "module.lstm.bias_ih_l0  dot:  276611.25    \n",
      "module.lstm.bias_hh_l0  dot:  276611.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7227121.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12223.46484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  361958.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  361958.375    \n",
      "module.adapter.frcn_linear.weight  dot:  36649872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22736.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  351531.21875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  196.57594299316406    \n",
      "module.attflat_img.mlp.linear.weight  dot:  252695.03125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.954948286060244e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14890142.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41121.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11395.810546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  38.75630187988281    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  84820.1171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.682870814396665e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5552899.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41121.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  403.54296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  74.22384643554688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2444.9912109375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7408297026122455e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3835063.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3284765.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452829.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  995/6933] Loss: -911.5547 [iq: 7.3126,ans: 7.1868,interp: 8.7896,fusion: -934.8437]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  140159.0    \n",
      "module.ans_embedding.weight  dot:  1337337.5    \n",
      "module.lstm.weight_ih_l0  dot:  2278508.0    \n",
      "module.lstm.weight_hh_l0  dot:  1329252.25    \n",
      "module.lstm.bias_ih_l0  dot:  112393.109375    \n",
      "module.lstm.bias_hh_l0  dot:  112393.109375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  35697164.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3840.682861328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3617265.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3617265.75    \n",
      "module.adapter.frcn_linear.weight  dot:  33109056.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20490.974609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  353265.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  178.76504516601562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  250218.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  11067324.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  30300.791015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2342.26025390625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.408496856689453    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  39459.87109375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.4583536034915596e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3899622.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  30300.791015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  205.9014892578125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.500720977783203    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  580.9166870117188    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6207821.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7576969.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452830.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step  996/6933] Loss: -921.2007 [iq: 7.3362,ans: 6.5874,interp: 6.4305,fusion: -941.5548]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  90951.3828125    \n",
      "module.ans_embedding.weight  dot:  1134010.125    \n",
      "module.lstm.weight_ih_l0  dot:  1098214.625    \n",
      "module.lstm.weight_hh_l0  dot:  1203472.25    \n",
      "module.lstm.bias_ih_l0  dot:  107127.0    \n",
      "module.lstm.bias_hh_l0  dot:  107127.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14776890.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26409.583984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  747404.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  747404.25    \n",
      "module.adapter.frcn_linear.weight  dot:  23527680.0    \n",
      "module.adapter.frcn_linear.bias  dot:  14578.1337890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  155014.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  80.1129150390625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  131019.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  9211487.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  26474.63671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3795.91552734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  12.224715232849121    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  29500.322265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7408297026122455e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3560595.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  26474.63671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  693.2341918945312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  175.54681396484375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  6002.2275390625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7195134205394424e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4150151.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3981299.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452830.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  997/6933] Loss: -884.0132 [iq: 9.9110,ans: 6.8078,interp: 7.1593,fusion: -907.8913]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  231289.328125    \n",
      "module.ans_embedding.weight  dot:  284674.0625    \n",
      "module.lstm.weight_ih_l0  dot:  3102816.0    \n",
      "module.lstm.weight_hh_l0  dot:  1609262.0    \n",
      "module.lstm.bias_ih_l0  dot:  97458.59375    \n",
      "module.lstm.bias_hh_l0  dot:  97458.59375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  6531329.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4926.623046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  273138.03125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  273138.03125    \n",
      "module.adapter.frcn_linear.weight  dot:  51682944.0    \n",
      "module.adapter.frcn_linear.bias  dot:  32529.7265625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  682748.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  306.4182434082031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  559563.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  14954132.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41826.76171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11659.923828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.23841094970703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  88164.1015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.424748567544157e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7720346.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41826.76171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  315.816162109375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  91.1728515625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1828.470703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.881784197001252e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3888182.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3102297.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452831.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  998/6933] Loss: -928.8781 [iq: 11.6599,ans: 8.3934,interp: 8.1000,fusion: -957.0314]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  188267.125    \n",
      "module.ans_embedding.weight  dot:  914900.3125    \n",
      "module.lstm.weight_ih_l0  dot:  3387871.25    \n",
      "module.lstm.weight_hh_l0  dot:  1445981.5    \n",
      "module.lstm.bias_ih_l0  dot:  218907.328125    \n",
      "module.lstm.bias_hh_l0  dot:  218907.328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19537380.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10276.78515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1118271.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1118271.0    \n",
      "module.adapter.frcn_linear.weight  dot:  31847590.0    \n",
      "module.adapter.frcn_linear.bias  dot:  17562.99609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  321329.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  146.01873779296875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  220699.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.604316927725449e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  10892326.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  29759.26171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4762.013671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.0985164642334    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  53484.125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.648850441910326e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4599382.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  29759.26171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  224.71897888183594    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  45.25175476074219    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  602.0004272460938    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4335365.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4313020.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452831.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step  999/6933] Loss: -894.7367 [iq: 13.9696,ans: 9.2150,interp: 12.4777,fusion: -930.3990]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1414297.5    \n",
      "module.ans_embedding.weight  dot:  398604.6875    \n",
      "module.lstm.weight_ih_l0  dot:  25910300.0    \n",
      "module.lstm.weight_hh_l0  dot:  41577808.0    \n",
      "module.lstm.bias_ih_l0  dot:  2881811.75    \n",
      "module.lstm.bias_hh_l0  dot:  2881811.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12367910.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  28031.359375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  807620.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  807620.125    \n",
      "module.adapter.frcn_linear.weight  dot:  55768800.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37750.66796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  451693.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  255.1967315673828    \n",
      "module.attflat_img.mlp.linear.weight  dot:  399009.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  18521754.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53230.2578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12684.08203125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  69.18498229980469    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  186177.296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5710605.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53230.2578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3333.732421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  489.36309814453125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  18739.9453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4947815.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5236951.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452831.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1000/6933] Loss: -914.4260 [iq: 10.8398,ans: 9.2364,interp: 9.4660,fusion: -943.9683]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  193574.5    \n",
      "module.ans_embedding.weight  dot:  643848.375    \n",
      "module.lstm.weight_ih_l0  dot:  3878010.25    \n",
      "module.lstm.weight_hh_l0  dot:  6480350.5    \n",
      "module.lstm.bias_ih_l0  dot:  324510.0    \n",
      "module.lstm.bias_hh_l0  dot:  324510.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16139090.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  237167.90625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  886826.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  886826.5    \n",
      "module.adapter.frcn_linear.weight  dot:  38008516.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22970.62109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  523054.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  207.19705200195312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  508114.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.412207322777249e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13889686.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38272.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  12539.548828125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  48.98658752441406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  99487.4375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5508017.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38272.0078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  21802.853515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2867.153076171875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  35340.30078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.530065542800003e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4441245.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3921203.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452831.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1001/6933] Loss: -925.7971 [iq: 8.2966,ans: 7.9404,interp: 9.1868,fusion: -951.2208]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  138718.765625    \n",
      "module.ans_embedding.weight  dot:  332735.78125    \n",
      "module.lstm.weight_ih_l0  dot:  3025307.75    \n",
      "module.lstm.weight_hh_l0  dot:  1880117.0    \n",
      "module.lstm.bias_ih_l0  dot:  173029.625    \n",
      "module.lstm.bias_hh_l0  dot:  173029.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10963461.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17543.0859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  669391.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  669391.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  75888520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  51361.8203125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  893409.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  334.6617126464844    \n",
      "module.attflat_img.mlp.linear.weight  dot:  866614.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23696568.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  67178.046875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4203.4462890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.193161964416504    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  55202.875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  6.765787929907674e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  8204597.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  67178.046875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  713.9813232421875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  82.11671447753906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2532.404541015625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.877698079828406e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4426628.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4503853.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452831.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1002/6933] Loss: -927.3986 [iq: 9.1824,ans: 8.1595,interp: 9.2867,fusion: -954.0271]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  399784.875    \n",
      "module.ans_embedding.weight  dot:  406226.4375    \n",
      "module.lstm.weight_ih_l0  dot:  9729651.0    \n",
      "module.lstm.weight_hh_l0  dot:  8963166.0    \n",
      "module.lstm.bias_ih_l0  dot:  636738.5    \n",
      "module.lstm.bias_hh_l0  dot:  636738.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9716012.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  7921.6298828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  542025.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  542025.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  59839828.0    \n",
      "module.adapter.frcn_linear.bias  dot:  35153.1796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  870622.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  460.36431884765625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  839449.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19381092.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  53089.16015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3946.404296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  26.338165283203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  50273.66015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.386127430843771e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7996084.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  53089.16015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  75.36103057861328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  12.237388610839844    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  495.9713134765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.469446951953614e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4453994.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4078476.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452832.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1003/6933] Loss: -911.4133 [iq: 8.5798,ans: 8.0806,interp: 11.5070,fusion: -939.5808]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  669549.25    \n",
      "module.ans_embedding.weight  dot:  397725.75    \n",
      "module.lstm.weight_ih_l0  dot:  11555072.0    \n",
      "module.lstm.weight_hh_l0  dot:  7524834.0    \n",
      "module.lstm.bias_ih_l0  dot:  593816.0    \n",
      "module.lstm.bias_hh_l0  dot:  593816.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9216300.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  371.7193603515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  646094.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  646094.5    \n",
      "module.adapter.frcn_linear.weight  dot:  64501292.0    \n",
      "module.adapter.frcn_linear.bias  dot:  42372.2734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  691692.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  281.58013916015625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  527509.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.9467628337442875e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  19295092.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  54717.65625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4243.15234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  41.23596954345703    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  32871.2265625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.412026217120001e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6803031.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  54717.65625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  69.66879272460938    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  33.67329406738281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  59.1263427734375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4239730.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4765233.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452832.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1004/6933] Loss: -936.9852 [iq: 6.2880,ans: 6.4714,interp: 6.3451,fusion: -956.0897]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  192328.0625    \n",
      "module.ans_embedding.weight  dot:  692635.9375    \n",
      "module.lstm.weight_ih_l0  dot:  1663312.0    \n",
      "module.lstm.weight_hh_l0  dot:  1560705.75    \n",
      "module.lstm.bias_ih_l0  dot:  82628.96875    \n",
      "module.lstm.bias_hh_l0  dot:  82628.96875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11460463.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5930.07421875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  709277.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  709277.0    \n",
      "module.adapter.frcn_linear.weight  dot:  39615436.0    \n",
      "module.adapter.frcn_linear.bias  dot:  24157.515625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  309732.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  150.10397338867188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  214960.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.760781742632389e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13042494.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36401.1171875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5286.13330078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  38.76336669921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  51855.609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4769459.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36401.1171875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  98.1580810546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  75.48467254638672    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  236.00990295410156    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3851017.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4280248.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452832.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1005/6933] Loss: -932.9936 [iq: 8.6695,ans: 8.2235,interp: 8.1475,fusion: -958.0342]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  593936.0    \n",
      "module.ans_embedding.weight  dot:  818636.875    \n",
      "module.lstm.weight_ih_l0  dot:  2036879.375    \n",
      "module.lstm.weight_hh_l0  dot:  619392.8125    \n",
      "module.lstm.bias_ih_l0  dot:  91814.546875    \n",
      "module.lstm.bias_hh_l0  dot:  91814.546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11844744.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9953.341796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  270555.09375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  270555.09375    \n",
      "module.adapter.frcn_linear.weight  dot:  38533712.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20979.173828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  457615.75    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  218.60935974121094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  320286.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.1836967789568007e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  12163832.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  32644.75    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6901.7685546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  54.589324951171875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  90275.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.8267663765291218e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3921940.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  32644.75    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  51.215362548828125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  11.87900161743164    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  190.09307861328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.130118552187923e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3875748.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3065926.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452833.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1006/6933] Loss: -926.1707 [iq: 10.0082,ans: 8.1310,interp: 7.8393,fusion: -952.1492]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  280907.875    \n",
      "module.ans_embedding.weight  dot:  808617.625    \n",
      "module.lstm.weight_ih_l0  dot:  9011841.0    \n",
      "module.lstm.weight_hh_l0  dot:  10635746.0    \n",
      "module.lstm.bias_ih_l0  dot:  704715.75    \n",
      "module.lstm.bias_hh_l0  dot:  704715.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7609540.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6391.0361328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  125495.78125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  125495.78125    \n",
      "module.adapter.frcn_linear.weight  dot:  38190872.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22262.71484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  355855.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  202.438720703125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  262378.0625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.915943125321064e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  15506444.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  43212.546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  9724.4462890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  32.456459045410156    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  81268.34375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.583054184578941e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5462197.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  43212.546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  294.37969970703125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  63.987640380859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1078.930908203125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.5010215292932116e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3065161.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2188898.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452833.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1007/6933] Loss: -906.2262 [iq: 11.2466,ans: 8.1519,interp: 8.7353,fusion: -934.3600]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  132335.421875    \n",
      "module.ans_embedding.weight  dot:  935952.5    \n",
      "module.lstm.weight_ih_l0  dot:  3313711.75    \n",
      "module.lstm.weight_hh_l0  dot:  3262644.5    \n",
      "module.lstm.bias_ih_l0  dot:  211809.84375    \n",
      "module.lstm.bias_hh_l0  dot:  211809.84375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9075191.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6427.37890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  176462.921875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  176462.921875    \n",
      "module.adapter.frcn_linear.weight  dot:  30312580.0    \n",
      "module.adapter.frcn_linear.bias  dot:  16615.759765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  298002.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  143.61512756347656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  188168.921875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.0027179087046534e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  11378505.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  30613.7734375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4574.1572265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  24.79336166381836    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  59606.8515625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.801471055136062e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4560538.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  30613.7734375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  172.84266662597656    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  14.95462417602539    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1527.32421875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.115907697472721e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3516766.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2673145.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452833.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1008/6933] Loss: -949.1602 [iq: 9.7303,ans: 7.5122,interp: 7.3093,fusion: -973.7119]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  56295.19140625    \n",
      "module.ans_embedding.weight  dot:  712254.0    \n",
      "module.lstm.weight_ih_l0  dot:  1973402.0    \n",
      "module.lstm.weight_hh_l0  dot:  1965190.5    \n",
      "module.lstm.bias_ih_l0  dot:  123171.265625    \n",
      "module.lstm.bias_hh_l0  dot:  123171.265625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13081374.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  13836.6572265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  340312.0625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  340312.0625    \n",
      "module.adapter.frcn_linear.weight  dot:  39189356.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25159.92578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  264295.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  106.6371078491211    \n",
      "module.attflat_img.mlp.linear.weight  dot:  229799.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3655957193113863e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  12536124.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34691.015625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  14908.171875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.53716278076172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  139950.359375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3306475921126548e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4470196.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34691.015625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  522.444091796875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  90.32630920410156    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2296.796630859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4217121.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3121273.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452833.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1009/6933] Loss: -923.6546 [iq: 11.2934,ans: 7.8889,interp: 10.4084,fusion: -953.2454]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  179606.71875    \n",
      "module.ans_embedding.weight  dot:  567690.125    \n",
      "module.lstm.weight_ih_l0  dot:  4223781.0    \n",
      "module.lstm.weight_hh_l0  dot:  4711407.5    \n",
      "module.lstm.bias_ih_l0  dot:  338745.71875    \n",
      "module.lstm.bias_hh_l0  dot:  338745.71875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15393126.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9915.478515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1047094.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1047094.5    \n",
      "module.adapter.frcn_linear.weight  dot:  42222216.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28368.62109375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  327310.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  165.30474853515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  225529.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4190391084412113e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13248848.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38202.41796875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3769.70849609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  33.25738525390625    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  42315.98046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.978719410544727e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4309547.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38202.41796875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  34.08612823486328    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.769449234008789    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  72.18949127197266    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5294429.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5461035.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452833.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1010/6933] Loss: -969.6160 [iq: 7.7944,ans: 7.1827,interp: 7.1122,fusion: -991.7052]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  331677.4375    \n",
      "module.ans_embedding.weight  dot:  1034166.5625    \n",
      "module.lstm.weight_ih_l0  dot:  5806503.0    \n",
      "module.lstm.weight_hh_l0  dot:  5600526.0    \n",
      "module.lstm.bias_ih_l0  dot:  456367.375    \n",
      "module.lstm.bias_hh_l0  dot:  456367.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14225154.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  74079.0703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  445766.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  445766.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  42810552.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25462.654296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  669164.875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  346.9906005859375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  567208.9375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  14816019.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  42483.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8134.5478515625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  45.010257720947266    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  80746.703125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.469442036584951e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5624632.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  42483.8515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  8494.822265625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1372.5079345703125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  44443.83984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.01581023779363e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4282780.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3497148.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452834.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1011/6933] Loss: -858.2361 [iq: 9.5286,ans: 9.4507,interp: 10.2190,fusion: -887.4345]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  643579.5625    \n",
      "module.ans_embedding.weight  dot:  1027594.0    \n",
      "module.lstm.weight_ih_l0  dot:  12928328.0    \n",
      "module.lstm.weight_hh_l0  dot:  9465942.0    \n",
      "module.lstm.bias_ih_l0  dot:  712383.0    \n",
      "module.lstm.bias_hh_l0  dot:  712383.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  31656432.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  73757.09375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3530364.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3530364.5    \n",
      "module.adapter.frcn_linear.weight  dot:  51696968.0    \n",
      "module.adapter.frcn_linear.bias  dot:  36826.6015625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  439928.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  210.37417602539062    \n",
      "module.attflat_img.mlp.linear.weight  dot:  384977.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17504812.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  50438.08984375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10276.0615234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  51.0461540222168    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  149570.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5133410.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  50438.08984375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7920.93896484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1312.8111572265625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  37603.453125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5940185.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  8339737.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452834.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1012/6933] Loss: -949.6901 [iq: 7.0211,ans: 7.5778,interp: 9.1779,fusion: -973.4669]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  279456.5    \n",
      "module.ans_embedding.weight  dot:  725057.8125    \n",
      "module.lstm.weight_ih_l0  dot:  4253082.5    \n",
      "module.lstm.weight_hh_l0  dot:  2828960.75    \n",
      "module.lstm.bias_ih_l0  dot:  225237.0625    \n",
      "module.lstm.bias_hh_l0  dot:  225237.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  14192377.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17178.5625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  764619.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  764619.25    \n",
      "module.adapter.frcn_linear.weight  dot:  63095072.0    \n",
      "module.adapter.frcn_linear.bias  dot:  46319.140625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  543893.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  221.673583984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  343458.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.572964477731148e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19324348.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  56933.94140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2836.43994140625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  11.16100788116455    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  28175.990234375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.68839164871315e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5394706.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  56933.94140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  18.73278045654297    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  5.757651329040527    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  63.72981262207031    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.604316927725449e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4794568.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5117769.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452834.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1013/6933] Loss: -921.5898 [iq: 7.7788,ans: 7.6788,interp: 7.6337,fusion: -944.6812]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1368746.25    \n",
      "module.ans_embedding.weight  dot:  719161.0    \n",
      "module.lstm.weight_ih_l0  dot:  12393661.0    \n",
      "module.lstm.weight_hh_l0  dot:  4736887.0    \n",
      "module.lstm.bias_ih_l0  dot:  733159.3125    \n",
      "module.lstm.bias_hh_l0  dot:  733159.3125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16174799.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  24803.171875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  798596.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  798596.375    \n",
      "module.adapter.frcn_linear.weight  dot:  50765320.0    \n",
      "module.adapter.frcn_linear.bias  dot:  29952.419921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  460176.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  222.35049438476562    \n",
      "module.attflat_img.mlp.linear.weight  dot:  352832.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-12    \n",
      "module.attflat_img.linear_merge.weight  dot:  17887980.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48342.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  17400.705078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  55.072532653808594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  144289.53125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.1391778065881226e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6975253.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48342.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  197.71145629882812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  30.708938598632812    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  402.74090576171875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4887398.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5154005.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452834.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1014/6933] Loss: -891.1793 [iq: 7.9478,ans: 8.2349,interp: 10.2048,fusion: -917.5668]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  123597.96875    \n",
      "module.ans_embedding.weight  dot:  685061.125    \n",
      "module.lstm.weight_ih_l0  dot:  1508858.75    \n",
      "module.lstm.weight_hh_l0  dot:  649530.8125    \n",
      "module.lstm.bias_ih_l0  dot:  58428.046875    \n",
      "module.lstm.bias_hh_l0  dot:  58428.046875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9542786.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  21153.80859375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  532103.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  532103.625    \n",
      "module.adapter.frcn_linear.weight  dot:  41133916.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22641.1484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  711792.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  291.733642578125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  595469.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13519554.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  36948.63671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4190.26953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  25.004411697387695    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  54648.16796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.238689482212067e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4673446.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  36948.63671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  233.81561279296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  73.76217651367188    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  402.34307861328125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.752553823233029e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3525709.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4101073.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452834.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1015/6933] Loss: -896.7360 [iq: 8.8286,ans: 8.1921,interp: 8.4565,fusion: -922.2133]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  83419.875    \n",
      "module.ans_embedding.weight  dot:  232320.109375    \n",
      "module.lstm.weight_ih_l0  dot:  2561479.0    \n",
      "module.lstm.weight_hh_l0  dot:  2717750.75    \n",
      "module.lstm.bias_ih_l0  dot:  172327.78125    \n",
      "module.lstm.bias_hh_l0  dot:  172327.78125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  5750828.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2201.467041015625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  176343.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  176343.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  40996940.0    \n",
      "module.adapter.frcn_linear.bias  dot:  25036.80078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  341381.125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  159.96420288085938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  266973.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  8.208189683500677e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15985683.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44040.45703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3376.7021484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.926145553588867    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45645.10546875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0267342531733448e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5349976.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44040.45703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  7.294190406799316    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.2179195880889893    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  71.52078247070312    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.996003610813204e-16    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3731182.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2515945.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452835.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1016/6933] Loss: -941.7927 [iq: 8.3714,ans: 7.9770,interp: 7.7443,fusion: -965.8854]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  688268.0    \n",
      "module.ans_embedding.weight  dot:  531792.375    \n",
      "module.lstm.weight_ih_l0  dot:  25060832.0    \n",
      "module.lstm.weight_hh_l0  dot:  16310874.0    \n",
      "module.lstm.bias_ih_l0  dot:  1511200.0    \n",
      "module.lstm.bias_hh_l0  dot:  1511200.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12107174.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  47706.4921875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  593903.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  593903.5    \n",
      "module.adapter.frcn_linear.weight  dot:  45083400.0    \n",
      "module.adapter.frcn_linear.bias  dot:  30194.923828125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  384065.0    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  181.930419921875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  303024.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.630216375924647e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  17108094.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  48613.6875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2924.54931640625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  14.637995719909668    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  36603.9140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.963265271522687e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6653723.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  48613.6875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  387.20660400390625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  70.98384857177734    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1234.7369384765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  8.185452315956354e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4993049.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4067116.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452835.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1017/6933] Loss: -963.6051 [iq: 7.9987,ans: 7.2629,interp: 7.3140,fusion: -986.1807]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1434103.875    \n",
      "module.ans_embedding.weight  dot:  675854.375    \n",
      "module.lstm.weight_ih_l0  dot:  23656028.0    \n",
      "module.lstm.weight_hh_l0  dot:  7297703.5    \n",
      "module.lstm.bias_ih_l0  dot:  1519479.625    \n",
      "module.lstm.bias_hh_l0  dot:  1519479.625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10107402.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  14118.28515625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  278369.34375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  278369.34375    \n",
      "module.adapter.frcn_linear.weight  dot:  47495952.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28679.7890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  566332.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  239.1256561279297    \n",
      "module.attflat_img.mlp.linear.weight  dot:  398542.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.6520865503698587e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14726326.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40064.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8228.017578125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  130.7094268798828    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  79565.25    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0412804840598255e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4988189.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40064.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  574.5454711914062    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  105.948486328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1451.518310546875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4815368.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3257079.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452836.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1018/6933] Loss: -950.8344 [iq: 9.1103,ans: 7.7647,interp: 7.8972,fusion: -975.6066]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  145224.875    \n",
      "module.ans_embedding.weight  dot:  708488.375    \n",
      "module.lstm.weight_ih_l0  dot:  3855553.25    \n",
      "module.lstm.weight_hh_l0  dot:  2316951.75    \n",
      "module.lstm.bias_ih_l0  dot:  233875.734375    \n",
      "module.lstm.bias_hh_l0  dot:  233875.734375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19115662.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  75872.5078125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1005755.5625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1005755.5625    \n",
      "module.adapter.frcn_linear.weight  dot:  34392780.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19960.740234375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  292403.625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  162.8705596923828    \n",
      "module.attflat_img.mlp.linear.weight  dot:  274033.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.0108716348186135e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12033380.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  32176.99609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6279.4912109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  41.694053649902344    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  72138.609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.2290968243178213e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5200650.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  32176.99609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  581.4359130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  219.5903778076172    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2229.305419921875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5430989.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5201902.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452836.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1019/6933] Loss: -904.8481 [iq: 10.1755,ans: 8.4389,interp: 8.0334,fusion: -931.4958]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  605700.125    \n",
      "module.ans_embedding.weight  dot:  359885.1875    \n",
      "module.lstm.weight_ih_l0  dot:  5154785.0    \n",
      "module.lstm.weight_hh_l0  dot:  2610160.0    \n",
      "module.lstm.bias_ih_l0  dot:  262346.0    \n",
      "module.lstm.bias_hh_l0  dot:  262346.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  8663304.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8216.5234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  503481.46875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  503481.46875    \n",
      "module.adapter.frcn_linear.weight  dot:  45343704.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28239.6484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  471346.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  273.83935546875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  349723.375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.637978807091713e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14925804.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41239.0078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8272.470703125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  59.011375427246094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  119961.0859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5942553.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41239.0078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  111.68052673339844    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  16.437667846679688    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  568.931640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  3891628.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3872277.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452836.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1020/6933] Loss: -942.0566 [iq: 8.1235,ans: 7.8722,interp: 7.7396,fusion: -965.7918]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  162300.375    \n",
      "module.ans_embedding.weight  dot:  702468.875    \n",
      "module.lstm.weight_ih_l0  dot:  2124065.5    \n",
      "module.lstm.weight_hh_l0  dot:  2681574.0    \n",
      "module.lstm.bias_ih_l0  dot:  150649.4375    \n",
      "module.lstm.bias_hh_l0  dot:  150649.4375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  13433738.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2818.5947265625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1038294.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1038294.375    \n",
      "module.adapter.frcn_linear.weight  dot:  26368520.0    \n",
      "module.adapter.frcn_linear.bias  dot:  15239.4775390625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  199835.828125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  107.6738510131836    \n",
      "module.attflat_img.mlp.linear.weight  dot:  147369.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.81122697237879e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  10832704.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  29803.44921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3131.211669921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  15.320564270019531    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  45317.6953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.106937012693379e-12    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3781339.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  29803.44921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  10.600483894348145    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2.1334288120269775    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  25.876327514648438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.9984014443252818e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4423957.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4869605.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452836.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1021/6933] Loss: -917.2645 [iq: 8.9065,ans: 7.9158,interp: 8.0764,fusion: -942.1633]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  335125.9375    \n",
      "module.ans_embedding.weight  dot:  449384.625    \n",
      "module.lstm.weight_ih_l0  dot:  4337663.0    \n",
      "module.lstm.weight_hh_l0  dot:  4987962.5    \n",
      "module.lstm.bias_ih_l0  dot:  324972.375    \n",
      "module.lstm.bias_hh_l0  dot:  324972.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22739260.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  195709.671875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2059592.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2059592.75    \n",
      "module.adapter.frcn_linear.weight  dot:  33348548.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21372.875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  203918.90625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  126.99641418457031    \n",
      "module.attflat_img.mlp.linear.weight  dot:  145205.140625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.283275873400271e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12531080.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34861.8359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2248.4990234375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  13.058189392089844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  28662.734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9453239019640023e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3768124.75    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34861.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  14336.708984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  2394.88330078125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  79114.0859375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.412026217120001e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  6205150.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6663779.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452837.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1022/6933] Loss: -941.1481 [iq: 7.7987,ans: 7.7250,interp: 7.7856,fusion: -964.4573]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  42421236.0    \n",
      "module.ans_embedding.weight  dot:  405004.09375    \n",
      "module.lstm.weight_ih_l0  dot:  408101856.0    \n",
      "module.lstm.weight_hh_l0  dot:  31565412.0    \n",
      "module.lstm.bias_ih_l0  dot:  22947112.0    \n",
      "module.lstm.bias_hh_l0  dot:  22947112.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  4949509.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  20542.37890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  167609.34375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  167609.34375    \n",
      "module.adapter.frcn_linear.weight  dot:  34499376.0    \n",
      "module.adapter.frcn_linear.bias  dot:  21058.2890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  430031.78125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  200.8515625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  344008.34375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2450982467271388e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13436636.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38152.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  15962.359375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  134.64718627929688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  74010.9609375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0051067533822788e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5246790.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38152.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  228.71728515625    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  47.770545959472656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  495.4244384765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.7985612998927536e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3859618.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2993409.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452837.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1023/6933] Loss: -954.0166 [iq: 7.1942,ans: 7.4032,interp: 7.9400,fusion: -976.5540]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  72382.84375    \n",
      "module.ans_embedding.weight  dot:  1277347.5    \n",
      "module.lstm.weight_ih_l0  dot:  2078346.25    \n",
      "module.lstm.weight_hh_l0  dot:  2141934.75    \n",
      "module.lstm.bias_ih_l0  dot:  124412.421875    \n",
      "module.lstm.bias_hh_l0  dot:  124412.421875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17504456.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  9416.056640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  911556.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  911556.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  38650076.0    \n",
      "module.adapter.frcn_linear.bias  dot:  24860.396484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  606526.5    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  277.307373046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  494418.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.2741809263825417e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  14367386.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40783.140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5668.0537109375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  23.332496643066406    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  37271.51953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.5067947717616335e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  7145162.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40783.140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  125.65534210205078    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  21.287670135498047    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1115.487548828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4892978.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4290163.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452838.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1024/6933] Loss: -912.8046 [iq: 7.7342,ans: 7.6249,interp: 8.3616,fusion: -936.5253]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  681333.75    \n",
      "module.ans_embedding.weight  dot:  655066.125    \n",
      "module.lstm.weight_ih_l0  dot:  13435821.0    \n",
      "module.lstm.weight_hh_l0  dot:  7409469.0    \n",
      "module.lstm.bias_ih_l0  dot:  398303.25    \n",
      "module.lstm.bias_hh_l0  dot:  398303.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11293864.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  78802.609375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  437899.4375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  437899.4375    \n",
      "module.adapter.frcn_linear.weight  dot:  35939300.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22658.890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  295788.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  144.95204162597656    \n",
      "module.attflat_img.mlp.linear.weight  dot:  232519.28125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1004885891452432e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12477502.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  35582.5546875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5806.7802734375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.800193786621094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  59138.4296875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.707203515863512e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4606427.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  35582.5546875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1532.7022705078125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  405.71624755859375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4223.67529296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.197442310920451e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3597290.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2566183.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452838.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1025/6933] Loss: -901.1910 [iq: 7.9860,ans: 7.7311,interp: 8.3648,fusion: -925.2730]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  31036518.0    \n",
      "module.ans_embedding.weight  dot:  1367896.25    \n",
      "module.lstm.weight_ih_l0  dot:  308695232.0    \n",
      "module.lstm.weight_hh_l0  dot:  37110216.0    \n",
      "module.lstm.bias_ih_l0  dot:  20217560.0    \n",
      "module.lstm.bias_hh_l0  dot:  20217560.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  17040114.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  5320.93896484375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  174458.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  174458.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  47025344.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27508.271484375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  454285.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  238.77960205078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  345980.5    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.094947017729282e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  16750849.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  45015.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  60103.40625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  415.21044921875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  194245.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.65288599368796e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5885647.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  45015.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  55.17366409301758    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.034568786621094    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  367.47491455078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5323016.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3322728.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452839.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1026/6933] Loss: -921.8308 [iq: 7.8652,ans: 7.7198,interp: 8.1721,fusion: -945.5879]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  185776.78125    \n",
      "module.ans_embedding.weight  dot:  1640827.875    \n",
      "module.lstm.weight_ih_l0  dot:  2593902.5    \n",
      "module.lstm.weight_hh_l0  dot:  1338068.0    \n",
      "module.lstm.bias_ih_l0  dot:  153645.53125    \n",
      "module.lstm.bias_hh_l0  dot:  153645.53125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  46453672.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  8401.0517578125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  5292015.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  5292015.0    \n",
      "module.adapter.frcn_linear.weight  dot:  25881548.0    \n",
      "module.adapter.frcn_linear.bias  dot:  15008.677734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  325097.8125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  194.46319580078125    \n",
      "module.attflat_img.mlp.linear.weight  dot:  324899.6875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.70142663794104e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  9950286.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  28481.8515625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  1989.3914794921875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  7.747490882873535    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  24681.51953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4784973245696165e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3844889.25    \n",
      "module.attflat_lang.linear_merge.bias  dot:  28481.8515625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  153.42971801757812    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  18.051149368286133    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  713.6014404296875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  7376813.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9813421.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452839.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1027/6933] Loss: -916.3613 [iq: 6.1120,ans: 6.3051,interp: 6.8763,fusion: -935.6547]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  102591.90625    \n",
      "module.ans_embedding.weight  dot:  1000222.6875    \n",
      "module.lstm.weight_ih_l0  dot:  962089.0625    \n",
      "module.lstm.weight_hh_l0  dot:  823988.875    \n",
      "module.lstm.bias_ih_l0  dot:  41967.10546875    \n",
      "module.lstm.bias_hh_l0  dot:  41967.10546875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  16068948.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17202.109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1693920.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1693920.125    \n",
      "module.adapter.frcn_linear.weight  dot:  31407044.0    \n",
      "module.adapter.frcn_linear.bias  dot:  18765.4375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  278557.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  144.75753784179688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  236593.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  11836059.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  33593.95703125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3207.1083984375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  30.439592361450195    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  38336.9453125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  7.51754214434186e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4223089.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  33593.95703125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  109.10948944091797    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  43.60199737548828    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  190.4495391845703    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.266986929404084e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4294044.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4736587.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452840.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1028/6933] Loss: -927.6677 [iq: 7.3332,ans: 6.6845,interp: 6.8615,fusion: -948.5469]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  178023.453125    \n",
      "module.ans_embedding.weight  dot:  744475.9375    \n",
      "module.lstm.weight_ih_l0  dot:  4204976.0    \n",
      "module.lstm.weight_hh_l0  dot:  714282.3125    \n",
      "module.lstm.bias_ih_l0  dot:  25794.490234375    \n",
      "module.lstm.bias_hh_l0  dot:  25794.490234375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10669941.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  23876.9375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  925677.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  925677.5    \n",
      "module.adapter.frcn_linear.weight  dot:  22225484.0    \n",
      "module.adapter.frcn_linear.bias  dot:  12535.1259765625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  448474.3125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  198.0898895263672    \n",
      "module.attflat_img.mlp.linear.weight  dot:  375557.71875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  8546803.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  23843.078125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6468.080078125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  27.369892120361328    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  83937.421875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8366144161063858e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  2935816.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  23843.078125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1364.33935546875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  334.3985900878906    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  9056.056640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4345597.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3340518.75    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452840.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1029/6933] Loss: -908.6672 [iq: 7.0554,ans: 6.3886,interp: 5.9867,fusion: -928.0979]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  295421.0625    \n",
      "module.ans_embedding.weight  dot:  602877.125    \n",
      "module.lstm.weight_ih_l0  dot:  3866873.5    \n",
      "module.lstm.weight_hh_l0  dot:  1854089.5    \n",
      "module.lstm.bias_ih_l0  dot:  185492.5    \n",
      "module.lstm.bias_hh_l0  dot:  185492.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9187873.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6009.0185546875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  163520.078125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  163520.078125    \n",
      "module.adapter.frcn_linear.weight  dot:  39080744.0    \n",
      "module.adapter.frcn_linear.bias  dot:  22603.701171875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  380773.34375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  197.88470458984375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  367630.625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.2960867934452835e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  13749798.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37877.8671875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6210.0498046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  29.100997924804688    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  58629.2890625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5058272.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37877.8671875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  918.1638793945312    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  197.28797912597656    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  3305.8642578125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.566835632933362e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3500637.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2982077.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452840.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1030/6933] Loss: -888.9178 [iq: 10.1879,ans: 8.8249,interp: 8.8949,fusion: -916.8254]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1602152.125    \n",
      "module.ans_embedding.weight  dot:  650488.75    \n",
      "module.lstm.weight_ih_l0  dot:  22125964.0    \n",
      "module.lstm.weight_hh_l0  dot:  3184300.5    \n",
      "module.lstm.bias_ih_l0  dot:  1264334.125    \n",
      "module.lstm.bias_hh_l0  dot:  1264334.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  7704910.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  19515.337890625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  137394.53125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  137394.53125    \n",
      "module.adapter.frcn_linear.weight  dot:  31012848.0    \n",
      "module.adapter.frcn_linear.bias  dot:  16940.80859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  438047.4375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  170.77493286132812    \n",
      "module.attflat_img.mlp.linear.weight  dot:  331568.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  9.313225746154785e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  11011822.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  31003.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  3914.839111328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  19.58458709716797    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  40860.953125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.856151119587594e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5038583.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  31003.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  4591.921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  896.457763671875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  23828.84375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  9.094947017729282e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3820109.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2519229.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452841.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1031/6933] Loss: -935.6561 [iq: 9.0184,ans: 7.5193,interp: 7.4923,fusion: -959.6861]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1159501.5    \n",
      "module.ans_embedding.weight  dot:  650403.4375    \n",
      "module.lstm.weight_ih_l0  dot:  60661084.0    \n",
      "module.lstm.weight_hh_l0  dot:  47401984.0    \n",
      "module.lstm.bias_ih_l0  dot:  3891570.5    \n",
      "module.lstm.bias_hh_l0  dot:  3891570.5    \n",
      "module.ans_lstm.weight_ih_l0  dot:  9725990.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17076.181640625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  330720.375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  330720.375    \n",
      "module.adapter.frcn_linear.weight  dot:  31805436.0    \n",
      "module.adapter.frcn_linear.bias  dot:  17961.13671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  445404.25    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  195.36849975585938    \n",
      "module.attflat_img.mlp.linear.weight  dot:  351279.96875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.3283064365386963e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13400883.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  37271.55859375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6407.45654296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  23.950342178344727    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  65637.1640625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.176800878965878e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4749688.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  37271.55859375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  173.48800659179688    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  23.576765060424805    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1106.694580078125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.0463630789890885e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4102058.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3225596.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452841.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1032/6933] Loss: -903.6978 [iq: 8.3874,ans: 7.7181,interp: 7.5938,fusion: -927.3971]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  167420.25    \n",
      "module.ans_embedding.weight  dot:  1104889.75    \n",
      "module.lstm.weight_ih_l0  dot:  3581422.5    \n",
      "module.lstm.weight_hh_l0  dot:  3024548.5    \n",
      "module.lstm.bias_ih_l0  dot:  245135.953125    \n",
      "module.lstm.bias_hh_l0  dot:  245135.953125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  19776422.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  3314.394775390625    \n",
      "module.ans_lstm.bias_ih_l0  dot:  752185.25    \n",
      "module.ans_lstm.bias_hh_l0  dot:  752185.25    \n",
      "module.adapter.frcn_linear.weight  dot:  32757652.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20978.18359375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  254958.453125    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  142.3050537109375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  218657.3125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.5370460459962487e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14488228.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  41602.609375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  10046.6748046875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.775535583496094    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  94791.015625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.1746159600534156e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5157115.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  41602.609375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  23.733509063720703    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  1.995497703552246    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  94.32366180419922    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4714777.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4103553.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452841.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1033/6933] Loss: -891.3387 [iq: 8.1890,ans: 7.9978,interp: 8.2026,fusion: -915.7281]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1011759.0625    \n",
      "module.ans_embedding.weight  dot:  380553.25    \n",
      "module.lstm.weight_ih_l0  dot:  14359218.0    \n",
      "module.lstm.weight_hh_l0  dot:  6355657.5    \n",
      "module.lstm.bias_ih_l0  dot:  702960.1875    \n",
      "module.lstm.bias_hh_l0  dot:  702960.1875    \n",
      "module.ans_lstm.weight_ih_l0  dot:  6620549.5    \n",
      "module.ans_lstm.weight_hh_l0  dot:  4752.79736328125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  323522.875    \n",
      "module.ans_lstm.bias_hh_l0  dot:  323522.875    \n",
      "module.adapter.frcn_linear.weight  dot:  35543696.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19121.388671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  398439.90625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  195.75436401367188    \n",
      "module.attflat_img.mlp.linear.weight  dot:  305115.65625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  7.130438461899757e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  13103084.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34973.7421875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11694.966796875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  75.95167541503906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  114669.9375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  9.459313332627062e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4836850.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34973.7421875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  67.3939208984375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  13.29688835144043    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  117.97997283935547    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  3528454.25    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2808490.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452841.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1034/6933] Loss: -918.7491 [iq: 7.6703,ans: 7.9849,interp: 8.8523,fusion: -943.2566]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  28162372.0    \n",
      "module.ans_embedding.weight  dot:  1260015.0    \n",
      "module.lstm.weight_ih_l0  dot:  235727488.0    \n",
      "module.lstm.weight_hh_l0  dot:  16956538.0    \n",
      "module.lstm.bias_ih_l0  dot:  14142128.0    \n",
      "module.lstm.bias_hh_l0  dot:  14142128.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  23534892.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  18325.62109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1322404.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1322404.0    \n",
      "module.adapter.frcn_linear.weight  dot:  32022160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  19481.212890625    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  369299.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  181.0832061767578    \n",
      "module.attflat_img.mlp.linear.weight  dot:  361256.8125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.322590368450619e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  11683368.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  31931.98828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  66452.25    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  372.5312805175781    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  178175.21875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.8969147081170377e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  3983724.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  31931.98828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1308.1790771484375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  269.07366943359375    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  2941.157470703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.508893258758917e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5283945.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5551678.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452842.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1035/6933] Loss: -885.2386 [iq: 5.9224,ans: 6.2279,interp: 6.8244,fusion: -904.2134]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  700821.125    \n",
      "module.ans_embedding.weight  dot:  771492.8125    \n",
      "module.lstm.weight_ih_l0  dot:  5805897.0    \n",
      "module.lstm.weight_hh_l0  dot:  1060639.0    \n",
      "module.lstm.bias_ih_l0  dot:  105574.859375    \n",
      "module.lstm.bias_hh_l0  dot:  105574.859375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  18741992.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17451.0703125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1452498.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1452498.75    \n",
      "module.adapter.frcn_linear.weight  dot:  52894360.0    \n",
      "module.adapter.frcn_linear.bias  dot:  31843.5078125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  980670.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  402.5947265625    \n",
      "module.attflat_img.mlp.linear.weight  dot:  766977.4375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_img.linear_merge.weight  dot:  17054788.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  46524.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4097.322265625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  20.350788116455078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  42565.48828125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.70142663794104e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5060909.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  46524.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1739.617919921875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  392.4136047363281    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  16295.353515625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  2.2737367544323206e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4638858.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5041315.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452842.625\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1036/6933] Loss: -899.0144 [iq: 7.8957,ans: 7.5676,interp: 8.1081,fusion: -922.5858]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  322317.625    \n",
      "module.ans_embedding.weight  dot:  437679.96875    \n",
      "module.lstm.weight_ih_l0  dot:  4909859.5    \n",
      "module.lstm.weight_hh_l0  dot:  8552814.0    \n",
      "module.lstm.bias_ih_l0  dot:  539463.0625    \n",
      "module.lstm.bias_hh_l0  dot:  539463.0625    \n",
      "module.ans_lstm.weight_ih_l0  dot:  6644112.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2960.3466796875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  130071.40625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  130071.40625    \n",
      "module.adapter.frcn_linear.weight  dot:  25730624.0    \n",
      "module.adapter.frcn_linear.bias  dot:  15469.67578125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  303915.1875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  150.25604248046875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  236707.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4551915228366852e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  11087676.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  30432.1640625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5182.5849609375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  45.784393310546875    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  64885.44921875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.4930279235159105e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4090204.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  30432.1640625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  407.1612854003906    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  32.314117431640625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1878.7454833984375    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  3195914.75    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2669845.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452843.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1037/6933] Loss: -922.7322 [iq: 8.1971,ans: 7.6862,interp: 8.7130,fusion: -947.3285]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  71165600.0    \n",
      "module.ans_embedding.weight  dot:  448375.25    \n",
      "module.lstm.weight_ih_l0  dot:  668559680.0    \n",
      "module.lstm.weight_hh_l0  dot:  57813792.0    \n",
      "module.lstm.bias_ih_l0  dot:  38285876.0    \n",
      "module.lstm.bias_hh_l0  dot:  38285876.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  12917215.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2937.53125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  441202.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  441202.125    \n",
      "module.adapter.frcn_linear.weight  dot:  59612176.0    \n",
      "module.adapter.frcn_linear.bias  dot:  40811.671875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  694756.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  325.08929443359375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  614376.0    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.1141310096718371e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  19562240.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  55326.296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4257.7314453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  21.404644012451172    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  35050.1796875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.3648104868480004e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6712503.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  55326.296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  48.28296661376953    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.741972923278809    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  173.48397827148438    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  5.684341886080802e-14    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5182319.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4372404.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452843.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1038/6933] Loss: -925.9182 [iq: 9.5324,ans: 8.7510,interp: 8.3489,fusion: -952.5505]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  152494.171875    \n",
      "module.ans_embedding.weight  dot:  544327.6875    \n",
      "module.lstm.weight_ih_l0  dot:  1070478.0    \n",
      "module.lstm.weight_hh_l0  dot:  658584.375    \n",
      "module.lstm.bias_ih_l0  dot:  31027.6328125    \n",
      "module.lstm.bias_hh_l0  dot:  31027.6328125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  15678517.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33782.3984375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1044748.125    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1044748.125    \n",
      "module.adapter.frcn_linear.weight  dot:  40684160.0    \n",
      "module.adapter.frcn_linear.bias  dot:  27453.05859375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  326200.84375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  165.68426513671875    \n",
      "module.attflat_img.mlp.linear.weight  dot:  249761.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.9122126104775816e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  14143307.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  39349.7578125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  4925.251953125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  60.12287521362305    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  36323.27734375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  8.74024408403784e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4683445.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  39349.7578125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  69850.78125    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  7950.1611328125    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  71567.1953125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.4210854715202004e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4458715.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4547362.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452843.75\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1039/6933] Loss: -884.3624 [iq: 10.2347,ans: 8.6839,interp: 8.4204,fusion: -911.7014]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  275908.21875    \n",
      "module.ans_embedding.weight  dot:  630005.875    \n",
      "module.lstm.weight_ih_l0  dot:  1700641.0    \n",
      "module.lstm.weight_hh_l0  dot:  557982.625    \n",
      "module.lstm.bias_ih_l0  dot:  49254.98828125    \n",
      "module.lstm.bias_hh_l0  dot:  49254.98828125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  11867100.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  33406.65234375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  577176.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  577176.5    \n",
      "module.adapter.frcn_linear.weight  dot:  33123562.0    \n",
      "module.adapter.frcn_linear.bias  dot:  18992.08984375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  383745.375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  181.03622436523438    \n",
      "module.attflat_img.mlp.linear.weight  dot:  305191.1875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.148184183984995e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12574702.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  34668.44921875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  2948.521484375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  17.506126403808594    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  47956.67578125    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.383195862876164e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4278863.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  34668.44921875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  1490.109130859375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  428.623291015625    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4128.828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.6984638402136625e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4374812.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  4517594.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452844.0\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1040/6933] Loss: -894.9489 [iq: 9.4573,ans: 8.6675,interp: 8.9594,fusion: -922.0330]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2462854.75    \n",
      "module.ans_embedding.weight  dot:  1082109.75    \n",
      "module.lstm.weight_ih_l0  dot:  28792640.0    \n",
      "module.lstm.weight_hh_l0  dot:  23619810.0    \n",
      "module.lstm.bias_ih_l0  dot:  2425760.0    \n",
      "module.lstm.bias_hh_l0  dot:  2425760.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  36004832.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  26798.08203125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  3239105.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  3239105.0    \n",
      "module.adapter.frcn_linear.weight  dot:  42952048.0    \n",
      "module.adapter.frcn_linear.bias  dot:  26312.044921875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  643286.6875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  217.50857543945312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  597501.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  5.820766091346741e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  15052652.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  40138.9140625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5940.654296875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  36.67862319946289    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  42068.3046875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.8213199837191496e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4926560.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  40138.9140625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  336.72601318359375    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  65.65265655517578    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  1777.253173828125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6641665.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  9224442.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452844.125\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1041/6933] Loss: -895.6915 [iq: 6.9869,ans: 6.7912,interp: 6.4112,fusion: -915.8807]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  676223.125    \n",
      "module.ans_embedding.weight  dot:  546884.0625    \n",
      "module.lstm.weight_ih_l0  dot:  44474740.0    \n",
      "module.lstm.weight_hh_l0  dot:  25662930.0    \n",
      "module.lstm.bias_ih_l0  dot:  2202502.25    \n",
      "module.lstm.bias_hh_l0  dot:  2202502.25    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10496146.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  10303.73046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  385246.15625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  385246.15625    \n",
      "module.adapter.frcn_linear.weight  dot:  55228672.0    \n",
      "module.adapter.frcn_linear.bias  dot:  33298.734375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  677402.9375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  355.1893005371094    \n",
      "module.attflat_img.mlp.linear.weight  dot:  433154.125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  6.030518306943122e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  23207414.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  60809.21875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16665.212890625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  64.212158203125    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  122333.0625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.370246304257307e-10    \n",
      "module.attflat_lang.linear_merge.weight  dot:  9924162.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  60809.21875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  121.41785430908203    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  35.056114196777344    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  556.16259765625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  4.733102798581967e-12    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4480918.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2924438.25    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452844.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1042/6933] Loss: -945.0575 [iq: 8.1364,ans: 8.2058,interp: 8.1306,fusion: -969.5303]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  414616.9375    \n",
      "module.ans_embedding.weight  dot:  1093799.0    \n",
      "module.lstm.weight_ih_l0  dot:  25129084.0    \n",
      "module.lstm.weight_hh_l0  dot:  19610044.0    \n",
      "module.lstm.bias_ih_l0  dot:  1606265.125    \n",
      "module.lstm.bias_hh_l0  dot:  1606265.125    \n",
      "module.ans_lstm.weight_ih_l0  dot:  26436206.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  50997.109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  1978336.625    \n",
      "module.ans_lstm.bias_hh_l0  dot:  1978336.625    \n",
      "module.adapter.frcn_linear.weight  dot:  60882636.0    \n",
      "module.adapter.frcn_linear.bias  dot:  43524.0703125    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  359338.84375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  180.83279418945312    \n",
      "module.attflat_img.mlp.linear.weight  dot:  376427.5625    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.051375875249505e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  22760036.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  64888.03125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  16198.7880859375    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  41.80263137817383    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  100571.859375    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  2.70142663794104e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  10272668.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  64888.03125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  3020.01904296875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  872.85546875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  19039.095703125    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  3.624123223744391e-11    \n",
      "module.attflat_ans.linear_merge.weight  dot:  5079495.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  6919451.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452844.375\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1043/6933] Loss: -831.1761 [iq: 8.4731,ans: 8.5302,interp: 8.8236,fusion: -857.0031]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  2415438.25    \n",
      "module.ans_embedding.weight  dot:  437807.8125    \n",
      "module.lstm.weight_ih_l0  dot:  40404692.0    \n",
      "module.lstm.weight_hh_l0  dot:  37529552.0    \n",
      "module.lstm.bias_ih_l0  dot:  2032286.75    \n",
      "module.lstm.bias_hh_l0  dot:  2032286.75    \n",
      "module.ans_lstm.weight_ih_l0  dot:  10132153.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  12212.498046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  380913.84375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  380913.84375    \n",
      "module.adapter.frcn_linear.weight  dot:  36558220.0    \n",
      "module.adapter.frcn_linear.bias  dot:  20652.91796875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  459473.59375    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  221.82247924804688    \n",
      "module.attflat_img.mlp.linear.weight  dot:  420774.78125    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.4734382602910046e-09    \n",
      "module.attflat_img.linear_merge.weight  dot:  14772886.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  38279.9296875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  6919.12060546875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  44.566261291503906    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  71649.71875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  5.875463671145553e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  5502869.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  38279.9296875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  16.651493072509766    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  4.444397926330566    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  51.150299072265625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  1.3877787807814457e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4397774.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  3284762.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452844.875\n",
      "\n",
      "----------  MAIN LOSS  --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Version hakku][Epoch  1][Step 1044/6933] Loss: -956.4188 [iq: 7.1945,ans: 7.2673,interp: 7.4817,fusion: -978.3622]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  1032890.375    \n",
      "module.ans_embedding.weight  dot:  320127.0625    \n",
      "module.lstm.weight_ih_l0  dot:  28736028.0    \n",
      "module.lstm.weight_hh_l0  dot:  8562015.0    \n",
      "module.lstm.bias_ih_l0  dot:  1868976.0    \n",
      "module.lstm.bias_hh_l0  dot:  1868976.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  6237909.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  1163.347412109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  212850.109375    \n",
      "module.ans_lstm.bias_hh_l0  dot:  212850.109375    \n",
      "module.adapter.frcn_linear.weight  dot:  56703504.0    \n",
      "module.adapter.frcn_linear.bias  dot:  37117.85546875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  834587.0625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  323.9935302734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  1081137.75    \n",
      "module.attflat_img.mlp.linear.bias  dot:  4.4565240386873484e-11    \n",
      "module.attflat_img.linear_merge.weight  dot:  17592770.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  47275.9765625    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  5677.67138671875    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  42.50251007080078    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  55715.96875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.0608346201479435e-08    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4858444.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  47275.9765625    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  0.7370851635932922    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  0.047373976558446884    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  4.7456183433532715    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  7.993605777301127e-15    \n",
      "module.attflat_ans.linear_merge.weight  dot:  3865827.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  2670453.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452845.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1045/6933] Loss: -915.5105 [iq: 7.0250,ans: 7.1492,interp: 7.6009,fusion: -937.2856]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  300351.125    \n",
      "module.ans_embedding.weight  dot:  1264199.125    \n",
      "module.lstm.weight_ih_l0  dot:  5748627.0    \n",
      "module.lstm.weight_hh_l0  dot:  11111552.0    \n",
      "module.lstm.bias_ih_l0  dot:  546160.375    \n",
      "module.lstm.bias_hh_l0  dot:  546160.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22394126.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  2667.862548828125    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2193702.5    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2193702.5    \n",
      "module.adapter.frcn_linear.weight  dot:  30887112.0    \n",
      "module.adapter.frcn_linear.bias  dot:  17861.04296875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  344779.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  164.6938018798828    \n",
      "module.attflat_img.mlp.linear.weight  dot:  259105.984375    \n",
      "module.attflat_img.mlp.linear.bias  dot:  1.255671122635249e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  12375254.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  33142.3828125    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  8837.9765625    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  117.62542724609375    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  68806.40625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  3.7690739418394514e-11    \n",
      "module.attflat_lang.linear_merge.weight  dot:  4753448.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  33142.3828125    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  35.802879333496094    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  6.874643802642822    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  110.92204284667969    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  6.963318810448982e-13    \n",
      "module.attflat_ans.linear_merge.weight  dot:  4627632.5    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5894337.5    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  16  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452845.5\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1046/6933] Loss: -941.5789 [iq: 7.2687,ans: 7.2999,interp: 6.8009,fusion: -962.9484]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  970168.9375    \n",
      "module.ans_embedding.weight  dot:  853870.25    \n",
      "module.lstm.weight_ih_l0  dot:  28537320.0    \n",
      "module.lstm.weight_hh_l0  dot:  13666682.0    \n",
      "module.lstm.bias_ih_l0  dot:  1741808.0    \n",
      "module.lstm.bias_hh_l0  dot:  1741808.0    \n",
      "module.ans_lstm.weight_ih_l0  dot:  22937720.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  17053.37109375    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2027147.75    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2027147.75    \n",
      "module.adapter.frcn_linear.weight  dot:  72094288.0    \n",
      "module.adapter.frcn_linear.bias  dot:  53638.24609375    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  646063.5625    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  285.8372802734375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  842054.25    \n",
      "module.attflat_img.mlp.linear.bias  dot:  2.18506102100946e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  22745406.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  62066.46875    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  11810.9814453125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  45.37098693847656    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  131147.140625    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  1.7783330363840832e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6363693.0    \n",
      "module.attflat_lang.linear_merge.bias  dot:  62066.46875    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  2847.285888671875    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  681.0498046875    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  11035.8056640625    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  6594068.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  5396706.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.bias  dot:  0.0    NOT UPDATING\n",
      "Gradient not updating in:  17  of total:  46\n",
      "@@@@@@@@@@@@@@@@@@@ Overall dot product:  1452846.25\n",
      "\n",
      "----------  MAIN LOSS  --------\n",
      "[Version hakku][Epoch  1][Step 1047/6933] Loss: -900.8920 [iq: 7.2033,ans: 6.9733,interp: 6.9025,fusion: -921.9711]          \n",
      "----------  FIRST LOSS  --------\n",
      "\n",
      "----------  FUSION LOSS  --------\n",
      "module.decoder_mlp_1.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_1.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.fc.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.weight  dot:  0.0    NOT UPDATING\n",
      "module.decoder_mlp_2.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.embedding.weight  dot:  432987.6875    \n",
      "module.ans_embedding.weight  dot:  2279322.0    \n",
      "module.lstm.weight_ih_l0  dot:  8851855.0    \n",
      "module.lstm.weight_hh_l0  dot:  11268500.0    \n",
      "module.lstm.bias_ih_l0  dot:  569322.375    \n",
      "module.lstm.bias_hh_l0  dot:  569322.375    \n",
      "module.ans_lstm.weight_ih_l0  dot:  34410604.0    \n",
      "module.ans_lstm.weight_hh_l0  dot:  6444.998046875    \n",
      "module.ans_lstm.bias_ih_l0  dot:  2659065.0    \n",
      "module.ans_lstm.bias_hh_l0  dot:  2659065.0    \n",
      "module.adapter.frcn_linear.weight  dot:  43318760.0    \n",
      "module.adapter.frcn_linear.bias  dot:  28816.98046875    \n",
      "module.attflat_img.mlp.fc.linear.weight  dot:  292517.46875    \n",
      "module.attflat_img.mlp.fc.linear.bias  dot:  141.3822021484375    \n",
      "module.attflat_img.mlp.linear.weight  dot:  185770.875    \n",
      "module.attflat_img.mlp.linear.bias  dot:  3.112745616817847e-10    \n",
      "module.attflat_img.linear_merge.weight  dot:  16408029.0    \n",
      "module.attflat_img.linear_merge.bias  dot:  44042.8359375    \n",
      "module.attflat_lang.mlp.fc.linear.weight  dot:  7788.57861328125    \n",
      "module.attflat_lang.mlp.fc.linear.bias  dot:  98.87388610839844    \n",
      "module.attflat_lang.mlp.linear.weight  dot:  43438.26171875    \n",
      "module.attflat_lang.mlp.linear.bias  dot:  4.014456322920523e-09    \n",
      "module.attflat_lang.linear_merge.weight  dot:  6866935.5    \n",
      "module.attflat_lang.linear_merge.bias  dot:  44042.8359375    \n",
      "module.attflat_ans.mlp.fc.linear.weight  dot:  205.07998657226562    \n",
      "module.attflat_ans.mlp.fc.linear.bias  dot:  51.944149017333984    \n",
      "module.attflat_ans.mlp.linear.weight  dot:  343.17254638671875    \n",
      "module.attflat_ans.mlp.linear.bias  dot:  0.0    NOT UPDATING\n",
      "module.attflat_ans.linear_merge.weight  dot:  5219695.0    \n",
      "module.attflat_ans.linear_merge.bias  dot:  7372046.0    \n",
      "module.proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.proj.weight  dot:  0.0    NOT UPDATING\n",
      "module.proj.bias  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.a_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj_norm.b_2  dot:  0.0    NOT UPDATING\n",
      "module.ans_proj.weight  dot:  0.0    NOT UPDATING\n"
     ]
    }
   ],
   "source": [
    "execution.run(__C.RUN_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
