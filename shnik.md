# Table for Papers

## Topics to read

- [ ] Graph convolution networks
- [ ] Variational AutoEncoders
- [ ] Capsule Networks
- [ ] Transformer Networks
- [ ] Beam search
- [ ] Bi-Affin
- [ ] BiDAF Learning
- [ ] Continual Learning
- [ ] Imitation Learning
- [ ] Domain Adaptation
- [ ] Dirichlet Latent Variables
- [ ] Cross Attention

Paper | Again? | Abstract Summary | Topic  
--- | --- | --- | ---
[Interaction over Interaction Networks](https://www.aclweb.org/anthology/P19-1001.pdf) | :heavy_check_mark: | Deep "utterance-response" interaction networks | Dialogue
[Incremental Transformer with Deliberation Decoder for Document Grounded Conversations](https://www.aclweb.org/anthology/P19-1002.pdf) | :x: | same as title | Dialogue
[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf) | :x: | Utterance are re-written by covering references | Dialogue
[Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study](https://www.aclweb.org/anthology/P19-1004.pdf) | :heavy_check_mark: Nice | Check sensitivity of utterances to perturbations | Dialogue
[Boosting Dialog Response Generation](https://www.aclweb.org/anthology/P19-1005.pdf) | :x: | Iterative training ensemble based on boosting |Dialogue 
[Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection](https://www.aclweb.org/anthology/P19-1006.pdf) | :x: | chooses best candidate response(context based) using stm networks | Dialogue
[Semantic Parsing with Dual Learning](https://www.aclweb.org/anthology/P19-1007.pdf) | :heavy_check_mark: | Dual Learning to improve results in semantic parsing | Semantic Parsing
[Semantic expressive capacity with bounded memory](https://www.aclweb.org/anthology/P19-1008.pdf) | :x: | investigate the capacity of mechanisms for compositional semantic parsing | Semantic Parsing
[AMR Parsing as Sequence-to-Graph Transduction](https://www.aclweb.org/anthology/P19-1009.pdf) | :x: | attention model for damr parsing as sequence to graph transduction | Parsing
[Generating Logical Forms from Graph Representations of Text and Entities](https://www.aclweb.org/anthology/P19-1010.pdf) | :heavy_check_mark: | Uses Graph neural network to incorporate information from various entities | Parsing
[Learning Compressed Sentence Representations for On-Device Text Processing](https://www.aclweb.org/anthology/P19-1011.pdf) | :x: | computation and space  | Complexity
[The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers](https://www.aclweb.org/anthology/P19-1012.pdf) | :x: | "Explaining the utility of structural features in BiLSTM based dependency parsers | Explainable / Understanding NLP
[Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation](https://www.aclweb.org/anthology/P19-101333.pdf) | :x: |  |
[Reliability-aware Dynamic Feature Composition for Name Tagging](https://www.aclweb.org/anthology/P19-1016.pdf) | :x: | Basically, weighted word embeddings with rare words assigned lesser weights | Name tagging relevant approaches
[Unsupervised Pivot Translation for Distant Languages](https://www.aclweb.org/anthology/P19-1017.pdf) | :x: | Multiple hops to translate into distant language | NMT
[Effective Adversarial Regularization for Neural Machine Translation](https://www.aclweb.org/anthology/P19-1020.pdf) | :heavy_check_mark: | Adverserial perturbations for adverserial regularization | NMT
[Neural Relation Extraction for Knowledge Base Enrichment](https://www.aclweb.org/anthology/P19-1023.pdf) | :heavy_check_mark: | forms 3-tuples to express relationships between entities to enrich knowledge | Knowledge
[Attention Guided Graph Convolutional Networks for Relation Extraction](https://www.aclweb.org/anthology/P19-1024.pdf) | :heavy_check_mark: | Extracting information from dependency graphs using attention based graph cnns | relational extraction
[Self-Regulated Interactive Sequence-to-Sequence Learning](https://www.aclweb.org/anthology/P19-1029.pdf) | :x: | optimization of cost for feedback on learning | IDK
[u Only Need Attention to Traverse Trees](https://www.aclweb.org/anthology/P19-1030.pdf) | :heavy_check_mark: | Tree transformer | Attention
[Cross-Domain Generalization of Neural Constituency Parsers](https://www.aclweb.org/anthology/P19-1031.pdf) | :heavy_check_mark: | neural constituency parsers generalization in other domains | NCP
[Adaptive Attention Span in Transformers](https://www.aclweb.org/anthology/P19-1032.pdf) | :heavy_check_mark: | self attention mechanism that can learn its optimal attention span | attention
[Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings](https://www.aclweb.org/anthology/P19-1036.pdf) | :x: | uses textual similarity between different words | text classification
[Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://www.aclweb.org/anthology/P19-1041.pdf) | :heavy_check_mark: | Looks like an interesting read on style transfer by disentangled representations (also see in CV) | style transfer
[Cross-Sentence Grammatical Error Correction](https://www.aclweb.org/anthology/P19-1042.pdf) | :heavy_check_mark: | Grammar Error Correction with context | error correction
[Learning Emphasis Selection for Written Text in Visual Media from Crowd-Sourced Label Distributions](https://www.aclweb.org/anthology/P19-1112.pdf) | :heavy_check_mark: | employs an end to end label distribution learning model | Emphasis selection
[Improving Neural Language Models by Segmenting, Attending, and Predicting the Future](https://www.aclweb.org/anthology/P19-1144.pdf) | :heavy_check_mark: Nice Work | Improving language models by phrase segmentation | Improving languge models
[Visually Grounded Neural Syntax Acquisition](https://www.aclweb.org/anthology/P19-1180.pdf) | :heavy_check_mark: | joint visual-semantic embedding space | Visual grounding for syntax parsing
[Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation](https://www.aclweb.org/anthology/P19-1081.pdf) | :heavy_check_mark: | New metric for language understanding in navigation, new dataset, google research | vision and language navigation 
[Expressing Visual Relationships via Language](https://www.aclweb.org/anthology/P19-1182.pdf) | :heavy_check_mark: Nice work | New datsets and attention based baselines on visual relationships using language | relationship between pairs of images
[Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video](https://www.aclweb.org/anthology/P19-1183.pdf) | :heavy_check_mark: Nice Work | Localizing the given sentence in the given video | vision and language
[The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue](https://www.aclweb.org/anthology/P19-1184.pdf) | :heavy_check_mark: | Dataset on dialogues with visual context | Dialogue with images in context
[Keep Meeting Summaries on Topic: ive Multi-Modal Meeting Summarization](https://www.aclweb.org/anthology/P19-1209.pdf) | :heavy_check_mark: | Multi people meeting summarization using video and transcript | multimodal summarization
[Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information](https://www.aclweb.org/anthology/P19-1257.pdf) | :heavy_check_mark: | Commenting on articles inculcating information from images also | commenting on articles
[Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension](https://www.aclweb.org/anthology/P19-1347.pdf) | :heavy_check_mark: Excellent Work | Textbook question answering considering images in text along with out of domain knowledge using GCN | TQA
[ELI5: Long Form Question Answering](https://www.aclweb.org/anthology/P19-1346.pdf) | :heavy_check_mark: | A dataset from r/explainlikeimfive | Long form question answering
[Generating Question Relevant Captions to Aid Visual Question Answering](https://www.aclweb.org/anthology/P19-1348.pdf) | :heavy_check_mark: nice | Image captioning to help in visual qa via online gradient based method |  VQA
[Multi-grained Attention with Object-level Grounding for Visual Question Answering](https://www.aclweb.org/anthology/P19-1349.pdf) | :heavy_check_mark:, Nice | learns explicit word object correspondence by word level attentions | VQA
[Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering](https://www.aclweb.org/anthology/P19-1350.pdf) | :heavy_check_mark: | Devised a set of linguistically informed VQA tasks | VQA
[Improving Visual Question Answering by Referring to Generated Paragraph Captions](https://www.aclweb.org/anthology/P19-1351.pdf) | :heavy_check_mark: | Given image and paragraph description, answer the asked question | VQA
[Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)](https://www.aclweb.org/anthology/P19-1455.pdf) | :heavy_check_mark: | MUStARD dataset, video+audio -> sarcasm, code available | Sarcasm detection 
[Like a Baby: Visually Situated Neural Language Acquisition](https://www.aclweb.org/anthology/P19-1506.pdf) | :heavy_check_mark: | use of visual context in training to predict next word | vision and language
[Ordinal and Attribute Aware Response Generation in a Multimodal Dialogue System](https://www.aclweb.org/anthology/P19-1540.pdf) | :heavy_check_mark: | position and attribute aware attention mechanism for dialogue system | Dialogue system
[Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems](https://www.aclweb.org/anthology/P19-1564.pdf) | :heavy_check_mark: | Multimodal transformer network | video based dialogue
[Storyboarding of Recipes: Grounded Contextual Generation](https://www.aclweb.org/anthology/P19-1606.pdf) | :heavy_check_mark: | text generation (how to) from images |  text generation
[Latent Variable Model for Multi-modal Translation](https://www.aclweb.org/anthology/P19-1642.pdf) | :heavy_check_mark: | images at training time but not at testing time | translation
[Identifying Visible Actions in Lifestyle Vlogs](https://www.aclweb.org/anthology/P19-1643.pdf) | :heavy_check_mark: | identifies if actions mentioned in the speech description of a video are visually present | vision and language
[A Corpus for Reasoning about Natural Language Grounded in Photographs](https://www.aclweb.org/anthology/P19-1644.pdf) | :heavy_check_mark: | dataset of natural lang and phots, task to determine if the caption is fitting for the given image |  Dataset on visual reasoning
[Learning to Discover, Ground and Use Words with Segmental Neural Language Models](https://www.aclweb.org/anthology/P19-1645.pdf) | :heavy_check_mark: | language modeling with grounding in nonlinguistic modalities | visual context, language modeling
[What Should I Ask? Using Conversationally Informative Rewards for Goal-oriented Visual Dialog.](https://www.aclweb.org/anthology/P19-1646.pdf) | :heavy_check_mark: | end to end visual dialogue system with rl, then generate questions about image | visual dialogue with RL
[Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language](https://www.aclweb.org/anthology/P19-1647.pdf) | :heavy_check_mark: | Text with speech, speech with images, text with images | spoken language processing
[Multi-step Reasoning via Recurrent Dual Attention for Vi-sual Dialog](https://www.aclweb.org/anthology/P19-1648.pdf) | :heavy_check_mark: | visual dialog by iteratively refining semantic representation | Visual Dialogue
[Informative Image Captioning with External Sources of Information](https://www.aclweb.org/anthology/P19-1650.pdf) | :heavy_check_mark: | integrating image info together with fine grained labels | Image captionning
[CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication](https://www.aclweb.org/anthology/P19-1651.pdf) | :heavy_check_mark:, Nice work FAIR | two agents, one teller, other drawer | Natural language dialogue
[Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning](https://www.aclweb.org/anthology/P19-1652.pdf) | :heavy_check_mark: | Generate a restricted vocabulary grounded by images | Image captioning
[Distilling Translations with Visual Awareness](https://www.aclweb.org/anthology/P19-1653.pdf) | :heavy_check_mark: | translation using textual context in stage 1 and visual context in stage 2 (only if required) | translation
[VIFIDEL: Evaluating the Visual Fidelity of Image Descritions](https://www.aclweb.org/anthology/P19-1654.pdf) | :heavy_check_mark: | a new metric for generated image descriptions based on objects depicted and words in description | metric for image description
[Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation](https://www.aclweb.org/anthology/P19-1655.pdf) | :heavy_check_mark: | vision and language navigation: improve by using multiple modalities individually, combinedely using might hurt result | vision and language navigation 
[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://www.aclweb.org/anthology/P19-1656.pdf) | :heavy_check_mark: | cross modAl attention based transformer modEl | General language modeling and multimodal grounding
[Visual Story Post-Editing](https://www.aclweb.org/anthology/P19-1658.pdf) | :heavy_check_mark: | Edit visual stories created by models using training from this dataset | dataset on edition visual stories
[Multimodal ive Summarization for How2 Videos](https://www.aclweb.org/anthology/P19-1659.pdf) | :heavy_check_mark: | multi-modal seq-to-seq modal for summarization and a new metric | Summarization

