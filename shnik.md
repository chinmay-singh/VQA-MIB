# Table for Papers

Paper | Name? | Abstract? | Topic  
--- | --- | --- | ---
[Interaction over Interaction Networks](https://www.aclweb.org/anthology/P19-1001.pdf) | :heavy_check_mark: | Deep "utterance-response" interaction networks | Dialogue
[Incremental Transformer with Deliberation Decoder for Document Grounded Conversations](https://www.aclweb.org/anthology/P19-1002.pdf) | :x: | same as title | Dialogue
[Improving Multi-turn Dialogue Modelling with Utterance ReWriter](https://www.aclweb.org/anthology/P19-1003.pdf) | :x: | Utterance are re-written by covering references | Dialogue
[Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study](https://www.aclweb.org/anthology/P19-1004.pdf) | :heavy_check_mark: Nice | Check sensitivity of utterances to perturbations | Dialogue
[Boosting Dialog Response Generation](https://www.aclweb.org/anthology/P19-1005.pdf) | :x: | Iterative training ensemble based on boosting |Dialogue 
[Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection](https://www.aclweb.org/anthology/P19-1006.pdf) | :x: | chooses best candidate response(context based) using stm networks | Dialogue
[Semantic Parsing with Dual Learning](https://www.aclweb.org/anthology/P19-1007.pdf) | :heavy_check_mark: | Dual Learning to improve results in semantic parsing | Semantic Parsing
[Semantic expressive capacity with bounded memory](https://www.aclweb.org/anthology/P19-1008.pdf) | :x: | investigate the capacity of mechanisms for compositional semantic parsing | Semantic Parsing
[AMR Parsing as Sequence-to-Graph Transduction](https://www.aclweb.org/anthology/P19-1009.pdf) | :x: | attention model for damr parsing as sequence to graph transduction | Parsing
[Generating Logical Forms from Graph Representations of Text and Entities](https://www.aclweb.org/anthology/P19-1010.pdf) | :heavy_check_mark: | Uses Graph neural network to incorporate information from various entities | Parsing
[Learning Compressed Sentence Representations for On-Device Text Processing](https://www.aclweb.org/anthology/P19-1011.pdf) | :x: | computation and space  | Complexity
[The (Non-)Utility of Structural Features in BiLSTM-based Dependency Parsers](https://www.aclweb.org/anthology/P19-1012.pdf) | :x: | "Explaining the utility of structural features in BiLSTM based dependency parsers | Explainable / Understanding NLP
[Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation](https://www.aclweb.org/anthology/P19-101333.pdf) | :x: |  |
[Reliability-aware Dynamic Feature Composition for Name Tagging](https://www.aclweb.org/anthology/P19-1016.pdf) | :x: | Basically, weighted word embeddings with rare words assigned lesser weights | Name tagging relevant approaches
[Unsupervised Pivot Translation for Distant Languages](https://www.aclweb.org/anthology/P19-1017.pdf) | :x: | Multiple hops to translate into distant language | NMT
[Effective Adversarial Regularization for Neural Machine Translation](https://www.aclweb.org/anthology/P19-1020.pdf) | :heavy_check_mark: | Adverserial perturbations for adverserial regularization | NMT
[Neural Relation Extraction for Knowledge Base Enrichment](https://www.aclweb.org/anthology/P19-1023.pdf) | :heavy_check_mark: | forms 3-tuples to express relationships between entities to enrich knowledge | Knowledge
[Attention Guided Graph Convolutional Networks for Relation Extraction](https://www.aclweb.org/anthology/P19-1024.pdf) | :heavy_check_mark: | Extracting information from dependency graphs using attention based graph cnns | relational extraction
[Self-Regulated Interactive Sequence-to-Sequence Learning](https://www.aclweb.org/anthology/P19-1029.pdf) | :x: | optimization of cost for feedback on learning | IDK
[u Only Need Attention to Traverse Trees](https://www.aclweb.org/anthology/P19-1030.pdf) | :heavy_check_mark: | Tree transformer | Attention
[Cross-Domain Generalization of Neural Constituency Parsers](https://www.aclweb.org/anthology/P19-1031.pdf) | :heavy_check_mark: | neural constituency parsers generalization in other domains | NCP
[Adaptive Attention Span in Transformers](https://www.aclweb.org/anthology/P19-1032.pdf) | :heavy_check_mark: | self attention mechanism that can learn its optimal attention span | attention
[Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings](https://www.aclweb.org/anthology/P19-1036.pdf) | :x: | uses textual similarity between different words | text classification
[Disentangled Representation Learning for Non-Parallel Text Style Transfer](https://www.aclweb.org/anthology/P19-1041.pdf) | :heavy_check_mark: | Looks like an interesting read on style transfer by disentangled representations (also see in CV) | style transfer
[Cross-Sentence Grammatical Error Correction](https://www.aclweb.org/anthology/P19-1042.pdf) | :heavy_check_mark: | Grammar Error Correction with context | error correction
