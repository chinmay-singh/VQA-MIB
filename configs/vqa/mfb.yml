# Network
MODEL_USE: mfb
HIGH_ORDER: False # True for MFH, False for MFB
HIDDEN_SIZE: 512
MFB_K: 5
MFB_O: 1000
LSTM_OUT_SIZE: 1024
DROPOUT_R: 0.1
I_GLIMPSES: 2
Q_GLIMPSES: 2
WITH_FUSION_LOSS: True
AUGMENTED_ANSWER: True
LSTM_NUM_DIRECTIONS: 1
LSTM_LAYERS: 1
QUES_STDDEV: 0.1
ANS_STDDEV: 0.1

# Training mode options: [original, simultaneous_qa, pretrained_ans, pretraining_ans]
TRAINING_MODE: original

# Execution
BATCH_SIZE: 64
LR_BASE: 0.0007
LR_DECAY_R: 0.5
LR_DECAY_LIST: [6, 12]
WARMUP_EPOCH: 3
MAX_EPOCH: 22
GRAD_NORM_CLIP: -1
GRAD_ACCU_STEPS: 1
LOSS_FUNC: kld
LOSS_REDUCTION: sum
OPT: Adam
OPT_PARAMS: {betas: '(0.9, 0.99)', eps: '1e-9'}
